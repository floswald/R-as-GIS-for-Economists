[
["index.html", "R as GIS for Economists Welcome", " R as GIS for Economists Taro Mieno 2020-08-07 Welcome This book is being developed as part of my effort to put together course materials for my data science course targeted at upper-level undergraduate and graduate students at the University of Nebraska Lincoln. This book aims particularly at spatial data processing for econometric projects, where spatial variables become part of an econometric analysis. Over the years, I have seen many students and researchers who spend so much time just processing spatial data (often involving clicking the ArcGIS (or QGIS) user interface to death), which is a waste of time from the perspective of academic productivity. My hope is that this book will help researchers become more proficient in spatial data processing and enhance the overall productivity of the fields of economics for which spatial data are essential. About me I am an Assistant Professor at the Department of Agricultural Economics at the University of Nebraska Lincoln, where I also teach Econometrics for Master’s students. My research interests lie in precision agriculture, water economics, and agricultural policy. My personal website is here. Contributors of the book Here is the list of contributors to the book and the parts they contributed to: Bowen Chen, Postdoctoral researcher, Department of Agricultural and Consumer Economics, University of Illinois at Urbana-Champaign Section 9.2 Shinya Uryu, Data engineer, National Institute of Environmental Studies, Japan Section 8 Comments and Suggestions? Any constructive comments and suggestions about how I can improve the book are all welcome. Please send me an email at tmieno2@unl.edu or create an issue on the github page of this book. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["preface.html", "Preface ", " Preface "],
["why-r-as-gis-for-economists.html", "Why R as GIS for Economists?", " Why R as GIS for Economists? R has extensive capabilities as GIS software. In my opinion, \\(99\\%\\) of your spatial data processing needs as an economist will be satisfied by R. But, there are several popular options for GIS tasks other than R: Python ArcGIS QGIS Here I compare them briefly and discuss why R is a good option. R vs Python Both R and Python are actually heavily dependent on open source software GDAL and GEOS for their core GIS operations (GDAL for reading spatial data, and GEOS for geometrical operations like intersecting two spatial layers).1 So, when you run GIS tasks on R or Python you basically tell R or Python what you want to do and they talk to the software, let it do the job, and return the results to you. This means that R and Python are not much different in their capability at GIS tasks as they are dependent on the common open source software for many GIS tasks. When GDAL and GEOS get better, R and Python get better (with a short lag all thanks to those who make updates available on R). Both of them have good spatial visualization tools as well. Moreover, both R and Python can communicate with QGIS and ArcGIS (as long you as have them installed of course) and use their functionalities from within R and Python via the bridging packages: RQGIS and PyQGIS for QGIS, and R-ArcGIS and ArcPy.2 So, if you are more familiar with Python than R, go ahead and go with Python. From now on, my discussions assume that you are going for the R option, as otherwise, you would not be reading the rest of the book anyway. R vs ArcGIS or QGIS ArcGIS is commercial software and it is quite expensive (you are likely to be able to get a significant discount if you are a student at or work for a University). On the other hand, QGIS is open source and free. It has seen significant development over the decade, and I would say it is just as competitive as ArcGIS. QGIS also uses open source geospatial software GDAL, GEOS, and others (SAGA, GRASS GIS). Both of them have a graphical interface that helps you implement various GIS tasks unlike R which requires programming. Now, since R can use ArcGIS and QGIS through the bridging packages, a more precise question we should be asking is whether you should program GIS tasks using R (possibly using the bridging packages) or manually implement GIS tasks using the graphical interface of ArcGIS or QGIS. The answer is programming GIS tasks using R. First, manual GIS operations are hard to repeat. It is often the case that in the course of a project you need to redo the same GIS task except that the underlying datasets have changed. If you have programmed the process with R, you just run the same code and that’s it. You get the desired results. If you did not program it, you need to go through many clicks on the graphical interface all over again, potentially trying to remember how you actually did it the last time.3 Second and more important, manual operations are not scalable. It has become much more common that we need to process many large spatial datasets. Imagine you are doing the same operations on \\(1,000\\) files using a graphical interface, or even \\(50\\) files. Do you know what is good at doing the same tasks over and over again without complaining? A computer. Just let it do what it likes to do. You have better things do. Finally, should you learn ArcGIS or QGIS in addition to (or before) R? I am doubtful. As economists, the GIS tasks we need to do are not super convoluted most of the time. Suppose \\(\\Omega_R\\) and \\(\\Omega_{AQ}\\) represent the set of GIS tasks R and \\(ArcGIS/QGIS\\) can implement, respectively. Further, let \\(\\Omega_E\\) represent the set of skills economists need to implement. Then, \\(\\Omega_E \\in \\Omega_R\\) \\(99\\%\\) (or maybe \\(95\\%\\) to be safe) of the time and \\(\\Omega_E \\not\\subset \\Omega_{AQ}\\setminus\\Omega_R\\) \\(99\\%\\) of the time. Personally, I have never had to rely on either ArcGIS or QGIS for my research projects after I learned how to use R as GIS. One of the things ArcGIS and QGIS can do but R cannot do (\\(\\Omega_{AQ}\\setminus\\Omega_R\\)) is create spatial objects by hand using a graphical user interface, like drawing polygons and lines. Another thing that R lags behind ArcGIS and QGIS is 3D data visualization. But, I must say neither of them is essential for economists at the moment. Finally, sometime it is easier and faster to make a map using ArcGIS and QGIS especially for a complicated map.4 Using R as GIS, however, comes with a learning curve for those who have never used R because basic knowledge of R and general programming is required. On the other hand, the GUI-based use of ArcGIS and QGIS has a very low start-up cost. For those who have used R for other purposes like data wrangling and regression analysis, you have already (or almost) climbed up the hill and are ready to learn how to use R as GIS. Summary You have never used any GIS software, but are very comfortable with R? Learn how to use R as GIS first. If you find out you really cannot complete the GIS tasks you would like to do using R, then turn to other options. You have never used any GIS software and R? This is tough. If you expect significant amount of GIS work, learning R basics and how to use R as GIS is a good investment of your time. You have used ArcGIS or QGIS and do not like them because they crash often? Why don’t you try R?5 You may realize you actually do not need them. You have used ArcGIS or QGIS before and are very comfortable with them, but you need to program repetitive GIS tasks? Learn R and maybe take advantage of R-ArcGIS or RQGIS, which this book does not cover. You know for sure that you need to run only a simple GIS task once and never have to do any GIS tasks ever again? Stop reading and ask one of your friends to do the job. Pay him/her \\(\\$20\\) per hour, which is way below the opportunity cost of setting up either ArcGIS or QGI and learning to do that simple task. For example, see the very first sentence of this page↩︎ We do not learn them in this lecture note because I do not see the benefits of using them.↩︎ You could take a step-by-step note of what you did though.↩︎ Let me know if you know something that is essential for economists that only ArcGIS or QGIS can do. I will add that to the list here.↩︎ I am not saying R does not crash. R does crash. But, often times, the fault is yours, rather than the software’s.↩︎ "],
["how-is-this-book-different-from-other-online-books-and-resources.html", "How is this book different from other online books and resources?", " How is this book different from other online books and resources? We are seeing an explosion of online (and free) resources that teach how to use R for spatial data processing.6 Here is an incomplete list of such resources: Geocomputation with R Spatial Data Science Spatial Data Science with R Introduction to GIS using R Code for An Introduction to Spatial Analysis and Mapping in R Introduction to GIS in R Intro to GIS and Spatial Analysis Introduction to Spatial Data Programming with R Reproducible GIS analysis with R R for Earth-System Science Rspatial NEON Data Skills Simple Features for R Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny Thanks to all these resources, it has become much easier to self-teach R for GIS work than six or seven years ago when I first started using R for GIS. Even though I have not read through all these resources carefully, I am pretty sure every topic found in this book can also be found somewhere in these resources (except the demonstrations). So, you may wonder why on earth you can benefit from reading this book. It all boils down to search costs. Researchers in different disciplines require different sets of spatial data skills. The available resources are typically very general covering so many topics that economists are unlikely to use. It is particularly hard for those who do not have much experience in GIS to identify whether particular skills are essential or not. So, they could spend so much time learning something that is not really useful. The value of this book lies in its deliberate incomprehensiveness. It only packages materials that satisfy the need of most economists, cutting out many topics that are likely to be of limited use for economists. For those who are looking for more comprehensive treatments of spatial data handling and processing in one book, I personally like Geocomputation with R a lot. Increasingly, the developer of R packages created a website dedicated to their R packages, where you can often find vignettes (tutorials), like Simple Features for R. This phenomenon is largely thanks to packages like bookdown (Xie 2016), blogdown (Xie, Hill, and Thomas 2017), and pkgdown (Wickham and Hesselberth 2020) that has lowered the cost of professional contents creation much much lower than before. Indeed, this book was built taking advantage of the bookdown package.↩︎ "],
["what-is-going-to-be-covered-in-this-book.html", "What is going to be covered in this book?", " What is going to be covered in this book? The book starts with the very basics of spatial data handling (e.g., importing and exporting spatial datasets) and moves on to more practical spatial data operations (e.g., spatial data join) that are useful for research projects. This books is still under development. Right now, only Chapters 1 through 5 and Appendix A are available. I will work on the rest of the book over the summer. The “coming soon” chapters are close to be done. I just need to add finishing touches to those chapters. The “wait a bit” chapters need some more work, adding contents, etc. Chapter 1: Demonstrations of R as GIS (available) groundwater pumping and groundwater level precision agriculture land use and weather corn planted acreage and railroads groundwater pumping and weather Chapter 2: The basics of vector data handling using sf package (available) spatial data structure in sf import and export vector data (re)projection of spatial datasets single-layer geometrical operations (e.g., create buffers, find centroids) other miscellaneous basic operations Chapter 3: Spatial interactions of vector datasets (available) understand topological relations of multiple sf objects spatially subsetting a layer based on another layer extracting values from one layer to another layer Chapter 4: The basics of raster data handling using the raster and terra packages (available) understand object classes by the terra and raster packages import and export raster data stack raster data quick plotting Chapter 5: Spatial interactions of vector and raster datasets (available) cropping a raster layer to the geographic extent of a vector layer extracting values from a raster layer to a vector layer Chapter 6: Speed things up (available) make raster data extraction faster by parallelization Chapter 7: Spatiotemporal raster data handling with the stars package (available) Chapter 8: Creating publication-quality maps (wait a bit) use the tmap and ggplot2 packages to create maps Chapter 9: Download and process publicly available spatial datasets (partially available) USDA NASS QuickStat (tidyUSDA) - available PRISM (prism) - available Daymet (daymetr) - available Cropland Data Layer (cdlTools) - under construction USGS (dataRetrieval) - under construction Sentinel 2 (sen2r) - under construction Census (tidycensus) - under construction Appendix A: Loop and parallel computation (available) Appendix B: Cheatsheet (wait a bit) As you can see above, this book does not spend any time on the very basics of GIS concepts. Before you start reading the book, you should know the followings at least (it’s not much): What Geographic Coordinate System (GCS), Coordinate Reference System (CRS), and projection are (this is a good resource) Distinctions between vector and raster data (this is a simple summary of the difference) This book is about spatial data processing and does not provide detailed explanations on non-spatial R operations, assuming some basic knowledge of R. In particular, the dplyr and data.table packages are extensively used for data wrangling. For data wrangling using tidyverse (a collection of packages including dplyr), see R for Data Science. For data.table, this is a good resource. Finally, this book does not cover spatial statistics or spatial econometrics at all. This book is about spatial data processing. Spatial analysis is something you do after you have processed spatial data. "],
["conventions-of-the-book-and-some-notes.html", "Conventions of the book and some notes", " Conventions of the book and some notes Here are some notes of the conventions of this book and notes for R beginners and those who are not used to reading rmarkdown-generated html documents. Texts in gray boxes They are one of the following: objects defined on R during demonstrations R functions R packages When it is a function, I always put parentheses at the end like this: st_read().7 Sometimes, I combine a package and function in one like this: sf::st_read(). This means it is a function called st_read() from the sf package. Colored Boxes Codes are in blue boxes, and outcomes are in red boxes. Codes: runif(5) Outcomes: ## [1] 0.78090310 0.85418522 0.58754715 0.57286746 0.03565908 Parentheses around codes Sometimes you will see codes enclosed by parenthesis like this: ( a &lt;- runif(5) ) ## [1] 0.6605491 0.7835087 0.7385062 0.9176595 0.9610564 The parentheses prints what’s inside of a newly created object (here a) without explicitly evaluating the object. So, basically I am signaling that we will be looking inside of the object that was just created. This one prints nothing. a &lt;- runif(5) Footnotes Footnotes appear at the bottom of the page. You can easily get to a footnote by clicking on the footnote number. You can also go back to the main narrative where the footnote number is by clicking on the curved arrow at the end of the footnote. So, don’t worry about having to scroll all the way up to where you were after reading footnotes. This is a function that draws values randomly from the uniform distribution.↩︎ "],
["session-information.html", "Session Information", " Session Information Here is the session information when compiling the book: sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Mojave 10.14.1 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.18 Rcpp_1.0.5 lattice_0.20-41 codetools_0.2-16 ## [5] lubridate_1.7.8 digest_0.6.25 grid_4.0.2 magrittr_1.5 ## [9] evaluate_0.14 stringi_1.4.6 rlang_0.4.6 rstudioapi_0.11 ## [13] raster_3.3-7 sp_1.4-2 generics_0.0.2 rmarkdown_2.1 ## [17] rgdal_1.5-8 tools_4.0.2 stringr_1.4.0 yaml_2.2.1 ## [21] xfun_0.16 compiler_4.0.2 terra_0.8-5 htmltools_0.4.0 ## [25] knitr_1.29 "],
["demo.html", "Chapter 1 R as GIS: Demonstrations ", " Chapter 1 R as GIS: Demonstrations "],
["before-you-start.html", "Before you start", " Before you start The primary objective of this chapter is to showcase the power of R as GIS through demonstrations using mock-up econometric research projects8. Each project consists of a project overview (objective, datasets used, econometric model, and GIS tasks involved) and demonstration. This is really NOT a place you learn the nuts and bolts of how R does spatial operations. Indeed, we intentionally do not explain all the details of how the R codes work. We reiterate that the main purpose of the demonstrations is to get you a better idea of how R can be used to process spatial data to help your research projects involving spatial datasets. Finally, note that these mock-up projects use extremely simple econometric models that completely lacks careful thoughts you would need in real research projects. So, don’t waste your time judging the econometric models, and just focus on GIS tasks. If you are not familiar with html documents generated by rmarkdown, you might benefit from reading the conventions of the book in the Preface. Finally, for those who are interested in replicating the demonstrations, directions for replication are provided below. However, I would suggest focusing on the narratives for the first time around, learn the nuts and bolts of spatial operations from Chapters 2 through 5, and then come back to replicate them. Target Audience The target audience of this chapter is those who are not very familiar with R as GIS. Knowledge of R certainly helps. But, I tried to write in a way that R beginners can still understand the power of R as GIS9. Do not get bogged down by all the complex-looking R codes. Just focus on the narratives and figures to get a sense of what R can do. Direction for replication Datasets Running the codes in this chapter involves reading datasets from a disk. All the datasets that will be imported are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:10 set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Notes stargazer() function creates regression tables using regression results. To generate readable tables on your console, change type = \"html\" to type = \"text\". Note that this lecture does not deal with spatial econometrics at all. This lecture is about spatial data processing, not spatial econometrics. This is a great resource for spatial econometrics in R.↩︎ I welcome any suggestions to improve the reading experience of unexperienced R users.↩︎ I thought about using the here package, but I found it a bit confusing for unexperienced R users.↩︎ "],
["Demo1.html", "1.1 Demonstration 1: The impact of groundwater pumping on depth to water table", " 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.1.1 Project Overview Objective: Understand the impact of groundwater pumping on groundwater level. Datasets Groundwater pumping by irrigation wells in Chase, Dundy, and Perkins Counties in the southwest corner of Nebraska Groundwater levels observed at USGS monitoring wells located in the three counties and retrieved from the National Water Information System (NWIS) maintained by USGS using the dataRetrieval package. Econometric Model In order to achieve the project objective, we will estimate the following model: \\[ y_{i,t} - y_{i,t-1} = \\alpha + \\beta gw_{i,t-1} + v \\] where \\(y_{i,t}\\) is the depth to groundwater table11 in March12 in year \\(t\\) at USGS monitoring well \\(i\\), and \\(gw_{i,t-1}\\) is the total amount of groundwater pumping that happened within the 2-mile radius of the monitoring well \\(i\\). GIS tasks read an ESRI shape file as an sf (spatial) object use sf::st_read() download depth to water table data using the dataRetrieval package developed by USGS use dataRetrieval::readNWISdata() and dataRetrieval::readNWISsite() create a buffer around USGS monitoring wells use sf::st_buffer() convert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects use sf::st_as_sf() and sf::st_set_crs() reproject an sf object to another CRS use sf::st_transform() identify irrigation wells located inside the buffers and calculate total pumping use sf::st_join() create maps use the tmap package Preparation for replication Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( sf, # vector data operations dplyr, # data wrangling dataRetrieval, # download USGS NWIS data lubridate, # Date object handling stargazer, # regression table generation lfe # fast regression with many fixed effects ) 1.1.2 Project Demonstration The geographic focus of the project is the southwest corner of Nebraska consisting of Chase, Dundy, and Perkins County (see Figure 1.1 for their locations within Nebraska). Let’s read a shape file of the three counties represented as polygons. We will use it later to spatially filter groundwater level data downloaded from NWIS. three_counties &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;urnrd&quot;) %&gt;% #--- project to WGS84/UTM 14N ---# st_transform(32614) Reading layer `urnrd&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395 geographic CRS: NAD83 Figure 1.1: The location of Chase, Dundy, and Perkins County in Nebraska We have already collected groundwater pumping data, so let’s import it. #--- groundwater pumping data ---# ( urnrd_gw &lt;- readRDS(&quot;./Data/urnrd_gw_pumping.rds&quot;) ) well_id year vol_af lon lat 1: 1706 2007 182.566 245322.3 4542717 2: 2116 2007 46.328 245620.9 4541125 3: 2583 2007 38.380 245660.9 4542523 4: 2597 2007 70.133 244816.2 4541143 5: 3143 2007 135.870 243614.0 4541579 --- 18668: 2006 2012 148.713 284782.5 4432317 18669: 2538 2012 115.567 284462.6 4432331 18670: 2834 2012 15.766 283338.0 4431341 18671: 2834 2012 381.622 283740.4 4431329 18672: 4983 2012 NA 284636.0 4432725 well_id is the unique irrigation well identifier, and vol_af is the amount of groundwater pumped in acre-feet. This dataset is just a regular data.frame with coordinates. We need to convert this dataset into a object of class sf so that we can later identify irrigation wells located within a 2-mile radius of USGS monitoring wells (see Figure 1.2 for the spatial distribution of the irrigation wells). urnrd_gw_sf &lt;- urnrd_gw %&gt;% #--- convert to sf ---# st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---# st_set_crs(32614) #--- now sf ---# urnrd_gw_sf Simple feature collection with 18672 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146 projected CRS: WGS 84 / UTM zone 14N First 10 features: well_id year vol_af geometry 1 1706 2007 182.566 POINT (245322.3 4542717) 2 2116 2007 46.328 POINT (245620.9 4541125) 3 2583 2007 38.380 POINT (245660.9 4542523) 4 2597 2007 70.133 POINT (244816.2 4541143) 5 3143 2007 135.870 POINT (243614 4541579) 6 5017 2007 196.799 POINT (243539.9 4543146) 7 1706 2008 171.250 POINT (245322.3 4542717) 8 2116 2008 171.650 POINT (245620.9 4541125) 9 2583 2008 46.100 POINT (245660.9 4542523) 10 2597 2008 124.830 POINT (244816.2 4541143) Figure 1.2: Spatial distribution of irrigation wells Here are the rest of the steps we will take to obtain a regression-ready dataset for our analysis. download groundwater level data observed at USGS monitoring wells from National Water Information System (NWIS) using the dataRetrieval package identify the irrigation wells located within the 2-mile radius of the USGS wells and calculate the total groundwater pumping that occurred around each of the USGS wells by year merge the groundwater pumping data to the groundwater level data Let’s download groundwater level data from NWIS first. The following code downloads groundwater level data for Nebraska from Jan 1, 1990, through Jan 1, 2016. #--- download groundwater level data ---# NE_gwl &lt;- readNWISdata( stateCd=&quot;Nebraska&quot;, startDate = &quot;1990-01-01&quot;, endDate = &quot;2016-01-01&quot;, service = &quot;gwlevels&quot; ) %&gt;% dplyr::select(site_no, lev_dt, lev_va) %&gt;% rename(date = lev_dt, dwt = lev_va) #--- take a look ---# head(NE_gwl, 10) site_no date dwt 1 400008097545301 2000-11-08 17.40 2 400008097545301 2008-10-09 13.99 3 400008097545301 2009-04-09 11.32 4 400008097545301 2009-10-06 15.54 5 400008097545301 2010-04-12 11.15 6 400008100050501 1990-03-15 24.80 7 400008100050501 1990-10-04 27.20 8 400008100050501 1991-03-08 24.20 9 400008100050501 1991-10-07 26.90 10 400008100050501 1992-03-02 24.70 site_no is the unique monitoring well identifier, date is the date of groundwater level monitoring, and dwt is depth to water table. We calculate the average groundwater level in March by USGS monitoring well (right before the irrigation season starts):13 #--- Average depth to water table in March ---# NE_gwl_march &lt;- NE_gwl %&gt;% mutate( date = as.Date(date), month = month(date), year = year(date), ) %&gt;% #--- select observation in March ---# filter(year &gt;= 2007, month == 3) %&gt;% #--- gwl average in March ---# group_by(site_no, year) %&gt;% summarize(dwt = mean(dwt)) #--- take a look ---# head(NE_gwl_march, 10) # A tibble: 10 x 3 # Groups: site_no [2] site_no year dwt &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400032101022901 2008 118. 2 400032101022901 2009 117. 3 400032101022901 2010 118. 4 400032101022901 2011 118. 5 400032101022901 2012 118. 6 400032101022901 2013 118. 7 400032101022901 2014 116. 8 400032101022901 2015 117. 9 400038099244601 2007 24.3 10 400038099244601 2008 21.7 Since NE_gwl is missing geographic coordinates for the monitoring wells, we will download them using the readNWISsite() function and select only the monitoring wells that are inside the three counties. #--- get the list of site ids ---# NE_site_ls &lt;- NE_gwl$site_no %&gt;% unique() #--- get the locations of the site ids ---# sites_info &lt;- readNWISsite(siteNumbers = NE_site_ls) %&gt;% dplyr::select(site_no, dec_lat_va, dec_long_va) %&gt;% #--- turn the data into an sf object ---# st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;)) %&gt;% #--- NAD 83 ---# st_set_crs(4269) %&gt;% #--- project to WGS UTM 14 ---# st_transform(32614) %&gt;% #--- keep only those located inside the three counties ---# .[three_counties, ] We now identify irrigation wells that are located within the 2-mile radius of the monitoring wells14. We first create polygons of 2-mile radius circles around the monitoring wells (see Figure 1.3). buffers &lt;- st_buffer(sites_info, dist = 2*1609.34) # in meter Figure 1.3: 2-mile buffers around USGS monitoring wells We now identify which irrigation wells are inside each of the buffers and get the associated groundwater pumping values. The st_join() function from the sf package will do the trick. #--- find irrigation wells inside the buffer and calculate total pumping ---# pumping_neaby &lt;- st_join(buffers, urnrd_gw_sf) Let’s take a look at a USGS monitoring well (site_no = \\(400012101323401\\)). filter(pumping_neaby, site_no == 400012101323401, year == 2010) Simple feature collection with 7 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444 projected CRS: WGS 84 / UTM zone 14N site_no well_id year vol_af geometry 9.3 400012101323401 6331 2010 NA POLYGON ((286128 4431225, 2... 9.24 400012101323401 1883 2010 180.189 POLYGON ((286128 4431225, 2... 9.25 400012101323401 2006 2010 79.201 POLYGON ((286128 4431225, 2... 9.26 400012101323401 2538 2010 68.205 POLYGON ((286128 4431225, 2... 9.27 400012101323401 2834 2010 NA POLYGON ((286128 4431225, 2... 9.28 400012101323401 2834 2010 122.981 POLYGON ((286128 4431225, 2... 9.29 400012101323401 4983 2010 NA POLYGON ((286128 4431225, 2... As you can see, this well has seven irrigation wells within its 2-mile radius in 2010. Now, we will get total nearby pumping by monitoring well and year. ( total_pumping_nearby &lt;- pumping_neaby %&gt;% #--- calculate total pumping by monitoring well ---# group_by(site_no, year) %&gt;% summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %&gt;% #--- NA means 0 pumping ---# mutate( nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping) ) ) Simple feature collection with 2396 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 237904.5 ymin: 4428006 xmax: 313476.5 ymax: 4545687 projected CRS: WGS 84 / UTM zone 14N # A tibble: 2,396 x 4 # Groups: site_no [401] site_no year nearby_pumping geometry * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;POLYGON [m]&gt; 1 4000121013… 2007 571. ((286128 4431225, 286123.6 4431057, 286110.… 2 4000121013… 2008 772. ((286128 4431225, 286123.6 4431057, 286110.… 3 4000121013… 2009 500. ((286128 4431225, 286123.6 4431057, 286110.… 4 4000121013… 2010 451. ((286128 4431225, 286123.6 4431057, 286110.… 5 4000121013… 2011 545. ((286128 4431225, 286123.6 4431057, 286110.… 6 4000121013… 2012 1028. ((286128 4431225, 286123.6 4431057, 286110.… 7 4001301013… 2007 485. ((278847.4 4433844, 278843 4433675, 278829.… 8 4001301013… 2008 515. ((278847.4 4433844, 278843 4433675, 278829.… 9 4001301013… 2009 351. ((278847.4 4433844, 278843 4433675, 278829.… 10 4001301013… 2010 374. ((278847.4 4433844, 278843 4433675, 278829.… # … with 2,386 more rows We now merge nearby pumping data to the groundwater level data, and transform the data to obtain the dataset ready for regression analysis. #--- regression-ready data ---# reg_data &lt;- NE_gwl_march %&gt;% #--- pick monitoring wells that are inside the three counties ---# filter(site_no %in% unique(sites_info$site_no)) %&gt;% #--- merge with the nearby pumping data ---# left_join(., total_pumping_nearby, by = c(&quot;site_no&quot;, &quot;year&quot;)) %&gt;% #--- lead depth to water table ---# arrange(site_no, year) %&gt;% group_by(site_no) %&gt;% mutate( #--- lead depth ---# dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year), #--- first order difference in dwt ---# dwt_dif = dwt_lead1 - dwt ) #--- take a look ---# dplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping) # A tibble: 2,022 x 4 # Groups: site_no [230] site_no year dwt_dif nearby_pumping &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400130101374401 2011 NA 358. 2 400134101483501 2007 2.87 2038. 3 400134101483501 2008 0.78 2320. 4 400134101483501 2009 -2.45 2096. 5 400134101483501 2010 3.97 2432. 6 400134101483501 2011 1.84 2634. 7 400134101483501 2012 -1.35 985. 8 400134101483501 2013 44.8 NA 9 400134101483501 2014 -26.7 NA 10 400134101483501 2015 NA NA # … with 2,012 more rows Finally, we estimate the model using felm() from the lfe package (see (Gaure 2013) for how it works). #--- OLS with site_no and year FEs (error clustered by site_no) ---# reg_dwt &lt;- felm(dwt_dif ~ nearby_pumping | site_no + year | 0 | site_no, data = reg_data) Here is the regression result. stargazer(reg_dwt, type = &quot;html&quot;) Dependent variable: dwt_dif nearby_pumping 0.001*** (0.0001) Observations 1,342 R2 0.409 Adjusted R2 0.286 Residual Std. Error 1.493 (df = 1111) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 References "],
["demonstration-2-precision-agriculture.html", "1.2 Demonstration 2: Precision Agriculture", " 1.2 Demonstration 2: Precision Agriculture .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.2.1 Project Overview Objectives: Understand the impact of nitrogen on corn yield Understand how electric conductivity (EC) affects the marginal impact of nitrogen on corn Datasets: The experimental design of an on-farm randomized nitrogen trail on an 80-acre field Data generated by the experiment As-applied nitrogen rate Yield measures Electric conductivity Econometric Model: Here is the econometric model, we would like to estimate: \\[ yield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i \\] where \\(yield_i\\), \\(N_i\\), \\(EC_i\\), and \\(v_i\\) are corn yield, nitrogen rate, EC, and error term at subplot \\(i\\). Subplots which are obtained by dividing experimental plots into six of equal-area compartments. GIS tasks read spatial data in various formats: R data set (rds), shape file, and GeoPackage file use sf::st_read() create maps using the ggplot2 package use ggplot2::geom_sf() create subplots within experimental plots user-defined function that makes use of st_geometry() identify corn yield, as-applied nitrogen, and electric conductivity (EC) data points within each of the experimental plots and find their averages use sf::st_join() and sf::aggregate() Preparation for replication Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( sf, # vector data operations dplyr, # data wrangling ggplot2, # for map creation stargazer, # regression table generation patchwork # arrange multiple plots ) Run the following code to define the theme for map: theme_for_map &lt;- theme( axis.ticks = element_blank(), axis.text= element_blank(), axis.line = element_blank(), panel.border = element_blank(), panel.grid = element_line(color=&#39;transparent&#39;), panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,color=&#39;transparent&#39;) ) 1.2.2 Project Demonstration We have already run a whole-field randomized nitrogen experiment on a 80-acre field. Let’s import the trial design data #--- read the trial design data ---# trial_design_16 &lt;- readRDS(&quot;./Data/trial_design.rds&quot;) Figure 1.4 is the map of the trial design generated using ggplot2 package. #--- map of trial design ---# ggplot(data = trial_design_16) + geom_sf(aes(fill = factor(NRATE))) + scale_fill_brewer(name = &quot;N&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map Figure 1.4: The Experimental Design of the Randomize Nitrogen Trial We have collected yield, as-applied NH3, and EC data. Let’s read in these datasets:15 #--- read yield data (sf data saved as rds) ---# yield &lt;- readRDS(&quot;./Data/yield.rds&quot;) #--- read NH3 data (GeoPackage data) ---# NH3_data &lt;- st_read(&quot;Data/NH3.gpkg&quot;) #--- read ec data (shape file) ---# ec &lt;- st_read(dsn=&quot;Data&quot;, &quot;ec&quot;) Figure 1.5 shows the spatial distribution of the three variables. A map of each variable was made first, and then they are combined into one figure using the patchwork package16. #--- yield map ---# g_yield &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = yield, aes(color = yield), size = 0.5) + scale_color_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_NH3 &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) + scale_color_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_ec &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = ec, aes(color = ec), size = 0.5) + scale_color_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- stack the figures vertically and display (enabled by the patchwork package) ---# g_yield/g_NH3/g_ec Figure 1.5: Spatial distribution of yield, NH3, and EC Instead of using plot as the observation unit, we would like to create subplots inside each of the plots and make them the unit of analysis because it would avoid masking the within-plot spatial heterogeneity of EC. Here, we divide each plot into six subplots. The following function generate subplots by supplying a trial design and the number of subplots you would like to create within each plot: gen_subplots &lt;- function(plot, num_sub) { #--- extract geometry information ---# geom_mat &lt;- st_geometry(plot)[[1]][[1]] #--- upper left ---# top_start&lt;- (geom_mat[2,]) #--- upper right ---# top_end&lt;- (geom_mat[3,]) #--- lower right ---# bot_start&lt;- (geom_mat[1,]) #--- lower left ---# bot_end&lt;- (geom_mat[4,]) top_step_vec &lt;- (top_end-top_start)/num_sub bot_step_vec &lt;- (bot_end-bot_start)/num_sub # create a list for the sub-grid subplots_ls &lt;- list() for (j in 1:num_sub){ rec_pt1 &lt;- top_start + (j-1)*top_step_vec rec_pt2 &lt;- top_start + j*top_step_vec rec_pt3 &lt;- bot_start + j*bot_step_vec rec_pt4 &lt;- bot_start + (j-1)*bot_step_vec rec_j &lt;- rbind(rec_pt1,rec_pt2,rec_pt3,rec_pt4,rec_pt1) temp_quater_sf &lt;- list(st_polygon(list(rec_j))) %&gt;% st_sfc(.) %&gt;% st_sf(., crs = 26914) subplots_ls[[j]] &lt;- temp_quater_sf } return(do.call(&#39;rbind&#39;,subplots_ls)) } Let’s run the function to create six subplots within each of the experimental plots. #--- generate subplots ---# subplots &lt;- lapply( 1:nrow(trial_design_16), function(x) gen_subplots(trial_design_16[x, ], 6) ) %&gt;% do.call(&#39;rbind&#39;, .) Figure 1.6 is a map of the subplots generated. #--- here is what subplots look like ---# ggplot(subplots) + geom_sf() + theme_for_map Figure 1.6: Map of the subplots We now identify the mean value of corn yield, nitrogen rate, and EC for each of the subplots using sf::aggregate() and sf::st_join(). ( reg_data &lt;- subplots %&gt;% #--- yield ---# st_join(., aggregate(yield, ., mean), join = st_equals) %&gt;% #--- nitrogen ---# st_join(., aggregate(NH3_data, ., mean), join = st_equals) %&gt;% #--- EC ---# st_join(., aggregate(ec, ., mean), join = st_equals) ) Simple feature collection with 816 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734 projected CRS: NAD83 / UTM zone 14N First 10 features: yield aa_NH3 ec geometry 1 220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,... 2 218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,... 3 220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,... 4 215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,... 5 216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,... 6 227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,... 7 226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,... 8 225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,... 9 221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5... 10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,... Here are the visualization of the subplot-level data (Figure 1.7): (ggplot() + geom_sf(data = reg_data, aes(fill = yield), color = NA) + scale_fill_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) + scale_fill_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = ec), color = NA) + scale_fill_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map) Figure 1.7: Spatial distribution of subplot-level yield, NH3, and EC Let’s estimate the model and see the results: lm(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3*ec) + I(aa_NH3^2*ec), data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: yield aa_NH3 -1.223 (1.308) I(aa_NH32) 0.004 (0.003) I(aa_NH3 * ec) 0.002 (0.003) I(aa_NH32 * ec) -0.00001 (0.00002) Constant 327.993*** (125.638) Observations 784 R2 0.010 Adjusted R2 0.005 Residual Std. Error 5.712 (df = 779) F Statistic 2.023* (df = 4; 779) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Here we are demonstrating that R can read spatial data in different formats. R can read spatial data of many other formats. Here, we are reading a shapefile (.shp) and GeoPackage file (.gpkg).↩︎ Here is its github page. See the bottom of the page to find vignettes.↩︎ "],
["demo3.html", "1.3 Demonstration 3: Land Use and Weather", " 1.3 Demonstration 3: Land Use and Weather .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.3.1 Project Overview Objective Understand the impact of past precipitation on crop choice in Iowa (IA). Datasets IA county boundary Regular grids over IA, created using sf::st_make_grid() PRISM daily precipitation data downloaded using prism package Land use data from the Cropland Data Layer (CDL) for IA in 2015, downloaded using cdlTools package Econometric Model The econometric model we would like to estimate is: \\[ CS_i = \\alpha + \\beta_1 PrN_{i} + \\beta_2 PrC_{i} + v_i \\] where \\(CS_i\\) is the area share of corn divided by that of soy in 2015 for grid \\(i\\) (we will generate regularly-sized grids in the Demo section), \\(PrN_i\\) is the total precipitation observed in April through May and September in 2014, \\(PrC_i\\) is the total precipitation observed in June through August in 2014, and \\(v_i\\) is the error term. To run the econometric model, we need to find crop share and weather variables observed at the grids. We first tackle the crop share variable, and then the precipitation variable. GIS tasks download Cropland Data Layer (CDL) data by USDA NASS use cdlTools::getCDL() download PRISM weather data use prism::get_prism_dailys() crop PRISM data to the geographic extent of IA use raster::crop() create regular grids within IA, which become the observation units of the econometric analysis use sf::st_make_grid() remove grids that share small area with IA use sf::st_intersection() and sf::st_area assign crop share and weather data to each of the generated IA grids (parallelized) use exactextractr::exact_extract() and future.apply::future_lapply() create maps use the tmap package Preparation for replication Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( sf, # vector data operations raster, # raster data operations exactextractr, # fast raster data extraction for polygons maps, # to get county boundary data data.table, # data wrangling dplyr, # data wrangling lubridate, # Date object handling tmap, # for map creation stargazer, # regression table generation future.apply, # parallel computation cdlTools, # download CDL data rgdal, # required for cdlTools prism, # download PRISM data stringr # string manipulation ) 1.3.2 Project Demonstration The geographic focus of this project is Iowas. Let’s get Iowa state border (see Figure 1.8 for its map). #--- IA state boundary ---# IA_boundary &lt;- st_as_sf(maps::map(&quot;state&quot;, &quot;iowa&quot;, plot = FALSE, fill = TRUE)) Figure 1.8: Iowa state boundary The unit of analysis is artificial grids that we create over Iowa. The grids are regularly-sized rectangles except around the edge of the Iowa state border17. So, let’s create grids and remove those that do not overlap much with Iowa. #--- create regular grids (40 cells by 40 columns) over IA ---# IA_grids &lt;- IA_boundary %&gt;% #--- create grids ---# st_make_grid(, n = c(40, 40)) %&gt;% #--- convert to sf ---# st_as_sf() %&gt;% #--- find the intersections of IA grids and IA polygon ---# st_intersection(., IA_boundary) %&gt;% #--- calculate the area of each grid ---# mutate( area = as.numeric(st_area(.)), area_ratio = area/max(area) ) %&gt;% #--- keep only if the intersected area is large enough ---# filter(area_ratio &gt; 0.8) %&gt;% #--- assign grid id for future merge ---# mutate(grid_id = 1:nrow(.)) Here is what the generated grids look like (Figure 1.9): #--- plot the grids over the IA state border ---# tm_shape(IA_boundary) + tm_polygons(col = &quot;green&quot;) + tm_shape(IA_grids) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 1.9: Map of regular grids generated over IA Let’s work on crop share data. You can download CDL data using the getCDL() function from the cdlTools package. #--- download the CDL data for IA in 2015 ---# ( IA_cdl_2015 &lt;- getCDL(&quot;Iowa&quot;, 2015)$IA2015 ) The cells (30 meter by 30 meter) of the imported raster layer take a value ranging from 0 to 255. Corn and soybean are represented by 1 and 5, respectively (Figure 1.10). Figure 1.10 shows the map of one of the IA grids and the CDL cells it overlaps with. Figure 1.10: Spatial overlap of a IA grid and CDL layer We would like to extract all the cell values within the blue border. We use exactextractr::exact_extract() to identify which cells of the CDL raster layer fall within each of the IA grids and extract land use type values. We then find the share of corn and soybean for each of the grids. #--- reproject grids to the CRS of the CDL data ---# IA_grids_rp_cdl &lt;- st_transform(IA_grids, projection(IA_cdl_2015)) #--- extract crop type values and find frequencies ---# cdl_extracted &lt;- exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %&gt;% lapply(., function (x) data.table(x)[,.N, by = value]) %&gt;% #--- combine the list of data.tables into one data.table ---# rbindlist(idcol = TRUE) %&gt;% #--- find the share of each land use type ---# .[, share := N/sum(N), by = .id] %&gt;% .[, N := NULL] %&gt;% #--- keep only the share of corn and soy ---# .[value %in% c(1, 5), ] We then find the corn to soy ratio for each of the IA grids. #--- find corn/soy ratio ---# corn_soy &lt;- cdl_extracted %&gt;% #--- long to wide ---# dcast(.id ~ value, value.var = &quot;share&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;.id&quot;, &quot;1&quot;, &quot;5&quot;), c(&quot;grid_id&quot;, &quot;corn_share&quot;, &quot;soy_share&quot;)) %&gt;% #--- corn share divided by soy share ---# .[, c_s_ratio := corn_share / soy_share] We are still missing daily precipitation data at the moment. We have decided to use daily weather data from PRISM. Daily PRISM data is a raster data with the cell size of 4 km by 4 km. Figure 1.11 presents precipitation data downloaded for April 1, 2010. It covers the entire contiguous U.S. Figure 1.11: Map of PRISM raster data layer Let’s now download PRISM data18. This can be done using the get_prism_dailys() function from the prism package.19 options(prism.path = &quot;./Data/PRISM&quot;) get_prism_dailys( type = &quot;ppt&quot;, minDate = &quot;2014-04-01&quot;, maxDate = &quot;2014-09-30&quot;, keepZip = FALSE ) When we use get_prism_dailys() to download data20, it creates one folder for each day. So, I have about 180 folders inside the folder I designated as the download destination above with the options() function. We now try to extract precipitation value by day for each of the IA grids by geographically overlaying IA grids onto the PRISM data layer and identify which PRISM cells each of the IA grid encompass. Figure 1.12 shows how the first IA grid overlaps with the PRISM cells21. #--- read a PRISM dataset ---# prism_whole &lt;- raster(&quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot;) #--- align the CRS ---# IA_grids_rp_prism &lt;- st_transform(IA_grids, projection(prism_whole)) #--- crop the PRISM data for the 1st IA grid ---# PRISM_1 &lt;- crop(prism_whole, st_buffer(IA_grids_rp_prism[1, ], dist = 0.05)) #--- map them ---# tm_shape(PRISM_1) + tm_raster() + tm_shape(IA_grids_rp_prism[1, ]) + tm_polygons(alpha = 0) + tm_layout(frame = NA) Figure 1.12: Spatial overlap of an IA grid over PRISM cells As you can see, some PRISM grids are fully inside the analysis grid, while others are partially inside it. So, when assigning precipitation values to grids, we will use the coverage-weighted mean of precipitations22. Unlike the CDL layer, we have 183 raster layers to process. Fortunately, we can process many raster files at the same time very quickly by first “stacking” many raster files first and then applying the exact_extract() function. Using future_lapply(), we let \\(6\\) cores take care of this task with each processing 31 files, except one of them handling only 28 files.23 We first get all the paths to the PRISM files. #--- get all the dates ---# dates_ls &lt;- seq(as.Date(&quot;2014-04-01&quot;), as.Date(&quot;2014-09-30&quot;), &quot;days&quot;) #--- remove hyphen ---# dates_ls_no_hyphen &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- get all the prism file names ---# folder_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil&quot;) file_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil.bil&quot;) file_paths &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- take a look ---# head(file_paths) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot; [2] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil&quot; [3] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil&quot; [4] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil&quot; [5] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil&quot; [6] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil&quot; We now prepare for parallelized extractions and then implement them using future_apply() (you can have a look at Chapter A to familiarize yourself with parallel computation using the future.apply package). #--- define the number of cores to use ---# num_core &lt;- 6 #--- prepare some parameters for parallelization ---# file_len &lt;- length(file_paths) files_per_core &lt;- ceiling(file_len/num_core) #--- prepare for parallel processing ---# plan(multiprocess, workers = num_core) #--- reproject IA grids to the CRS of PRISM data ---# IA_grids_reprojected &lt;- st_transform(IA_grids, projection(prism_whole)) Here is the function that we run in parallel over 6 cores. #--- define the function to extract PRISM values by block of files ---# extract_by_block &lt;- function(i, files_per_core) { #--- files processed by core ---# start_file_index &lt;- (i-1) * files_per_core + 1 #--- indexes for files to process ---# file_index &lt;- seq( from = start_file_index, to = min((start_file_index + files_per_core), file_len), by = 1 ) #--- extract values ---# data_temp &lt;- file_paths[file_index] %&gt;% # get file names #--- stack files ---# stack() %&gt;% #--- extract ---# exact_extract(., IA_grids_reprojected) %&gt;% #--- combine into one data set ---# rbindlist(idcol = &quot;ID&quot;) %&gt;% #--- wide to long ---# melt(id.var = c(&quot;ID&quot;, &quot;coverage_fraction&quot;)) %&gt;% #--- calculate &quot;area&quot;-weighted mean ---# .[, .(value = sum(value * coverage_fraction)/sum(coverage_fraction)), by = .(ID, variable)] return(data_temp) } Now, let’s run the function in parallel and calculate precipitation by period. #--- run the function ---# precip_by_period &lt;- future_lapply(1:num_core, function(x) extract_by_block(x, files_per_core)) %&gt;% rbindlist() %&gt;% #--- recover the date ---# .[, variable := as.Date(str_extract(variable, &quot;[0-9]{8}&quot;), &quot;%Y%m%d&quot;)] %&gt;% #--- change the variable name to date ---# setnames(&quot;variable&quot;, &quot;date&quot;) %&gt;% #--- define critical period ---# .[,critical := &quot;non_critical&quot;] %&gt;% .[month(date) %in% 6:8, critical := &quot;critical&quot;] %&gt;% #--- total precipitation by critical dummy ---# .[, .(precip=sum(value)), by = .(ID, critical)] %&gt;% #--- wide to long ---# dcast(ID ~ critical, value.var = &quot;precip&quot;) We now have grid-level crop share and precipitation data. Let’s merge them and run regression.24 #--- crop share ---# reg_data &lt;- corn_soy[precip_by_period, on = c(grid_id = &quot;ID&quot;)] #--- OLS ---# reg_results &lt;- lm(c_s_ratio ~ critical + non_critical, data = reg_data) Here is the regression results table. #--- regression table ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: c_s_ratio critical -0.002*** (0.0003) non_critical -0.0003 (0.0003) Constant 2.701*** (0.161) Observations 1,218 R2 0.058 Adjusted R2 0.056 Residual Std. Error 0.743 (df = 1215) F Statistic 37.234*** (df = 2; 1215) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Again, do not read into the results as the econometric model is terrible. We by no means are saying that this is the right geographical unit of analysis. This is just about demonstrating how R can be used for analysis done at the higher spatial resolution than county.↩︎ You do not have to run this code to download the data. It is included in the data folder for replication (here).↩︎ prism github page↩︎ For this project, I could have just used monthly PRISM data, which can be downloaded using the get_prism_monthlys() function. But, in many applications, daily data is necessary, so I wanted to illustrate how to download and process them.↩︎ Do not use st_buffer() for spatial objects in geographic coordinates (latitude, longitude) if you intend to use the created buffers for any serious IA (it is difficult to get the right distance parameter anyway.). Significant distortion will be introduced to the buffer due to the fact that one degree in latitude and longitude means different distances at the latitude of IA. Here, I am just creating a buffer to extract PRISM cells to display on the map.↩︎ In practice, this may not be advisable. The coverage fraction calculation by exact_extract() is done using latitude and longitude. Therefore, the relative magnitude of the fraction numbers incorrectly reflects the actual relative magnitude of the overlapped area. When the spatial resolution of the sources grids (grids from which you extract values) is much smaller relative to that of the target grids (grids to which you assign values to), then a simple average would be very similar to a coverage-weighted mean. For example, CDL consists of 30m by 30m grids, and more than \\(1,000\\) grids are inside one analysis grid.↩︎ Parallelization of extracting values from many raster layers for polygons are discussed in much more detail in Chapter 6. When I tried stacking all 183 files into one stack and applying exact_extract, it did not finish the job after over five minutes. So, I terminated the process in the middle. The parallelized version gets the job done in about \\(30\\) seconds on my desktop.↩︎ We can match on grid_id from corn_soy and ID from “precip_by_period” because grid_id is identical with the row number and ID variables were created so that the ID value of \\(i\\) corresponds to \\(i\\) th row of IA_grids.↩︎ "],
["demo4.html", "1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage", " 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.4.1 Project Overview Objective Understand the impact of railroad on corn planted acreage in Illinois Datasets USDA corn planted acreage for Illinois downloaded from the USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA package US railroads (line data) downloaded from here Econometric Model We will estimate the following model: \\[ y_i = \\beta_0 + \\beta_1 RL_i + v_i \\] where \\(y_i\\) is corn planted acreage in county \\(i\\) in Illinois, \\(RL_i\\) is the total length of railroad, and \\(v_i\\) is the error term. GIS tasks Download USDA corn planted acreage by county as a spatial dataset (sf object) use tidyUSDA::getQuickStat() Import US railroad shape file as a spatial dataset (sf object) use sf:st_read() Spatially subset (crop) the railroad data to the geographic boundary of Illinois use sf_1[sf_2, ] Find railroads for each county (cross-county railroad will be chopped into pieces for them to fit within a single county) use sf::st_intersection() Calculate the travel distance of each railroad piece use sf::st_length() create maps using the ggplot2 package use ggplot2::geom_sf() Preparation for replication Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( tidyUSDA, # access USDA NASS data sf, # vector data operations dplyr, # data wrangling ggplot2, # for map creation stargazer, # regression table generation keyring # API management ) Run the following code to define the theme for map: theme_for_map &lt;- theme( axis.ticks = element_blank(), axis.text= element_blank(), axis.line = element_blank(), panel.border = element_blank(), panel.grid.major = element_line(color=&#39;transparent&#39;), panel.grid.minor = element_line(color=&#39;transparent&#39;), panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,color=&#39;transparent&#39;) ) 1.4.2 Project Demonstration We first download corn planted acreage data for 2018 from USDA NASS QuickStat service using tidyUSDA package25. ( IL_corn_planted &lt;- getQuickstat( #--- use your own API key here fore replication ---# key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, data_item = &quot;CORN - ACRES PLANTED&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2018&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 90 features and 5 fields (with 6 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2018 Bureau 011 CORN - ACRES PLANTED 264000 2 2018 Carroll 015 CORN - ACRES PLANTED 134000 3 2018 Henry 073 CORN - ACRES PLANTED 226500 4 2018 Jo Daviess 085 CORN - ACRES PLANTED 98500 5 2018 Lee 103 CORN - ACRES PLANTED 236500 6 2018 Mercer 131 CORN - ACRES PLANTED 141000 7 2018 Ogle 141 CORN - ACRES PLANTED 217000 8 2018 Putnam 155 CORN - ACRES PLANTED 32300 9 2018 Rock Island 161 CORN - ACRES PLANTED 68400 10 2018 Stephenson 177 CORN - ACRES PLANTED 166500 geometry 1 MULTIPOLYGON (((-89.8569 41... 2 MULTIPOLYGON (((-90.16133 4... 3 MULTIPOLYGON (((-90.43227 4... 4 MULTIPOLYGON (((-90.50668 4... 5 MULTIPOLYGON (((-89.63118 4... 6 MULTIPOLYGON (((-90.99255 4... 7 MULTIPOLYGON (((-89.68598 4... 8 MULTIPOLYGON (((-89.33303 4... 9 MULTIPOLYGON (((-90.33573 4... 10 MULTIPOLYGON (((-89.9205 42... A nice thing about this function is that the data is downloaded as an sf object with county geometry with geometry = TRUE. So, you can immediately plot it (Figure 1.13) and use it for later spatial interactions without having to merge the downloaded data to an independent county boundary data.26 ggplot(IL_corn_planted) + geom_sf(aes(fill = Value/1000)) + scale_fill_distiller(name = &quot;Planted Acreage (1000 acres)&quot;, palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map Figure 1.13: Map of Con Planted Acreage in Illinois in 2018 Let’s import the U.S. railroad data and reproject to the CRS of IL_corn_planted: rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% #--- reproject to the CRS of IL_corn_planted ---# st_transform(st_crs(IL_corn_planted)) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 geographic CRS: NAD83 Here is what it looks like: ggplot(rail_roads) + geom_sf() + theme_for_map Figure 1.14: Map of Railroads We now crop it to the Illinois state border (Figure 1.15) using sf_1[sf_2, ]: rail_roads_IL &lt;- rail_roads[IL_corn_planted, ] ggplot() + geom_sf(data = rail_roads_IL) + theme_for_map Figure 1.15: Map of railroads in Illinois Let’s now find railroads for each county, where cross-county railroads will be chopped into pieces so each piece fits completely within a single county, using st_intersection(). rails_IL_segmented &lt;- st_intersection(rail_roads_IL, IL_corn_planted) Here are the railroads for Richland County: ggplot() + geom_sf(data = dplyr::filter(IL_corn_planted, NAME == &quot;Richland&quot;)) + geom_sf(data = dplyr::filter(rails_IL_segmented, NAME == &quot;Richland&quot;), aes( color = LINEARID )) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map We now calculate the travel distance (Great-circle distance) of each railroad piece using st_length() and then sum them up by county to find total railroad length by county. ( rail_length_county &lt;- mutate( rails_IL_segmented, length_in_m = as.numeric(st_length(rails_IL_segmented)), ) %&gt;% #--- group by county ID ---# group_by(county_code) %&gt;% #--- sum rail length by county ---# summarize(length_in_m = sum(length_in_m)) %&gt;% #--- geometry no longer needed ---# st_drop_geometry() ) # A tibble: 82 x 2 county_code length_in_m * &lt;chr&gt; &lt;dbl&gt; 1 001 77221. 2 003 77290. 3 007 36764. 4 011 255441. 5 015 161726. 6 017 30585. 7 019 389226. 8 021 155794. 9 023 78587. 10 025 92030. # … with 72 more rows We merge the railroad length data to the corn planted acreage data and estimate the model. reg_data &lt;- left_join(IL_corn_planted, rail_length_county, by = &quot;county_code&quot;) lm(Value ~ length_in_m, data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: Value length_in_m 0.092* (0.047) Constant 108,154.800*** (11,418.900) Observations 82 R2 0.046 Adjusted R2 0.034 Residual Std. Error 69,040.680 (df = 80) F Statistic 3.866* (df = 1; 80) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 In order to actually download the data, you need to obtain the API key here. Once the API key was obtained, I stored it using set_key() from the keyring package, which was named “usda_nass_qs_api”. In the code to the left, I retrieve the API key using key_get(\"usda_nass_qs_api\") in the code. For your replication, replace key_get(\"usda_nass_qs_api\") with your own API key.↩︎ theme_for_map is a user defined object that defines the theme of figures generated using ggplot2 for this section. You can find it in Chap_1_Demonstration.R.↩︎ "],
["demonstration-5-groundwater-use-for-agricultural-irrigation.html", "1.5 Demonstration 5: Groundwater use for agricultural irrigation", " 1.5 Demonstration 5: Groundwater use for agricultural irrigation .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.5.1 Project Overview Objective Understand the impact of monthly precipitation on groundwater use for agricultural irrigation Datasets Annual groundwater pumping by irrigation wells in Kansas for 2010 and 2011 (originally obtained from the Water Information Management &amp; Analysis System (WIMAS) database) Daymet27 daily precipitation and maximum temperature downloaded using daymetr package Econometric Model The econometric model we would like to estimate is: \\[ y_{i,t} = \\alpha + P_{i,t} \\beta + T_{i,t} \\gamma + \\phi_i + \\eta_t + v_{i,t} \\] where \\(y\\) is the total groundwater extracted in year \\(t\\), \\(P_{i,t}\\) and \\(T_{i,t}\\) is the collection of monthly total precipitation and mean maximum temperature April through September in year \\(t\\), respectively, \\(\\phi_i\\) is the well fixed effect, \\(\\eta_t\\) is the year fixed effect, and \\(v_{i,t}\\) is the error term. GIS tasks download Daymet precipitation and maximum temperature data for each well from within R in parallel use daymetr::download_daymet() and future.apply::future_lapply() Preparation for replication Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( daymetr, # get Daymet data sf, #vector data operations dplyr, #data wrangling data.table, #data wrangling ggplot2, #for map creation RhpcBLASctl, #to get the number of available cores future.apply, #parallelization lfe, #fast regression with many fixed effects stargazer #regression table generation ) 1.5.2 Project Demonstration We have already collected annual groundwater pumping data by irrigation wells in 2010 and 2011 in Kansas from the Water Information Management &amp; Analysis System (WIMAS) database. Let’s read in the groundwater use data. #--- read in the data ---# ( gw_KS_sf &lt;- readRDS( &quot;./Data/gw_KS_sf.rds&quot;) ) Simple feature collection with 56225 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191 geographic CRS: NAD83 First 10 features: well_id year af_used geometry 1 1 2010 67.00000 POINT (-100.4423 37.52046) 2 1 2011 171.00000 POINT (-100.4423 37.52046) 3 3 2010 30.93438 POINT (-100.7118 39.91526) 4 3 2011 12.00000 POINT (-100.7118 39.91526) 5 7 2010 0.00000 POINT (-101.8995 38.78077) 6 7 2011 0.00000 POINT (-101.8995 38.78077) 7 11 2010 154.00000 POINT (-101.7114 39.55035) 8 11 2011 160.00000 POINT (-101.7114 39.55035) 9 12 2010 28.17239 POINT (-95.97031 39.16121) 10 12 2011 89.53479 POINT (-95.97031 39.16121) We have 28553 wells in total, and each well has records of groundwater pumping (af_used) for years 2010 and 2011. Here is the spatial distribution of the wells. KS_counties &lt;- readRDS(&quot;./Data/KS_county_borders.rds&quot;) tm_shape(KS_counties) + tm_polygons() + tm_shape(gw_KS_sf) + tm_symbols(size = 0.05, col = &quot;black&quot;) We now need to get monthly precipitation and maximum temperature data. We have decided that we use Daymet weather data. Here we use the download_daymet() function from the daymetr package28 that allows us to download all the weather variables for a specified geographic location and time period29. We write a wrapper function that downloads Daymet data and then processes it to find monthly total precipitation and mean maximum temperature30. We then loop over the 56225 wells, which is parallelized using the future_apply() function31 from the future.apply package. This process takes about an hour on my Mac with parallelization on 7 cores. The data is available in the data repository for this course (named as “all_daymet.rds”). #--- get the geographic coordinates of the wells ---# well_locations &lt;- gw_KS_sf %&gt;% unique(by = &quot;well_id&quot;) %&gt;% dplyr::select(well_id) %&gt;% cbind(., st_coordinates(.)) #--- define a function that downloads Daymet data by well and process it ---# get_daymet &lt;- function(i) { temp_site &lt;- well_locations[i, ]$well_id temp_long &lt;- well_locations[i, ]$X temp_lat &lt;- well_locations[i, ]$Y data_temp &lt;- download_daymet( site = temp_site, lat = temp_lat, lon = temp_long, start = 2010, end = 2011, #--- if TRUE, tidy data is returned ---# simplify = TRUE, #--- if TRUE, the downloaded data can be assigned to an R object ---# internal = TRUE ) %&gt;% data.table() %&gt;% #--- keep only precip and tmax ---# .[measurement %in% c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), ] %&gt;% #--- recover calender date from Julian day ---# .[, date := as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)] %&gt;% #--- get month ---# .[, month := month(date)] %&gt;% #--- keep only April through September ---# .[month %in% 4:9,] %&gt;% .[, .(site, year, month, date, measurement, value)] %&gt;% #--- long to wide ---# dcast(site + year + month + date~ measurement, value.var = &quot;value&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), c(&quot;prcp&quot;, &quot;tmax&quot;)) %&gt;% #--- find the total precip and mean tmax by month-year ---# .[, .(prcp = sum(prcp), tmax = mean(tmax)) , by = .(month, year)] %&gt;% .[, well_id := temp_site] return(data_temp) gc() } Here is what one run (for the first well) of get_daymet() returns #--- one run ---# ( returned_data &lt;- get_daymet(1)[] ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 6: 9 2010 15 31.43333 1 7: 4 2011 25 21.91667 1 8: 5 2011 26 26.30645 1 9: 6 2011 23 35.16667 1 10: 7 2011 35 38.62903 1 11: 8 2011 37 36.90323 1 12: 9 2011 9 28.66667 1 We get the number of cores you can use by RhpcBLASctl::get_num_procs() and parallelize the loop over wells using future_lapply().32 #--- prepare for parallelization ---# num_cores &lt;- get_num_procs() - 1 # number of cores plan(multiprocess, workers = num_cores) # set up cores #--- run get_daymet with parallelization ---# ( all_daymet &lt;- future_lapply(1:nrow(well_locations), get_daymet) %&gt;% rbindlist() ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 --- 336980: 5 2011 18 26.11290 78051 336981: 6 2011 25 34.61667 78051 336982: 7 2011 6 38.37097 78051 336983: 8 2011 39 36.66129 78051 336984: 9 2011 23 28.45000 78051 Before merging the Daymet data, we need to reshape the data into a wide format to get monthly precipitation and maximum temperature as columns. #--- long to wide ---# daymet_to_merge &lt;- dcast(all_daymet, well_id + year ~ month, value.var = c(&quot;prcp&quot;, &quot;tmax&quot;)) #--- take a look ---# daymet_to_merge well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56160: 78049 2011 27 6 38 37 34 36 22.81667 26.70968 56161: 78050 2010 35 48 68 111 56 9 21.38333 24.85484 56162: 78050 2011 26 7 44 38 34 35 22.76667 26.70968 56163: 78051 2010 30 62 48 29 76 3 21.05000 24.14516 56164: 78051 2011 33 18 25 6 39 23 21.90000 26.11290 tmax_6 tmax_7 tmax_8 tmax_9 1: 32.51667 33.50000 34.17742 31.43333 2: 35.16667 38.62903 36.90323 28.66667 3: 30.73333 32.80645 33.56452 28.93333 4: 30.08333 35.08065 32.90323 25.81667 5: 31.30000 33.12903 32.67742 30.16667 --- 56160: 35.01667 38.32258 36.54839 28.80000 56161: 33.16667 33.88710 34.40323 32.11667 56162: 34.91667 38.32258 36.54839 28.83333 56163: 32.90000 33.83871 34.38710 31.56667 56164: 34.61667 38.37097 36.66129 28.45000 Now, let’s merge the weather data to the groundwater pumping dataset. ( reg_data &lt;- data.table(gw_KS_sf) %&gt;% #--- keep only the relevant variables ---# .[, .(well_id, year, af_used)] %&gt;% #--- join ---# daymet_to_merge[., on = c(&quot;well_id&quot;, &quot;year&quot;)] ) well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56221: 79348 2011 NA NA NA NA NA NA NA NA 56222: 79349 2011 NA NA NA NA NA NA NA NA 56223: 79367 2011 NA NA NA NA NA NA NA NA 56224: 79372 2011 NA NA NA NA NA NA NA NA 56225: 80930 2011 NA NA NA NA NA NA NA NA tmax_6 tmax_7 tmax_8 tmax_9 af_used 1: 32.51667 33.50000 34.17742 31.43333 67.00000 2: 35.16667 38.62903 36.90323 28.66667 171.00000 3: 30.73333 32.80645 33.56452 28.93333 30.93438 4: 30.08333 35.08065 32.90323 25.81667 12.00000 5: 31.30000 33.12903 32.67742 30.16667 0.00000 --- 56221: NA NA NA NA 76.00000 56222: NA NA NA NA 182.00000 56223: NA NA NA NA 0.00000 56224: NA NA NA NA 134.00000 56225: NA NA NA NA 23.69150 Let’s run regression and display the results. #--- run FE ---# reg_results &lt;- felm( af_used ~ prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9 + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |well_id + year| 0 | well_id, data = reg_data ) #--- display regression results ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: af_used prcp_4 -0.053*** (0.017) prcp_5 0.112*** (0.010) prcp_6 -0.073*** (0.008) prcp_7 0.014 (0.010) prcp_8 0.093*** (0.014) prcp_9 -0.177*** (0.025) tmax_4 9.159*** (1.227) tmax_5 -7.505*** (1.062) tmax_6 15.134*** (1.360) tmax_7 3.969** (1.618) tmax_8 3.420*** (1.066) tmax_9 -11.803*** (1.801) Observations 55,754 R2 0.942 Adjusted R2 0.883 Residual Std. Error 46.864 (df = 27659) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 That’s it. Do not bother to try to read into the regression results. Again, this is just an illustration of how R can be used to prepare a regression-ready dataset with spatial variables. Daymet website↩︎ daymetr vignette↩︎ See here for a fuller explanation of how to use the daymetr package.↩︎ This may not be ideal for a real research project because the original raw data is not kept. It is often the case that your econometric plan changes on the course of your project (e.g., using other weather variables or using different temporal aggregation of weather variables instead of monthly aggregation). When this happens, you need to download the same data all over again.↩︎ For parallelized computation, see Chapter A↩︎ For Mac users, mclapply or pbmclapply (mclapply with progress bar) are good alternatives.↩︎ "],
["vector-basics.html", "Chapter 2 Vector Data Handling with sf ", " Chapter 2 Vector Data Handling with sf "],
["before-you-start-1.html", "Before you start", " Before you start In this chapter we learn how to use the sf package to handle and operate on spatial datasets. The sf package uses the class of simple feature (sf)33 for spatial objects in R. We first learn how sf objects store and represent spatial datasets. We then move on to the following practical topics: read and write a shapefile and spatial data in other formats (and why you might not want to use the shapefile system any more, but use other alternative formats) project and reproject spatial objects convert sf objects into sp objects, vice versa confirm that dplyr works well with sf objects implement non-interactive (does not involve two sf objects) geometric operations on sf objects create buffers find the area of polygons find the centroid of polygons calculate the length of lines sf or sp? The sf package was designed to replace the sp package, which has been one of the most popular and powerful spatial packages in R for more than a decade. It has been about four years since the sf package was first registered on CRAN. A couple of years back, many other spatial packages did not have support for the package yet. In this blog post the author responded to the questions of whether one should learn sp or sf saying, \"That’s a tough question. If you have time, I would say, learn to use both. sf is pretty new, so a lot of packages that depend on spatial classes still rely on sp. So you will need to know sp if you want to do any integration with many other packages, including raster (as of March 2018). However, in the future we should see an increasing shift toward the sf package and greater use of sf classes in other packages. I also think that sf is easier to learn to use than sp.\" The future has come, and it’s not a tough question anymore. I cannot think of any major spatial packages that do not support sf package, and sf has largely becomes the standard for handling vector data in \\(R\\)34. Thus, this lecture note does not cover how to use sp at all. sf has several advantages over the sp package (Pebesma 2018).35 First, it cut off the tie that sp had with ESRI shapefile system, which has a somewhat loose way of representing spatial data. Instead, it uses simple feature access, which is an open standard supported by Open Geospatial Consortium (OGC). Another important benefit is its compatibility with the tidyverse package, which includes widely popular packages like ggplot2 and dplyr. Consequently, map-making with ggplot() and data wrangling with a family of dplyr functions come very natural to many \\(R\\) users. sp objects have different slots for spatial information and attributes data, and they are not amenable to dplyr way of data transformation. Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder to replicate demonstrations in Chapter 1, then skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( sf, # vector data operations dplyr, # data wrangling data.table, # data wrangling tmap, # make maps mapview # create an interactive map ) References "],
["spatial-data-structure.html", "2.1 Spatial Data Structure", " 2.1 Spatial Data Structure Here we learn how the sf package stores spatial data along with the definition of three key sf object classes: simple feature geometry (sfg), simple feature geometry list-column (sfc), and simple feature (sf). The sf package provides a simply way of storing geographic information and the attributes of the geographic units in a single dataset. This special type of dataset is called simple feature (sf). It is best to take a look at an example to see how this is achieved. We use North Carolina county boundaries with county attributes (Figure 2.1). #--- a dataset that comes with the sf package ---# nc &lt;- st_read(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)) Reading layer `nc&#39; from data source `/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sf/shape/nc.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 100 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 Figure 2.1: North Carolina county boundary As you can see below, this dataset is of class sf (and data.frame at the same time). class(nc) [1] &quot;sf&quot; &quot;data.frame&quot; Now, let’s take a look inside of nc. #--- take a look at the data ---# head(nc) Simple feature collection with 6 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 geographic CRS: NAD27 AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 1 0.114 1.442 1825 1825 Ashe 37009 37009 5 1091 1 2 0.061 1.231 1827 1827 Alleghany 37005 37005 3 487 0 3 0.143 1.630 1828 1828 Surry 37171 37171 86 3188 5 4 0.070 2.968 1831 1831 Currituck 37053 37053 27 508 1 5 0.153 2.206 1832 1832 Northampton 37131 37131 66 1421 9 6 0.097 1.670 1833 1833 Hertford 37091 37091 46 1452 7 NWBIR74 BIR79 SID79 NWBIR79 geometry 1 10 1364 0 19 MULTIPOLYGON (((-81.47276 3... 2 10 542 3 12 MULTIPOLYGON (((-81.23989 3... 3 208 3616 6 260 MULTIPOLYGON (((-80.45634 3... 4 123 830 2 145 MULTIPOLYGON (((-76.00897 3... 5 1066 1606 3 1197 MULTIPOLYGON (((-77.21767 3... 6 954 1838 5 1237 MULTIPOLYGON (((-76.74506 3... Just like a regular data.frame, you see a number of variables (attributes) except that you have a variable called geometry at the end. Each row represents a single geographic unit (here, county). Ashe County (1st row) has area of \\(0.114\\), FIPS code of \\(37009\\), and so on. And the entry in geometry column at the first row represents the geographic information of Ashe County. An entry in the geometry column is a simple feature geometry (sfg), which is an \\(R\\) object that represents the geographic information of a single geometric feature (county in this example). There are different types of sfgs (POINT, LINESTRING, POLYGON, MULTIPOLYGON, etc). Here, sfgs representing counties in NC are of type MULTIPOLYGON. Let’s take a look inside the sfg for Ashe County using st_geometry(). st_geometry(nc[1, ])[[1]][[1]] [[1]] [,1] [,2] [1,] -81.47276 36.23436 [2,] -81.54084 36.27251 [3,] -81.56198 36.27359 [4,] -81.63306 36.34069 [5,] -81.74107 36.39178 [6,] -81.69828 36.47178 [7,] -81.70280 36.51934 [8,] -81.67000 36.58965 [9,] -81.34530 36.57286 [10,] -81.34754 36.53791 [11,] -81.32478 36.51368 [12,] -81.31332 36.48070 [13,] -81.26624 36.43721 [14,] -81.26284 36.40504 [15,] -81.24069 36.37942 [16,] -81.23989 36.36536 [17,] -81.26424 36.35241 [18,] -81.32899 36.36350 [19,] -81.36137 36.35316 [20,] -81.36569 36.33905 [21,] -81.35413 36.29972 [22,] -81.36745 36.27870 [23,] -81.40639 36.28505 [24,] -81.41233 36.26729 [25,] -81.43104 36.26072 [26,] -81.45289 36.23959 [27,] -81.47276 36.23436 As you can see, the sfg consists of a number of points (pairs of two numbers). Connecting the points in the order they are stored delineates the Ashe County boundary. plot(st_geometry(nc[1, ])) We will take a closer look at different types of sfg in the next section. Finally, the geometry variable is a list of individual sfgs, called simple feature geometry list-column (sfc). dplyr::select(nc, geometry) Simple feature collection with 100 features and 0 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 First 10 features: geometry 1 MULTIPOLYGON (((-81.47276 3... 2 MULTIPOLYGON (((-81.23989 3... 3 MULTIPOLYGON (((-80.45634 3... 4 MULTIPOLYGON (((-76.00897 3... 5 MULTIPOLYGON (((-77.21767 3... 6 MULTIPOLYGON (((-76.74506 3... 7 MULTIPOLYGON (((-76.00897 3... 8 MULTIPOLYGON (((-76.56251 3... 9 MULTIPOLYGON (((-78.30876 3... 10 MULTIPOLYGON (((-80.02567 3... Elements of a geometry list-column are allowed to be different in nature from other elements36. In the nc data, all the elements (sfgs) in geometry column are MULTIPOLYGON. However, you could also have LINESTRING or POINT objects mixed with MULTIPOLYGONS objects in a single sf object if you would like. This is just like a regular list object that can contain mixed types of elements: numeric, character, etc↩︎ "],
["simple-feature-geometry-simple-feature-geometry-list-column-and-simple-feature.html", "2.2 Simple feature geometry, simple feature geometry list-column, and simple feature", " 2.2 Simple feature geometry, simple feature geometry list-column, and simple feature Here, we learn how different types of sfg are constructed. We also learn how to create sfc and sf from sfg from scratch.37 2.2.1 Simple feature geometry (sfg) The sf package uses a class of sfg (simple feature geometry) objects to represent a geometry of a single geometric feature (say, a city as a point, a river as a line, county and school district as polygons). There are different types of sfgs. Here are some example feature types that we commonly encounter as an economist38: POINT: area-less feature that represents a point (e.g., well, city, farmland) LINESTRING: (e.g., a tributary of a river) MULTILINESTRING: (e.g., river with more than one tributary) POLYGON: geometry with a positive area (e.g., county, state, country) MULTIPOLYGON: collection of polygons to represent a single object (e.g., countries with islands: U.S., Japan) POINT is the simplest geometry type and is represented by a vector of two39 numeric values. An example below shows how a POINT feature can be made from scratch: #--- create a POINT ---# a_point &lt;- st_point(c(2,1)) The st_point() function creates a POINT object when supplied with a vector of two numeric values. If you check the class of the newly created object, #--- check the class of the object ---# class(a_point) [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; you can see that it’s indeed a POINT object. But, it’s also an sfg object. So, a_point is an sfg object of type POINT. A LINESTRING objects are represented by a sequence of points: #--- collection of points in a matrix form ---# s1 &lt;- rbind(c(2,3),c(3,4),c(3,5),c(1,5)) #--- see what s1 looks like ---# s1 [,1] [,2] [1,] 2 3 [2,] 3 4 [3,] 3 5 [4,] 1 5 #--- create a &quot;LINESTRING&quot; ---# a_linestring &lt;- st_linestring(s1) #--- check the class ---# class(a_linestring) [1] &quot;XY&quot; &quot;LINESTRING&quot; &quot;sfg&quot; s1 is a matrix where each row represents a point. By applying st_linestring() function to s1, you create a LINESTRING object. Let’s see what the line looks like. plot(a_linestring) As you can see, each pair of consecutive points in the matrix are connected by a straight line to form a line. A POLYGON is very similar to LINESTRING in the manner it is represented. #--- collection of points in a matrix form ---# p1 &lt;- rbind(c(0,0), c(3,0), c(3,2), c(2,5), c(1,3), c(0,0)) #--- see what s1 looks like ---# p1 [,1] [,2] [1,] 0 0 [2,] 3 0 [3,] 3 2 [4,] 2 5 [5,] 1 3 [6,] 0 0 #--- create a &quot;LINESTRING&quot; ---# a_polygon &lt;- st_polygon(list(p1)) #--- check the class ---# class(a_polygon) [1] &quot;XY&quot; &quot;POLYGON&quot; &quot;sfg&quot; #--- see what it looks like ---# plot(a_polygon) Just like the LINESTRING object we created earlier, a POLYGON is represented by a collection of points. The biggest difference between them is that we need to have some positive area enclosed by lines connecting the points. To do that, you have the the same point for the first and last points to close the loop: here, it’s c(0,0). A POLYGON can have a hole in it. The first matrix of a list becomes the exterior ring, and all the subsequent matrices will be holes within the exterior ring. #--- a hole within p1 ---# p2 &lt;- rbind(c(1,1), c(1,2), c(2,2), c(1,1)) #--- create a polygon with hole ---# a_plygon_with_a_hole &lt;- st_polygon(list(p1,p2)) #--- see what it looks like ---# plot(a_plygon_with_a_hole) You can create a MULTIPOLYGON object in a similar manner. The only difference is that you supply a list of lists of matrices, with each inner list representing a polygon. An example below: #--- second polygon ---# p3 &lt;- rbind(c(4,0), c(5,0), c(5,3), c(4,2), c(4,0)) #--- create a multipolygon ---# a_multipolygon &lt;- st_multipolygon(list(list(p1,p2), list(p3))) #--- see what it looks like ---# plot(a_multipolygon) Each of list(p1,p2), list(p3,p4), list(p5) represents a polygon. You supply a list of these lists to the st_multipolygon() function to make a MULTIPOLYGON object. 2.2.2 Create simple feature geometry list-column (sfc) and simple feature (sf) from scratch To make a simple feature geometry list-column (sfc), you can simply supply a list of sfg to the st_sfc() function as follows: #--- create an sfc ---# sfc_ex &lt;- st_sfc(list(a_point,a_linestring,a_polygon,a_multipolygon)) To create an sf object, you first add an sfc as a column to a data.frame. #--- create a data.frame ---# df_ex &lt;- data.frame( name=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;) ) #--- add the sfc as a column ---# df_ex$geometry &lt;- sfc_ex #--- take a look ---# df_ex name geometry 1 A POINT (2 1) 2 B LINESTRING (2 3, 3 4, 3 5, ... 3 C POLYGON ((0 0, 3 0, 3 2, 2 ... 4 D MULTIPOLYGON (((0 0, 3 0, 3... At this point, it is not yet recognized as an sf by R yet. #--- see what it looks like (this is not an sf object yet) ---# class(df_ex) [1] &quot;data.frame&quot; You can register it as an sf object using st_as_sf(). #--- let R recognize the data frame as sf ---# sf_ex &lt;- st_as_sf(df_ex) #--- see what it looks like ---# sf_ex Simple feature collection with 4 features and 1 field geometry type: GEOMETRY dimension: XY bbox: xmin: 0 ymin: 0 xmax: 5 ymax: 5 CRS: NA name geometry 1 A POINT (2 1) 2 B LINESTRING (2 3, 3 4, 3 5, ... 3 C POLYGON ((0 0, 3 0, 3 2, 2 ... 4 D MULTIPOLYGON (((0 0, 3 0, 3... As you can see sf_ex is now recognized also as an sf object. #--- check the class ---# class(sf_ex) [1] &quot;sf&quot; &quot;data.frame&quot; Creating spatial objects from scratch yourself is an unnecessary skill for many of us as economists. But, it is still good to know the underlying structure of the data. Also, occasionally the need arises. For example, I had to construct spatial objects from scratch when I designed on-farm randomized nitrogen trials. In such cases, it is of course necessary to understand how different types of sfg are constructed, create sfc from a collection of sfgs, and then create an sf from an sfc.↩︎ You will hardly see the other geometry types: MULTIPOINT and GEOMETRYCOLLECTION. You may see GEOMETRYCOLLECTION after intersecting two spatial objects. You can see here if you are interested in learning what they are.↩︎ or three to represent a point in the three-dimensional space↩︎ "],
["reading-and-writing-vector-data.html", "2.3 Reading and writing vector data", " 2.3 Reading and writing vector data The vast majority of people still use ArcGIS to handle spatial data, which has its own system of storing spatial data40 called shapefile. So, chances are that your collaborators use shapefiles. Moreover, there are many GIS data online that are available only as shapefiles. So, it is important to learn how to read and write shapefiles. 2.3.1 Reading a shapefile We can use st_read() to read a shapefile. It reads in a shapefile and then turn the data into an sf object. Let’s take a look at an example. #--- read a NE county boundary shapefile ---# nc_loaded &lt;- st_read(dsn = &quot;./Data&quot;, &quot;nc&quot;) Typically, you have two arguments to specify for st_read(). The first one is dsn, which is basically the path to folder in which the shapefile you want to import is stored. The second one is the name of the shapefile. Notice that you do not add .shp extension to the file name: nc, not nc.shp.41. 2.3.2 Writing to a shapefile Writing an sf object as a shapefile is just as easy. You use the st_write() function, with the first argument being the name of the sf object you are exporting, and the second being the name of the new shapefile. For example, the code below will export an sf object called nc_loaded as nc2.shp (along with other supporting files). st_write(nc_loaded, dsn=&quot;./Data&quot;, &quot;nc2&quot;, driver=&quot;ESRI Shapefile&quot;, append = FALSE) append = FALSE forces writing the data when a file already exists with the same name. Without the option, this happens. st_write(nc_loaded, dsn=&quot;./Data&quot;, &quot;nc2&quot;, driver=&quot;ESRI Shapefile&quot;) Layer nc2 in dataset ./Data already exists: use either append=TRUE to append to layer or append=FALSE to overwrite layer Error in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : Dataset already exists. 2.3.3 Better alternatives Now, if your collaborator is using ArcGIS and demanding that he/she needs a shapefile for his/her work, sure you can use the above command to write a shapefile. But, there is really no need to work with the shapefile system. One of the alternative data formats that is considered superior to the shapefile system is GeoPackage42, which overcomes various limitations associated with shapefile43. Unlike the shapefile system, it produces only a single file with .gpkg extension.44 Note that GeoPackage files can also be easily read into ArcGIS. So, it might be worthwhile to convince your collaborators to stop using shapefiles and start using GeoPackage. #--- write as a gpkg file ---# st_write(nc, dsn = &quot;./Data/nc.gpkg&quot;, append = FALSE) #--- read a gpkg file ---# nc &lt;- st_read(&quot;./Data/nc.gpkg&quot;) Or better yet, if your collaborator uses R (or if it is only you who is going to use the data), then just save it as an rds file using saveRDS(), which can be of course read using readRDS(). #--- save as an rds ---# saveRDS(nc, &quot;/Users/tmieno2/Box/Teaching/AAEA R/GIS/nc_county.rds&quot;) #--- read an rds ---# nc &lt;- readRDS(&quot;/Users/tmieno2/Box/Teaching/AAEA R/GIS/nc_county.rds&quot;) The use of rds files can be particularly attractive when the dataset is large because rds files are typically more memory efficient than shapefiles, eating up less of your disk memory. As you can see here, it is a myth that spatial datasets have to be stored as shapefiles. See here for how spatial datasets can be stores in various other formats.↩︎ When storing a spatial dataset, ArcGIS divides the information into separate files. All of them have the same prefix, but have different extensions. We typically say we read a shapefile, but we really are importing all these files including the shapefile with the .shp extension. When you read those data, you just refer to the common prefix because you really are importing all the files, not just a .shp file.↩︎ here↩︎ see the last paragraph of chapter 7.5 of this book, this blogpost, or this↩︎ Am I the only one who gets very frustrated when your collaborator attaches 15 files for three geographic objects to an email? It could have been just three files using the GeoPackage format.↩︎ "],
["projection-with-a-different-coordinate-reference-systems.html", "2.4 Projection with a different Coordinate Reference Systems", " 2.4 Projection with a different Coordinate Reference Systems You often need to reproject an sf using a different coordinate reference system (CRS) because you need it to have the same CRS as an sf object that you are interacting it with (spatial join) or mapping it with. In order to check the current CRS for an sf object, you can use the st_crs() function. st_crs(nc) Coordinate Reference System: User input: NAD27 wkt: GEOGCRS[&quot;NAD27&quot;, DATUM[&quot;North American Datum 1927&quot;, ELLIPSOID[&quot;Clarke 1866&quot;,6378206.4,294.978698213898, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;latitude&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;longitude&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4267]] wkt stands for Well Known Text45, which is one of many many formats to store CRS information.46 4267 is the SRID (Spatial Reference System Identifier) defined by the European Petroleum Survey Group (EPSG) for the CRS47. When you transform your sf using a different CRS, you can use its EPSG number if the CRS has an EPSG number.48 Let’s transform the sf to WGS 84 (another commonly used GCS), whose EPSG number is 4326. We can use the st_transform() function to achieve that, with the first argument being the sf object you are transforming and the second being the EPSG number of the new CRS. #--- transform ---# nc_wgs84 &lt;- st_transform(nc, 4326) #--- check if the transformation was successful ---# st_crs(nc_wgs84) Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCRS[&quot;WGS 84&quot;, DATUM[&quot;World Geodetic System 1984&quot;, ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;geodetic latitude (Lat)&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;geodetic longitude (Lon)&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;World&quot;], BBOX[-90,-180,90,180]], ID[&quot;EPSG&quot;,4326]] Notice that wkt was also altered accordingly to reflect the change in CRS: datum was changed to WGS 84. Now, let’s transform (reproject) the data using NAD83 / UTM zone 17N CRS. Its EPSG number is \\(26917\\).49 So, the following code does the job. #--- transform ---# nc_utm17N &lt;- st_transform(nc_wgs84, 26917) #--- check if the transformation was successful ---# st_crs(nc_utm17N) Coordinate Reference System: User input: EPSG:26917 wkt: PROJCRS[&quot;NAD83 / UTM zone 17N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 17N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-81, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;North America - 84°W to 78°W and NAD83 by country&quot;], BBOX[23.81,-84,84,-78]], ID[&quot;EPSG&quot;,26917]] As you can see in its CRS information, the projection system is now UTM zone 17N. You often need to change the CRS of an sf object when you interact (e.g., spatial subsetting, joining, etc) it with another sf object. In such a case, you can extract the CRS of the other sf object using st_crs() and use it for transformation.50 So, you do not need to find the EPSG of the CRS of the sf object you are interacting it with. #--- transform ---# nc_utm17N_2 &lt;- st_transform(nc_wgs84, st_crs(nc_utm17N)) #--- check if the transformation was successful ---# st_crs(nc_utm17N_2) Coordinate Reference System: User input: EPSG:26917 wkt: PROJCRS[&quot;NAD83 / UTM zone 17N&quot;, BASEGEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]], CONVERSION[&quot;UTM zone 17N&quot;, METHOD[&quot;Transverse Mercator&quot;, ID[&quot;EPSG&quot;,9807]], PARAMETER[&quot;Latitude of natural origin&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8801]], PARAMETER[&quot;Longitude of natural origin&quot;,-81, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ID[&quot;EPSG&quot;,8802]], PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996, SCALEUNIT[&quot;unity&quot;,1], ID[&quot;EPSG&quot;,8805]], PARAMETER[&quot;False easting&quot;,500000, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8806]], PARAMETER[&quot;False northing&quot;,0, LENGTHUNIT[&quot;metre&quot;,1], ID[&quot;EPSG&quot;,8807]]], CS[Cartesian,2], AXIS[&quot;(E)&quot;,east, ORDER[1], LENGTHUNIT[&quot;metre&quot;,1]], AXIS[&quot;(N)&quot;,north, ORDER[2], LENGTHUNIT[&quot;metre&quot;,1]], USAGE[ SCOPE[&quot;unknown&quot;], AREA[&quot;North America - 84°W to 78°W and NAD83 by country&quot;], BBOX[23.81,-84,84,-78]], ID[&quot;EPSG&quot;,26917]] sf versions prior to 0.9 provides CRS information in the form of proj4string. The newer version of sf presents CRS in the form of wtk (see this slide). You can find the reason behind this change in the same slide, starting from here.↩︎ See here for numerous other formats that represent the same CRS.↩︎ You can find the CRS-EPSG number correspondence here.↩︎ Potential pool of CRS is infinite. Only the commonly-used CRS have been assigned EPSG SRID.↩︎ See here.↩︎ In this example, we are using the same data with two different CRS. But, you get the point.↩︎ "],
["quick-and-interactive-view-of-an-sf-object.html", "2.5 Quick and interactive view of an sf object", " 2.5 Quick and interactive view of an sf object 2.5.1 Quick view The easiest way to visualize an sf object is to use plot(): plot(nc) Figure 2.2: Quick Visualization of an sf object As you can see, plot() create a map for each variable where the spatial units are color-differentiated based on the values of the variable. For creating more elaborate maps that are of publication-quality, see Chapter 8. 2.5.2 Interactive view Sometimes it is useful to be able to tell where certain spatial objects are and what values are associated with them on a map. The mapView() function from the mapview package can create an interactive map where you can point to a spatial object and the associated information is revealed on the map. Let’s use the North Carolina county map as an example here: #--- read the NC county map data ---# nc &lt;- st_read(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)) #--- generate an interactive map ---# mapView(nc) As you can see, if you put your cursor on a polygon (county) and click on it, then its information pops up. Alternatively, you could use the tmap package to create interactive maps. You can first create a static map following a syntax like this: #--- NOT RUN (for polygons) ---# tm_shape(sf) + tm_polygons() #--- NOT RUN (for points) ---# tm_shape(sf) + tm_symbols() This creates a static map of nc where county boundaries are drawn: ( tm_nc_polygons &lt;- tm_shape(nc) + tm_polygons() ) Then, you can apply tmap_leaflet() to the static map to have an interactive view of the map: tmap_leaflet(tm_nc_polygons) You could also change the view mode of tmap objects to the view mode using tmap_mode(\"view\") and then simply evaluate tm_nc_polygons. #--- change to the &quot;view&quot; mode ---# tmap_mode(&quot;view&quot;) #--- now you have an interactive biew ---# tm_nc_polygons Note that once you change the view mode to “view”, then the evaluation of all tmap objects become interactive. I prefer the first option, as I need to revert the view mode back to “plot” by tmap_mode(\"plot\") if I don’t want interactive views. "],
["turning-a-data-frame-of-points-into-an-sf.html", "2.6 Turning a data.frame of points into an sf", " 2.6 Turning a data.frame of points into an sf Often times, you have a dataset with geographic coordinates as variables in a csv or other formats, which would not be recognized as a spatial dataset by R immediately when it is read into R. In this case, you need to identify which variables represent the geographic coordinates from the data set, and create an sf yourself. Fortunately, it is easy to do so using the st_as_sf() function. Let’s first read a dataset (irrigation wells in Nebraska): #--- read irrigation well registration data ---# ( wells &lt;- readRDS(&#39;./Data/well_registration.rds&#39;) ) wellid ownerid nrdname acres regdate section longdd 1: 2 106106 Central Platte 160 12/30/55 3 -99.58401 2: 3 14133 South Platte 46 4/29/31 8 -102.62495 3: 4 14133 South Platte 46 4/29/31 8 -102.62495 4: 5 14133 South Platte 46 4/29/31 8 -102.62495 5: 6 15837 Central Platte 160 8/29/32 20 -99.62580 --- 105818: 244568 135045 Upper Big Blue NA 8/26/16 30 -97.58872 105819: 244569 105428 Little Blue NA 8/26/16 24 -97.60752 105820: 244570 135045 Upper Big Blue NA 8/26/16 30 -97.58294 105821: 244571 135045 Upper Big Blue NA 8/26/16 25 -97.59775 105822: 244572 105428 Little Blue NA 8/26/16 15 -97.64086 latdd 1: 40.69825 2: 41.11699 3: 41.11699 4: 41.11699 5: 40.73268 --- 105818: 40.89017 105819: 40.13257 105820: 40.88722 105821: 40.89639 105822: 40.13380 #--- check the class ---# class(wells) [1] &quot;data.table&quot; &quot;data.frame&quot; As you can see the data is not an sf object. In this dataset, longdd and latdd represent longitude and latitude, respectively. We now turn the dataset into an sf object: #--- recognize it as an sf ---# wells_sf &lt;- st_as_sf(wells, coords = c(&quot;longdd&quot;,&quot;latdd&quot;)) #--- take a look at the data ---# head(wells_sf[,1:5]) Simple feature collection with 6 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699 CRS: NA wellid ownerid nrdname acres regdate geometry 1 2 106106 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 2 3 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 3 4 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 4 5 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 5 6 15837 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 6 7 90248 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) Note that the CRS of wells_sf is NA. Obviously, \\(R\\) does not know the reference system without you telling it. We know51 that the geographic coordinates in the wells data is NAD 83 (\\(epsg=4269\\)) for this dataset. So, we can assign the right CRS using either st_set_crs() or st_crs(). #--- set CRS ---# wells_sf &lt;- st_set_crs(wells_sf, 4269) #--- or this ---# st_crs(wells_sf) &lt;- 4269 #--- see the change ---# head(wells_sf[,1:5]) Simple feature collection with 6 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699 geographic CRS: NAD83 wellid ownerid nrdname acres regdate geometry 1 2 106106 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 2 3 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 3 4 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 4 5 14133 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 5 6 15837 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 6 7 90248 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) Yes, YOU need to know the CRS of your data.↩︎ "],
["conv-sp.html", "2.7 Conversion to and from sp", " 2.7 Conversion to and from sp Though unlikely, you may find instances where sp objects are necessary or desirable.52 In that case, it is good to know how to convert an sf object to an sp object, vice versa. You can convert an sf object to its sp counterpart using as(sf_object, \"Spatial\"): #--- conversion ---# wells_sp &lt;- as(wells_sf, &quot;Spatial&quot;) #--- check the class ---# class(wells_sp) [1] &quot;SpatialPointsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; As you can see wells_sp is a class of SpatialPointsDataFrame, points with a data.frame supported by the sp package. The above syntax works for converting an sf of polygons into SpatialPolygonsDataFrame as well53. You can revert wells_sp back to an sf object using the st_as_sf() function, as follows: #--- revert back to sf ---# wells_sf &lt;- st_as_sf(wells_sp) #--- check the class ---# class(wells_sf) [1] &quot;sf&quot; &quot;data.frame&quot; We do not cover how to use the sp package as the benefit of learning it has become marginal compared to when sf was just introduced a few years back54. For example, those who run spatial econometric methods using spdep, creating neighbors from polygons is a bit faster using sp objects than using sf objects.↩︎ The function does not work for an sf object that consists of different geometry types (e.g., POINT and POLYGON). This is because sp objects do not allow different types of geometries in the single sp object. For example, SpatialPointsDataFrame consists only of points data.↩︎ For those interested in learning the sp package, this website is a good resource.↩︎ "],
["non-spatial-transformation-of-sf.html", "2.8 Non-spatial transformation of sf", " 2.8 Non-spatial transformation of sf 2.8.1 Using dplyr An important feature of an sf object is that it is basically a data.frame with geometric information stored as a variable (column). This means that transforming an sf object works just like transforming a data.frame. Basically, everything you can do to a data.frame, you can do to an sf as well. The code below just provides an example of basic operations including dplyr::select(), dplyr::filter(), and dplyr::mutate() in action with an sf object to just confirm that dplyr operations works with an sf object just like a data.frame. #--- here is what the data looks like ---# dplyr::select(wells_sf, wellid, nrdname, acres, regdate, nrdname) Simple feature collection with 105822 features and 4 fields geometry type: POINT dimension: XY bbox: xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942 geographic CRS: NAD83 First 10 features: wellid nrdname acres regdate geometry 1 2 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 2 3 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 3 4 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 4 5 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 5 6 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 6 7 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) 7 8 South Platte 113 8/7/37 POINT (-103.5257 41.24492) 8 10 South Platte 160 5/4/38 POINT (-103.0284 41.13243) 9 11 Middle Republican 807 5/6/38 POINT (-101.1193 40.3527) 10 12 Middle Republican 148 11/29/77 POINT (-101.1146 40.35631) Notice that geometry column will be retained after dplyr::select() even if you did not tell R to keep it above. Let’s apply dplyr::select(), dplyr::filter(), and dplyr::mutate() to the dataset. #--- do some transformations ---# wells_sf %&gt;% #--- select variables (geometry will always remain after select) ---# dplyr::select(wellid, nrdname, acres, regdate, nrdname) %&gt;% #--- removes observations with acre &lt; 30 ---# dplyr::filter(acres &gt; 30) %&gt;% #--- hectare instead of acre ---# dplyr::mutate(hectare = acres * 0.404686) Simple feature collection with 63271 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -104.0529 ymin: 40.00161 xmax: -96.87681 ymax: 41.73599 geographic CRS: NAD83 First 10 features: wellid nrdname acres regdate geometry hectare 1 2 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 64.74976 2 3 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 3 4 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 4 5 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 5 6 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 64.74976 6 7 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) 48.56232 7 8 South Platte 113 8/7/37 POINT (-103.5257 41.24492) 45.72952 8 10 South Platte 160 5/4/38 POINT (-103.0284 41.13243) 64.74976 9 11 Middle Republican 807 5/6/38 POINT (-101.1193 40.3527) 326.58160 10 12 Middle Republican 148 11/29/77 POINT (-101.1146 40.35631) 59.89353 Now, let’s try to get a summary of a variable by group using the group_by() and summarize() functions. #--- summary by group ---# wells_by_nrd &lt;- wells_sf %&gt;% #--- group by nrdname ---# dplyr::group_by(nrdname) %&gt;% #--- summarize ---# dplyr::summarize(tot_acres = sum(acres, na.rm = TRUE)) #--- take a look ---# wells_by_nrd Simple feature collection with 9 features and 2 fields geometry type: MULTIPOINT dimension: XY bbox: xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942 geographic CRS: NAD83 # A tibble: 9 x 3 nrdname tot_acres geometry &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOINT [°]&gt; 1 Central Plat… 1890918. ((-100.2329 41.14385), (-100.2328 41.05678), (-100.23… 2 Little Blue 995900. ((-98.72659 40.30463), (-98.72434 40.68021), (-98.724… 3 Lower Republ… 543079. ((-100.1968 40.32314), (-100.196 40.33553), (-100.195… 4 Middle Repub… 443472. ((-101.3691 40.1208), (-101.3448 40.64638), (-101.344… 5 South Platte 216109. ((-104.0531 41.18248), (-104.053 41.19347), (-104.052… 6 Tri-Basin 847058. ((-100.0927 40.42312), (-100.0904 40.43158), (-100.08… 7 Twin Platte 452678. ((-102.0557 41.05204), (-102.0556 41.05488), (-102.05… 8 Upper Big Bl… 1804782. ((-98.83619 40.85932), (-98.81149 40.78093), (-98.549… 9 Upper Republ… 551906. ((-102.0516 40.24644), (-102.0515 40.6287), (-102.051… So, we got total acres by NRD as we expected. One interesting change that happened is geometry variable. Each NRD now has multipoint sfg, which is the combination of all the wells (points) located inside the NRD as you can see below. tm_shape(wells_by_nrd) + tm_symbols(col = &quot;nrdname&quot;, size = 0.2) + tm_layout( frame = NA, legend.outside = TRUE, legend.outside.position = &quot;bottom&quot; ) This feature is unlikely to be of much use to us. If you would like to drop a geometry column, you can use the st_drop_geometry() function: #--- remove geometry ---# wells_no_longer_sf &lt;- st_drop_geometry(wells_by_nrd) #--- take a look ---# wells_no_longer_sf # A tibble: 9 x 2 nrdname tot_acres * &lt;chr&gt; &lt;dbl&gt; 1 Central Platte 1890918. 2 Little Blue 995900. 3 Lower Republican 543079. 4 Middle Republican 443472. 5 South Platte 216109. 6 Tri-Basin 847058. 7 Twin Platte 452678. 8 Upper Big Blue 1804782. 9 Upper Republican 551906. 2.8.2 Using data.table The data.table package provides data wrangling options that are extremely fast. It particularly shines when datasets are large and is much faster than dplyr. However, it cannot be as naturally integrated into the workflow involving sf objects as dplyr can. Let’s convert an sf object of points into a data.table object using data.table(). #--- convert an sf to data.table ---# ( wells_dt &lt;- data.table(wells_sf) ) wellid ownerid nrdname acres regdate section 1: 2 106106 Central Platte 160 12/30/55 3 2: 3 14133 South Platte 46 4/29/31 8 3: 4 14133 South Platte 46 4/29/31 8 4: 5 14133 South Platte 46 4/29/31 8 5: 6 15837 Central Platte 160 8/29/32 20 --- 105818: 244568 135045 Upper Big Blue NA 8/26/16 30 105819: 244569 105428 Little Blue NA 8/26/16 24 105820: 244570 135045 Upper Big Blue NA 8/26/16 30 105821: 244571 135045 Upper Big Blue NA 8/26/16 25 105822: 244572 105428 Little Blue NA 8/26/16 15 geometry 1: POINT (-99.58401 40.69825) 2: POINT (-102.6249 41.11699) 3: POINT (-102.6249 41.11699) 4: POINT (-102.6249 41.11699) 5: POINT (-99.6258 40.73268) --- 105818: POINT (-97.58872 40.89017) 105819: POINT (-97.60752 40.13257) 105820: POINT (-97.58294 40.88722) 105821: POINT (-97.59775 40.89639) 105822: POINT (-97.64086 40.1338) #--- check the class ---# class(wells_dt) [1] &quot;data.table&quot; &quot;data.frame&quot; You see that wells_dt is no longer an sf object, but the geometry column still remains in the data. When you convert an sf of polygons into a data.table, then the geometry column appears to have lost the geometry information as all the entries are just &lt;XY&gt;. ( nc_dt &lt;- data.table(nc) ) AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 1: 0.114 1.442 1825 1825 Ashe 37009 37009 5 1091 2: 0.061 1.231 1827 1827 Alleghany 37005 37005 3 487 3: 0.143 1.630 1828 1828 Surry 37171 37171 86 3188 4: 0.070 2.968 1831 1831 Currituck 37053 37053 27 508 5: 0.153 2.206 1832 1832 Northampton 37131 37131 66 1421 6: 0.097 1.670 1833 1833 Hertford 37091 37091 46 1452 7: 0.062 1.547 1834 1834 Camden 37029 37029 15 286 8: 0.091 1.284 1835 1835 Gates 37073 37073 37 420 9: 0.118 1.421 1836 1836 Warren 37185 37185 93 968 10: 0.124 1.428 1837 1837 Stokes 37169 37169 85 1612 11: 0.114 1.352 1838 1838 Caswell 37033 37033 17 1035 12: 0.153 1.616 1839 1839 Rockingham 37157 37157 79 4449 13: 0.143 1.663 1840 1840 Granville 37077 37077 39 1671 14: 0.109 1.325 1841 1841 Person 37145 37145 73 1556 15: 0.072 1.085 1842 1842 Vance 37181 37181 91 2180 16: 0.190 2.204 1846 1846 Halifax 37083 37083 42 3608 17: 0.053 1.171 1848 1848 Pasquotank 37139 37139 70 1638 18: 0.199 1.984 1874 1874 Wilkes 37193 37193 97 3146 19: 0.081 1.288 1880 1880 Watauga 37189 37189 95 1323 20: 0.063 1.000 1881 1881 Perquimans 37143 37143 72 484 21: 0.044 1.158 1887 1887 Chowan 37041 37041 21 751 22: 0.064 1.213 1892 1892 Avery 37011 37011 6 781 23: 0.086 1.267 1893 1893 Yadkin 37197 37197 99 1269 24: 0.128 1.554 1897 1897 Franklin 37069 37069 35 1399 25: 0.108 1.483 1900 1900 Forsyth 37067 37067 34 11858 26: 0.170 1.680 1903 1903 Guilford 37081 37081 41 16184 27: 0.111 1.392 1904 1904 Alamance 37001 37001 1 4672 28: 0.180 2.151 1905 1905 Bertie 37015 37015 8 1324 29: 0.104 1.294 1907 1907 Orange 37135 37135 68 3164 30: 0.077 1.271 1908 1908 Durham 37063 37063 32 7970 31: 0.142 1.640 1913 1913 Nash 37127 37127 64 4021 32: 0.059 1.319 1927 1927 Mitchell 37121 37121 61 671 33: 0.131 1.521 1928 1928 Edgecombe 37065 37065 33 3657 34: 0.122 1.516 1932 1932 Caldwell 37027 37027 14 3609 35: 0.080 1.307 1936 1936 Yancey 37199 37199 100 770 36: 0.118 1.899 1937 1937 Martin 37117 37117 59 1549 37: 0.219 2.130 1938 1938 Wake 37183 37183 92 14484 38: 0.118 1.601 1946 1946 Madison 37115 37115 58 765 39: 0.155 1.781 1947 1947 Iredell 37097 37097 49 4139 40: 0.069 1.201 1948 1948 Davie 37059 37059 30 1207 41: 0.066 1.070 1950 1950 Alexander 37003 37003 2 1333 42: 0.145 1.791 1951 1951 Davidson 37057 37057 29 5509 43: 0.134 1.755 1958 1958 Burke 37023 37023 12 3573 44: 0.100 1.331 1962 1962 Washington 37187 37187 94 990 45: 0.099 1.411 1963 1963 Tyrrell 37177 37177 89 248 46: 0.116 1.664 1964 1964 McDowell 37111 37111 56 1946 47: 0.201 1.805 1968 1968 Randolph 37151 37151 76 4456 48: 0.180 2.142 1973 1973 Chatham 37037 37037 19 1646 49: 0.094 1.307 1979 1979 Wilson 37195 37195 98 3702 50: 0.134 1.590 1980 1980 Rowan 37159 37159 80 4606 51: 0.168 1.791 1984 1984 Pitt 37147 37147 74 5094 52: 0.106 1.444 1986 1986 Catawba 37035 37035 18 5754 53: 0.168 1.995 1988 1988 Buncombe 37021 37021 11 7515 54: 0.207 1.851 1989 1989 Johnston 37101 37101 51 3999 55: 0.144 1.690 1996 1996 Haywood 37087 37087 44 2110 56: 0.094 3.640 2000 2000 Dare 37055 37055 28 521 57: 0.203 3.197 2004 2004 Beaufort 37013 37013 7 2692 58: 0.141 2.316 2013 2013 Swain 37173 37173 87 675 59: 0.070 1.105 2016 2016 Greene 37079 37079 40 870 60: 0.065 1.093 2026 2026 Lee 37105 37105 53 2252 61: 0.146 1.778 2027 2027 Rutherford 37161 37161 81 2992 62: 0.142 1.655 2029 2029 Wayne 37191 37191 96 6638 63: 0.154 1.680 2030 2030 Harnett 37085 37085 43 3776 64: 0.118 1.506 2032 2032 Cleveland 37045 37045 23 4866 65: 0.078 1.384 2034 2034 Lincoln 37109 37109 55 2216 66: 0.125 1.601 2039 2039 Jackson 37099 37099 50 1143 67: 0.181 1.980 2040 2040 Moore 37125 37125 63 2648 68: 0.143 1.887 2041 2041 Mecklenburg 37119 37119 60 21588 69: 0.091 1.321 2042 2042 Cabarrus 37025 37025 13 4099 70: 0.130 1.732 2044 2044 Montgomery 37123 37123 62 1258 71: 0.103 1.461 2045 2045 Stanly 37167 37167 84 2356 72: 0.095 1.471 2047 2047 Henderson 37089 37089 45 2574 73: 0.078 1.202 2056 2056 Graham 37075 37075 38 415 74: 0.104 1.548 2065 2065 Lenoir 37107 37107 54 3589 75: 0.098 1.389 2067 2067 Transylvania 37175 37175 88 1173 76: 0.091 1.470 2068 2068 Gaston 37071 37071 36 9014 77: 0.060 1.036 2071 2071 Polk 37149 37149 75 533 78: 0.131 1.677 2082 2082 Macon 37113 37113 57 797 79: 0.241 2.214 2083 2083 Sampson 37163 37163 82 3025 80: 0.082 1.388 2085 2085 Pamlico 37137 37137 69 542 81: 0.120 1.686 2088 2088 Cherokee 37039 37039 20 1027 82: 0.172 1.835 2090 2090 Cumberland 37051 37051 26 20366 83: 0.121 1.978 2091 2091 Jones 37103 37103 52 578 84: 0.163 1.716 2095 2095 Union 37179 37179 90 3915 85: 0.138 1.621 2096 2096 Anson 37007 37007 4 1570 86: 0.098 1.262 2097 2097 Hoke 37093 37093 47 1494 87: 0.167 2.709 2099 2099 Hyde 37095 37095 48 338 88: 0.204 1.871 2100 2100 Duplin 37061 37061 31 2483 89: 0.121 1.855 2107 2107 Richmond 37153 37153 77 2756 90: 0.051 1.096 2109 2109 Clay 37043 37043 22 284 91: 0.177 2.916 2119 2119 Craven 37049 37049 25 5868 92: 0.080 1.188 2123 2123 Scotland 37165 37165 83 2255 93: 0.195 1.783 2146 2146 Onslow 37133 37133 67 11158 94: 0.240 2.004 2150 2150 Robeson 37155 37155 78 7889 95: 0.125 2.868 2156 2156 Carteret 37031 37031 16 2414 96: 0.225 2.107 2162 2162 Bladen 37017 37017 9 1782 97: 0.214 2.152 2185 2185 Pender 37141 37141 71 1228 98: 0.240 2.365 2232 2232 Columbus 37047 37047 24 3350 99: 0.042 0.999 2238 2238 New Hanover 37129 37129 65 5526 100: 0.212 2.024 2241 2241 Brunswick 37019 37019 10 2181 AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79 geometry 1: 1 10 1364 0 19 &lt;XY&gt; 2: 0 10 542 3 12 &lt;XY&gt; 3: 5 208 3616 6 260 &lt;XY&gt; 4: 1 123 830 2 145 &lt;XY&gt; 5: 9 1066 1606 3 1197 &lt;XY&gt; 6: 7 954 1838 5 1237 &lt;XY&gt; 7: 0 115 350 2 139 &lt;XY&gt; 8: 0 254 594 2 371 &lt;XY&gt; 9: 4 748 1190 2 844 &lt;XY&gt; 10: 1 160 2038 5 176 &lt;XY&gt; 11: 2 550 1253 2 597 &lt;XY&gt; 12: 16 1243 5386 5 1369 &lt;XY&gt; 13: 4 930 2074 4 1058 &lt;XY&gt; 14: 4 613 1790 4 650 &lt;XY&gt; 15: 4 1179 2753 6 1492 &lt;XY&gt; 16: 18 2365 4463 17 2980 &lt;XY&gt; 17: 3 622 2275 4 933 &lt;XY&gt; 18: 4 200 3725 7 222 &lt;XY&gt; 19: 1 17 1775 1 33 &lt;XY&gt; 20: 1 230 676 0 310 &lt;XY&gt; 21: 1 368 899 1 491 &lt;XY&gt; 22: 0 4 977 0 5 &lt;XY&gt; 23: 1 65 1568 1 76 &lt;XY&gt; 24: 2 736 1863 0 950 &lt;XY&gt; 25: 10 3919 15704 18 5031 &lt;XY&gt; 26: 23 5483 20543 38 7089 &lt;XY&gt; 27: 13 1243 5767 11 1397 &lt;XY&gt; 28: 6 921 1616 5 1161 &lt;XY&gt; 29: 4 776 4478 6 1086 &lt;XY&gt; 30: 16 3732 10432 22 4948 &lt;XY&gt; 31: 8 1851 5189 7 2274 &lt;XY&gt; 32: 0 1 919 2 4 &lt;XY&gt; 33: 10 2186 4359 9 2696 &lt;XY&gt; 34: 6 309 4249 9 360 &lt;XY&gt; 35: 0 12 869 1 10 &lt;XY&gt; 36: 2 883 1849 1 1033 &lt;XY&gt; 37: 16 4397 20857 31 6221 &lt;XY&gt; 38: 2 5 926 2 3 &lt;XY&gt; 39: 4 1144 5400 5 1305 &lt;XY&gt; 40: 1 148 1438 3 177 &lt;XY&gt; 41: 0 128 1683 2 150 &lt;XY&gt; 42: 8 736 7143 8 941 &lt;XY&gt; 43: 5 326 4314 15 407 &lt;XY&gt; 44: 5 521 1141 0 651 &lt;XY&gt; 45: 0 116 319 0 141 &lt;XY&gt; 46: 5 134 2215 5 128 &lt;XY&gt; 47: 7 384 5711 12 483 &lt;XY&gt; 48: 2 591 2398 3 687 &lt;XY&gt; 49: 11 1827 4706 13 2330 &lt;XY&gt; 50: 3 1057 6427 8 1504 &lt;XY&gt; 51: 14 2620 6635 11 3059 &lt;XY&gt; 52: 5 790 6883 21 914 &lt;XY&gt; 53: 9 930 9956 18 1206 &lt;XY&gt; 54: 6 1165 4780 13 1349 &lt;XY&gt; 55: 2 57 2463 8 62 &lt;XY&gt; 56: 0 43 1059 1 73 &lt;XY&gt; 57: 7 1131 2909 4 1163 &lt;XY&gt; 58: 3 281 883 2 406 &lt;XY&gt; 59: 4 534 1178 4 664 &lt;XY&gt; 60: 5 736 2949 6 905 &lt;XY&gt; 61: 12 495 3543 8 576 &lt;XY&gt; 62: 18 2593 8227 23 3073 &lt;XY&gt; 63: 6 1051 4789 10 1453 &lt;XY&gt; 64: 10 1491 5526 21 1729 &lt;XY&gt; 65: 8 302 2817 7 350 &lt;XY&gt; 66: 2 215 1504 5 307 &lt;XY&gt; 67: 5 844 3534 5 1151 &lt;XY&gt; 68: 44 8027 30757 35 11631 &lt;XY&gt; 69: 3 856 5669 20 1203 &lt;XY&gt; 70: 3 472 1598 8 588 &lt;XY&gt; 71: 5 370 3039 7 528 &lt;XY&gt; 72: 5 158 3679 8 264 &lt;XY&gt; 73: 0 40 488 1 45 &lt;XY&gt; 74: 10 1826 4225 14 2047 &lt;XY&gt; 75: 3 92 1401 4 104 &lt;XY&gt; 76: 11 1523 11455 26 2194 &lt;XY&gt; 77: 1 95 673 0 79 &lt;XY&gt; 78: 0 9 1157 3 22 &lt;XY&gt; 79: 4 1396 3447 4 1524 &lt;XY&gt; 80: 1 222 631 1 277 &lt;XY&gt; 81: 2 32 1173 1 42 &lt;XY&gt; 82: 38 7043 26370 57 10614 &lt;XY&gt; 83: 1 297 650 2 305 &lt;XY&gt; 84: 4 1034 5273 9 1348 &lt;XY&gt; 85: 15 952 1875 4 1161 &lt;XY&gt; 86: 7 987 1706 6 1172 &lt;XY&gt; 87: 0 134 427 0 169 &lt;XY&gt; 88: 4 1061 2777 7 1227 &lt;XY&gt; 89: 4 1043 3108 7 1218 &lt;XY&gt; 90: 0 1 419 0 5 &lt;XY&gt; 91: 13 1744 7595 18 2342 &lt;XY&gt; 92: 8 1206 2617 16 1436 &lt;XY&gt; 93: 29 2217 14655 23 3568 &lt;XY&gt; 94: 31 5904 9087 26 6899 &lt;XY&gt; 95: 5 341 3339 4 487 &lt;XY&gt; 96: 8 818 2052 5 1023 &lt;XY&gt; 97: 4 580 1602 3 763 &lt;XY&gt; 98: 15 1431 4144 17 1832 &lt;XY&gt; 99: 12 1633 6917 9 2100 &lt;XY&gt; 100: 5 659 2655 6 841 &lt;XY&gt; SID74 NWBIR74 BIR79 SID79 NWBIR79 geometry But, the geometry information is still there as you can see from this: #--- take a look at what&#39;s inside the geometry column ---# nc_dt$geometry Geometry set for 100 features geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 First 5 geometries: #--- check the class of the geometry column ---# class(nc_dt$geometry) [1] &quot;sfc_MULTIPOLYGON&quot; &quot;sfc&quot; If you try to run sf operations on it, it will of course give you an error. Like this st_buffer(wells_dt, dist = 2) Error in UseMethod(&quot;st_buffer&quot;): no applicable method for &#39;st_buffer&#39; applied to an object of class &quot;c(&#39;data.table&#39;, &#39;data.frame&#39;)&quot; But, it is easy to revert a data.table object back to an sf object again by using the st_as_sf() function. #--- wells ---# ( wells_sf_again &lt;- st_as_sf(wells_dt) ) Simple feature collection with 105822 features and 6 fields geometry type: POINT dimension: XY bbox: xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942 geographic CRS: NAD83 First 10 features: wellid ownerid nrdname acres regdate section 1 2 106106 Central Platte 160 12/30/55 3 2 3 14133 South Platte 46 4/29/31 8 3 4 14133 South Platte 46 4/29/31 8 4 5 14133 South Platte 46 4/29/31 8 5 6 15837 Central Platte 160 8/29/32 20 6 7 90248 Central Platte 120 2/15/35 19 7 8 48113 South Platte 113 8/7/37 28 8 10 17073 South Platte 160 5/4/38 2 9 11 98432 Middle Republican 807 5/6/38 36 10 12 79294 Middle Republican 148 11/29/77 31 geometry 1 POINT (-99.58401 40.69825) 2 POINT (-102.6249 41.11699) 3 POINT (-102.6249 41.11699) 4 POINT (-102.6249 41.11699) 5 POINT (-99.6258 40.73268) 6 POINT (-99.64524 40.73164) 7 POINT (-103.5257 41.24492) 8 POINT (-103.0284 41.13243) 9 POINT (-101.1193 40.3527) 10 POINT (-101.1146 40.35631) #--- nc polygons ---# ( nc_sf_again &lt;- st_as_sf(nc_dt) ) Simple feature collection with 100 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 First 10 features: AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 1 0.114 1.442 1825 1825 Ashe 37009 37009 5 1091 1 2 0.061 1.231 1827 1827 Alleghany 37005 37005 3 487 0 3 0.143 1.630 1828 1828 Surry 37171 37171 86 3188 5 4 0.070 2.968 1831 1831 Currituck 37053 37053 27 508 1 5 0.153 2.206 1832 1832 Northampton 37131 37131 66 1421 9 6 0.097 1.670 1833 1833 Hertford 37091 37091 46 1452 7 7 0.062 1.547 1834 1834 Camden 37029 37029 15 286 0 8 0.091 1.284 1835 1835 Gates 37073 37073 37 420 0 9 0.118 1.421 1836 1836 Warren 37185 37185 93 968 4 10 0.124 1.428 1837 1837 Stokes 37169 37169 85 1612 1 NWBIR74 BIR79 SID79 NWBIR79 geometry 1 10 1364 0 19 MULTIPOLYGON (((-81.47276 3... 2 10 542 3 12 MULTIPOLYGON (((-81.23989 3... 3 208 3616 6 260 MULTIPOLYGON (((-80.45634 3... 4 123 830 2 145 MULTIPOLYGON (((-76.00897 3... 5 1066 1606 3 1197 MULTIPOLYGON (((-77.21767 3... 6 954 1838 5 1237 MULTIPOLYGON (((-76.74506 3... 7 115 350 2 139 MULTIPOLYGON (((-76.00897 3... 8 254 594 2 371 MULTIPOLYGON (((-76.56251 3... 9 748 1190 2 844 MULTIPOLYGON (((-78.30876 3... 10 160 2038 5 176 MULTIPOLYGON (((-80.02567 3... So, this means that if you need fast data transformation, you can first turn an sf to a data.table, transform the data using the data.table functionality, and then revert back to sf. However, for most economists, the geometry variable itself is not of interest in the sense that it never enters econometric models. For most of us, the geographic information contained in the geometry variable is just a glue to tie two datasets together by geographic referencing. Once we get values of spatial variables of interest, there is no point in keeping your data as an sf object. Personally, whenever I no longer need to carry around the geometry variable, I immediately turn an sf object into a data.table for fast data transformation especially when the data is large. Those who know the dtplyr package (it takes advantage of the speed of data.table while you can keep using dplyr syntax and functions) may wonder if it works well with sf objects. Nope: library(dtplyr) #--- convert an &quot;lazy&quot; data.table ---# wells_ldt &lt;- lazy_dt(wells_sf) #--- try ---# st_buffer(wells_ldt, dist = 2) Error in UseMethod(&quot;st_buffer&quot;): no applicable method for &#39;st_buffer&#39; applied to an object of class &quot;c(&#39;dtplyr_step_first&#39;, &#39;dtplyr_step&#39;)&quot; By the way, this package is awesome if you really love dplyr, but want the speed of data.table. dtplyr is of course slightly slower than data.table because internal translations of dplyr language to data.table language have to happen first.55 I personally use data.table unless it is necessary to use dplyr like when dealing with sf objects. It is more concise than dplyr, which is somewhat verbose (yet expressive because of it). Ultimately, it is your personal preference which to use. You might be interested in reading this discussion about the comparative advantages and disadvantages of the two packages.↩︎ "],
["non-interactive-geometrical-operations.html", "2.9 Non-interactive geometrical operations", " 2.9 Non-interactive geometrical operations There are various geometrical operations that are particularly useful for economists. Here, some of the most commonly used geometrical operations are introduced56. You can see the practical use of some of these functions in Chapter 1.4. 2.9.1 st_buffer st_buffer() creates a buffer around points, lines, or the border of polygons. Let’s create buffers around points. First, we read well locations data. #--- read wells location data ---# urnrd_wells_sf &lt;- readRDS(&quot;./Data/urnrd_wells.rds&quot;) %&gt;% #--- project to UTM 14N WGS 84 ---# st_transform(32614) Here is the spatial distribution of the wells (Figure 2.3). tm_shape(urnrd_wells_sf) + tm_symbols(col = &quot;red&quot;, size = 0.1) + tm_layout(frame = FALSE) Figure 2.3: Map of the wells Let’s create buffers around the wells. #--- create a one-mile buffer around the wells ---# wells_buffer &lt;- st_buffer(urnrd_wells_sf, dist = 1600) As you can see, there are many circles around wells with the radius of \\(1,600\\) meters (Figure 2.4). tm_shape(wells_buffer) + tm_polygons(alpha = 0) + tm_shape(urnrd_wells_sf) + tm_symbols(col = &quot;red&quot;, size = 0.1) + tm_layout(frame = NA) Figure 2.4: Buffers around wells A practical application of buffer creation can be seen in Chapter 1.1. We now create buffers around polygons. First, read NE county boundary data and select three counties (Chase, Dundy, and Perkins). NE_counties &lt;- readRDS(&quot;./Data/NE_county_borders.rds&quot;) %&gt;% filter(NAME %in% c(&quot;Perkins&quot;, &quot;Dundy&quot;, &quot;Chase&quot;)) %&gt;% st_transform(32614) Here is what they look like (Figure 2.5): tm_shape(NE_counties) + tm_polygons(&#39;NAME&#39;, palette=&quot;RdYlGn&quot;, contrast=.3, title=&quot;County&quot;) + tm_layout(frame = NA) Figure 2.5: Map of the three counties The following code creates buffers around polygons (see the results in Figure 2.6): NE_buffer &lt;- st_buffer(NE_counties, dist = 2000) tm_shape(NE_buffer) + tm_polygons(col=&#39;blue&#39;,alpha=0.2) + tm_shape(NE_counties) + tm_polygons(&#39;NAME&#39;, palette=&quot;RdYlGn&quot;, contrast=.3, title=&quot;County&quot;) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.6: Buffers around the three counties For example, this can be useful to identify observations which are close to the border of political boundaries when you want to take advantage of spatial discontinuity of policies across adjacent political boundaries. 2.9.2 st_area The st_area() function calculates the area of polygons. #--- generate area by polygon ---# ( NE_counties &lt;- mutate(NE_counties, area = st_area(NE_counties)) ) Simple feature collection with 3 features and 10 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676 projected CRS: WGS 84 / UTM zone 14N STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 [m^2] 2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 [m^2] 3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530 [m^2] Now, as you can see below, the default class of the results of st_area() is units, which does not accept numerical operations. class(NE_counties$area) [1] &quot;units&quot; So, let’s turn it into double. ( NE_counties &lt;- mutate(NE_counties, area = as.numeric(area)) ) Simple feature collection with 3 features and 10 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676 projected CRS: WGS 84 / UTM zone 14N STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 3 3046331 MULTIPOLYGON (((240811.3 44... 2389890530 st_area() is useful when you want to find area-weighted average of characteristics after spatially joining two polygon layers using the st_intersection() function (See Chapter 3.3.3). 2.9.3 st_centroid The st_centroid() function finds the centroid of each polygon. #--- create centroids ---# ( NE_centroids &lt;- st_centroid(NE_counties) ) Simple feature collection with 3 features and 10 fields geometry type: POINT dimension: XY bbox: xmin: 271156.7 ymin: 4450826 xmax: 276594.1 ymax: 4525635 projected CRS: WGS 84 / UTM zone 14N STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 POINT (276594.1 4525635) 2302174854 2 7978172 POINT (271469.9 4489429) 2316908196 3 3046331 POINT (271156.7 4450826) 2389890530 Here’s the map of the output (Figure 2.7). tm_shape(NE_counties) + tm_polygons() + tm_shape(NE_centroids)+ tm_symbols(size=0.5) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.7: The centroids of the polygons It can be useful when creating a map with labels because the centroid of polygons tend to be a good place to place labels (Figure 2.8).57 tm_shape(NE_counties) + tm_polygons() + tm_shape(NE_centroids)+ tm_text(&quot;NAME&quot;) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.8: County names placed at the centroids of the counties It may be also useful when you need to calculate the “distance” between polygons. 2.9.4 st_length We can use st_length() to calculate great circle distances58 of LINESTRING and MULTILINESTRING when they are represented in geodetic coordinates. On the other hand, if they are projected and use a Cartesian coordinate system, it will calculate Euclidean distance. We use U.S. railroad data for a demonstration. #--- import US railroad data and take only the first 10 of it ---# ( a_railroad &lt;- rail_roads &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;tl_2015_us_rails&quot;)[1:10, ] ) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 geographic CRS: NAD83 Simple feature collection with 10 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776 geographic CRS: NAD83 LINEARID FULLNAME MTFCC geometry 1 11020239500 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058... 2 11020239501 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 3 11020239502 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819... 4 11020239503 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 5 11020239504 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031... 6 11020239575 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 7 11020239576 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852... 8 11020239577 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 9 11020239589 Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736... 10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848... #--- check CRS ---# st_crs(a_railroad) Coordinate Reference System: User input: NAD83 wkt: GEOGCRS[&quot;NAD83&quot;, DATUM[&quot;North American Datum 1983&quot;, ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, LENGTHUNIT[&quot;metre&quot;,1]]], PRIMEM[&quot;Greenwich&quot;,0, ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], CS[ellipsoidal,2], AXIS[&quot;latitude&quot;,north, ORDER[1], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], AXIS[&quot;longitude&quot;,east, ORDER[2], ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ID[&quot;EPSG&quot;,4269]] It uses geodetic coordinate system. Let’s calculate the great circle distance of the lines (Chapter 1.4 for a practical use case of this function). ( a_railroad &lt;- mutate(a_railroad, length = st_length(a_railroad)) ) Simple feature collection with 10 features and 4 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776 geographic CRS: NAD83 LINEARID FULLNAME MTFCC geometry 1 11020239500 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058... 2 11020239501 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 3 11020239502 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819... 4 11020239503 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 5 11020239504 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031... 6 11020239575 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 7 11020239576 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852... 8 11020239577 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 9 11020239589 Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736... 10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848... length 1 661.3381 [m] 2 657.4261 [m] 3 19982.5998 [m] 4 13888.3385 [m] 5 7194.7745 [m] 6 1061.2335 [m] 7 7824.0945 [m] 8 31756.9803 [m] 9 4547.1970 [m] 10 17103.0691 [m] For the complete list of available geometrical operations under the sf package, see here.↩︎ When creating maps with the ggplot2 package, you can use geom_sf_text() or geom_sf_label(), which automatically finds where to put texts. See some examples here.↩︎ Great circle distance is the shortest distance between two points on the surface of a sphere (earth)↩︎ "],
["int-vv.html", "Chapter 3 Spatial Interactions of Vector Data: Subsetting and Joining ", " Chapter 3 Spatial Interactions of Vector Data: Subsetting and Joining "],
["before-you-start-2.html", "Before you start", " Before you start In this chapter we learn the spatial interactions of two spatial objects. We first look at the topological relations of two spatial objects (how they are spatially related with each other): specifically, st_intersects() and st_is_within_distance(). st_intersects() is particularly important as it is by far the most common topological relation economists will use and also because it is the default topological relation that sf uses for spatial subsetting and spatial joining. We then follow with spatial subsetting: filtering spatial data by the geographic features of another spatial data. Finally, we will learn spatial joining. Spatial joining is the act of assigning attribute values from one spatial data to another spatial data based on how the two spatial datasets are spatially related (topological relation). This is the most important spatial operation for economists who want to use spatial variables in their econometric analysis. For those who have used the sp package, these operations are akin to sp::over(). Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( sf, # vector data operations dplyr, # data wrangling data.table, # data wrangling ggplot2, # for map creation tmap # for map creation ) "],
["topo.html", "3.1 Topological relations", " 3.1 Topological relations Before we learn spatial subsetting and joining, we first look at topological relations. Topological relations refer to the way multiple spatial objects are spatially related to one another. You can identify various types of spatial relations using the sf package. Our main focus is on the intersections of spatial objects, which can be found using st_intersects().59 We also briefly cover st_is_within_distance()60. We first create sf objects we are going to use for illustrations. POINTS #--- create points ---# point_1 &lt;- st_point(c(2, 2)) point_2 &lt;- st_point(c(1, 1)) point_3 &lt;- st_point(c(1, 3)) #--- combine the points to make a single sf of points ---# ( points &lt;- list(point_1, point_2, point_3) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(point_name = c(&quot;point 1&quot;, &quot;point 2&quot;, &quot;point 3&quot;)) ) Simple feature collection with 3 features and 1 field geometry type: POINT dimension: XY bbox: xmin: 1 ymin: 1 xmax: 2 ymax: 3 CRS: NA x point_name 1 POINT (2 2) point 1 2 POINT (1 1) point 2 3 POINT (1 3) point 3 LINES #--- create points ---# line_1 &lt;- st_linestring(rbind(c(0, 0), c(2.5, 0.5))) line_2 &lt;- st_linestring(rbind(c(1.5, 0.5), c(2.5, 2))) #--- combine the points to make a single sf of points ---# ( lines &lt;- list(line_1, line_2) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(line_name = c(&quot;line 1&quot;, &quot;line 2&quot;)) ) Simple feature collection with 2 features and 1 field geometry type: LINESTRING dimension: XY bbox: xmin: 0 ymin: 0 xmax: 2.5 ymax: 2 CRS: NA x line_name 1 LINESTRING (0 0, 2.5 0.5) line 1 2 LINESTRING (1.5 0.5, 2.5 2) line 2 POLYGONS #--- create polygons ---# polygon_1 &lt;- st_polygon(list( rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)) )) polygon_2 &lt;- st_polygon(list( rbind(c(0.5, 1.5), c(0.5, 3.5), c(2.5, 3.5), c(2.5, 1.5), c(0.5, 1.5)) )) polygon_3 &lt;- st_polygon(list( rbind(c(0.5, 2.5), c(0.5, 3.2), c(2.3, 3.2), c(2, 2), c(0.5, 2.5)) )) #--- combine the polygons to make an sf of polygons ---# ( polygons &lt;- list(polygon_1, polygon_2, polygon_3) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(polygon_name = c(&quot;polygon 1&quot;, &quot;polygon 2&quot;, &quot;polygon 3&quot;)) ) Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: 0 ymin: 0 xmax: 2.5 ymax: 3.5 CRS: NA x polygon_name 1 POLYGON ((0 0, 2 0, 2 2, 0 ... polygon 1 2 POLYGON ((0.5 1.5, 0.5 3.5,... polygon 2 3 POLYGON ((0.5 2.5, 0.5 3.2,... polygon 3 Figure 3.1 shows how they look: ggplot() + geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) + scale_fill_discrete(name = &quot;Polygons&quot;) + geom_sf(data = lines, aes(color = line_name)) + scale_color_discrete(name = &quot;Lines&quot;) + geom_sf(data = points, aes(shape = point_name), size = 3) + scale_shape_discrete(name = &quot;Points&quot;) Figure 3.1: Visualization of the points, lines, and polygons 3.1.1 st_intersects() This function identifies which sfg object in an sf (or sfc) intersects with sfg object(s) in another sf. For example, you can use the function to identify which well is located within which county. st_intersects() is the most commonly used topological relation. It is important to understand what it does as it is the default topological relation used when performing spatial subsetting and joining, which we will cover later. points and polygons st_intersects(points, polygons) Sparse geometry binary predicate list of length 3, where the predicate was `intersects&#39; 1: 1, 2, 3 2: 1 3: 2, 3 As you can see, the output is a list of which polygon(s) each of the points intersect with. The numbers 1, 2, and 3 in the first row mean that 1st (polygon 1), 2nd (polygon 2), and 3rd (polygon 3) objects of the polygons intersect with the first point (point 1) of the points object. The fact that point 1 is considered to be intersecting with polygon 2 means that the area inside the border is considered a part of the polygon (of course). If you would like the results of st_intersects() in a matrix form with boolean values filling the matrix, you can add sparse = FALSE option. st_intersects(points, polygons, sparse = FALSE) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE FALSE FALSE [3,] FALSE TRUE TRUE lines and polygons st_intersects(lines, polygons) Sparse geometry binary predicate list of length 2, where the predicate was `intersects&#39; 1: 1 2: 1, 2 The output is a list of which polygon(s) each of the lines intersect with. polygons and polygons For polygons vs polygons interaction, st_intersects() identifies any polygons that either touches (even at a point like polygons 1 and 3) or share some area. st_intersects(polygons, polygons) Sparse geometry binary predicate list of length 3, where the predicate was `intersects&#39; 1: 1, 2, 3 2: 1, 2, 3 3: 1, 2, 3 3.1.2 st_is_within_distance() This function identifies whether two spatial objects are within the distance you specify as the name suggests61. Let’s first create two sets of points. set.seed(38424738) points_set_1 &lt;- lapply(1:5, function(x) st_point(runif(2))) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) points_set_2 &lt;- lapply(1:5, function(x) st_point(runif(2))) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) Here is how they are spatially distributed (Figure 3.2). Instead of circles of points, their corresponding id (or equivalently row number here) values are displayed. ggplot() + geom_sf_text(data = points_set_1, aes(label = id), color = &quot;red&quot;) + geom_sf_text(data = points_set_2, aes(label = id), color = &quot;blue&quot;) Figure 3.2: The locations of the set of points We want to know which of the blue points (points_set_2) are located within 0.2 from each of the red points (points_set_1). The following figure (Figure 3.3) gives us the answer visually. #--- create 0.2 buffers around points in points_set_1 ---# buffer_1 &lt;- st_buffer(points_set_1, dist = 0.2) ggplot() + geom_sf(data = buffer_1, color = &quot;red&quot;, fill = NA) + geom_sf_text(data = points_set_1, aes(label = id), color = &quot;red&quot;) + geom_sf_text(data = points_set_2, aes(label = id), color = &quot;blue&quot;) Figure 3.3: The blue points within 0.2 radius of the red points Confirm your visual inspection results with the outcome of the following code using st_is_within_distance() function. st_is_within_distance(points_set_1, points_set_2, dist = 0.2) Sparse geometry binary predicate list of length 5, where the predicate was `is_within_distance&#39; 1: 1 2: (empty) 3: (empty) 4: (empty) 5: 3 I would say it is very rare that you use other topological relations like st_within() or st_touches().↩︎ Run ?geos_binary_pred to see other topological relations you can find.↩︎ This function can be useful to identify neighbors. For example, you may want to find irrigation wells located around well \\(i\\) to label them as well \\(i\\)’s neighbor.↩︎ "],
["spatial-subsetting-or-flagging.html", "3.2 Spatial Subsetting (or Flagging)", " 3.2 Spatial Subsetting (or Flagging) Spatial subsetting refers to operations that narrow down the geographic scope of a spatial object (source data) based on another spatial object (target data). We illustrate spatial subsetting using Kansas county borders, the boundary of the High-Plains Aquifer (HPA), and agricultural irrigation wells in Kansas. First, let’s import all the files we will use in this section. #--- Kansas county borders ---# KS_counties &lt;- readRDS(&quot;./Data/KS_county_borders.rds&quot;) #--- HPA boundary ---# hpa &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;hp_bound2010&quot;) %&gt;% .[1, ] %&gt;% st_transform(st_crs(KS_counties)) #--- all the irrigation wells in KS ---# KS_wells &lt;- readRDS(&quot;./Data/Kansas_wells.rds&quot;) %&gt;% st_transform(st_crs(KS_counties)) #--- US railroad ---# rail_roads &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_transform(st_crs(KS_counties)) 3.2.1 polygons (source) vs polygons (target) The following map (Figure 3.4) shows the Kansas portion of the HPA and Kansas counties.62 #--- add US counties layer ---# tm_shape(KS_counties) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) Figure 3.4: Kansas portion of High-Plains Aquifer and Kansas counties The goal here is to select only the counties that intersect with the HPA boundary. When subsetting a data.frame by specifying the row numbers you would like to select, you can do #--- NOT RUN ---# data.frame[vector of row numbers, ] Spatial subsetting of sf objects works in a similar syntax: #--- NOT RUN ---# sf_1[sf_2, ] where you are subsetting sf_1 based on sf_2. Instead of row numbers, you provide another sf object in place. The following code spatially subsets Kansas counties based on the HPA boundary. counties_in_hpa &lt;- KS_counties[hpa, ] See the results below in Figure 3.5. #--- add US counties layer ---# tm_shape(counties_in_hpa) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) Figure 3.5: The results of spatially subsetting Kansas counties based on HPA boundary You can see that only the counties that intersect with the HPA boundary remained. This is because when you use the above syntax of sf_1[sf_2, ], the default underlying topological relations is st_intersects(). So, if an object in sf_1 intersects with any of the objects in sf_2 even slightly, then it will remain after subsetting. You can specify the spatial operation to be used as an option as in #--- NOT RUN ---# sf_1[sf_2, op = topological_relation_type] For example, if you only want counties that are completely within the HPA boundary, you can do the following (the map of the results in Figure 3.6): counties_within_hpa &lt;- KS_counties[hpa, , op = st_within] #--- add US counties layer ---# tm_shape(counties_within_hpa) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) Figure 3.6: Kansas counties that are completely within HPA boundary Sometimes, you just want to flag whether two spatial objects intersect or not, instead of dropping non-overlapping observations. In that case, you can use st_intersects(). #--- check the intersections of HPA and counties ---# intersects_hpa &lt;- st_intersects(KS_counties, hpa, sparse = FALSE) #--- take a look ---# head(intersects_hpa) [,1] [1,] FALSE [2,] TRUE [3,] FALSE [4,] TRUE [5,] TRUE [6,] TRUE #--- assign the index as a variable ---# KS_counties &lt;- mutate(KS_counties, intersects_hpa = intersects_hpa) #--- take a look ---# dplyr::select(KS_counties, COUNTYFP, intersects_hpa) Simple feature collection with 105 features and 2 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: COUNTYFP intersects_hpa geometry 1 133 FALSE MULTIPOLYGON (((-95.5255 37... 2 075 TRUE MULTIPOLYGON (((-102.0446 3... 3 123 FALSE MULTIPOLYGON (((-98.48738 3... 4 189 TRUE MULTIPOLYGON (((-101.5566 3... 5 155 TRUE MULTIPOLYGON (((-98.47279 3... 6 129 TRUE MULTIPOLYGON (((-102.0419 3... 7 073 FALSE MULTIPOLYGON (((-96.52278 3... 8 023 TRUE MULTIPOLYGON (((-102.0517 4... 9 089 TRUE MULTIPOLYGON (((-98.50445 4... 10 059 FALSE MULTIPOLYGON (((-95.50827 3... 3.2.2 points (source) vs polygons (target) The following map (Figure 3.7) shows the Kansas portion of the HPA and all the irrigation wells in Kansas. tm_shape(KS_wells) + tm_symbols(size = 0.1) + tm_shape(hpa) + tm_polygons(col = &quot;blue&quot;, alpha = 0.1) Figure 3.7: A map of Kansas irrigation wells and HPA We can select only wells that reside within the HPA boundary using the same syntax as the above example. KS_wells_in_hpa &lt;- KS_wells[hpa, ] As you can see in Figure 3.8 below, only the wells that are inside (or intersect with) the HPA remained because the default topological relation is st_intersects(). tm_shape(KS_wells_in_hpa) + tm_symbols(size = 0.1) + tm_shape(hpa) + tm_polygons(col = &quot;blue&quot;, alpha = 0.1) Figure 3.8: A map of Kansas irrigation wells and HPA If you just want to flag wells that intersects with HPA instead of dropping the non-intersecting wells, use st_intersects(): #--- wells ---# KS_wells &lt;- mutate(KS_wells, in_hpa = st_intersects(KS_wells, hpa, sparse = FALSE)) #--- take a look ---# dplyr::select(KS_wells, site, in_hpa) Simple feature collection with 37647 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199 geographic CRS: NAD83 First 10 features: site in_hpa geometry 1 1 TRUE POINT (-100.4423 37.52046) 2 3 TRUE POINT (-100.7118 39.91526) 3 5 TRUE POINT (-99.15168 38.48849) 4 7 TRUE POINT (-101.8995 38.78077) 5 8 TRUE POINT (-100.7122 38.0731) 6 9 FALSE POINT (-97.70265 39.04055) 7 11 TRUE POINT (-101.7114 39.55035) 8 12 FALSE POINT (-95.97031 39.16121) 9 15 TRUE POINT (-98.30759 38.26787) 10 17 TRUE POINT (-100.2785 37.71539) 3.2.3 lines (source) vs polygons (target) The following map (Figure 3.9) shows the Kansas counties and U.S. railroads. tm_shape(rail_roads) + tm_lines(col = &quot;blue&quot;) + tm_shape(KS_counties) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 3.9: U.S. railroads and Kansas county boundaries We can select only railroads that intersect with Kansas. railroads_KS &lt;- rail_roads[KS_counties, ] As you can see in Figure 3.10 below, only the railroads that intersect with Kansas were selected. Note the lines that go beyond the Kansas boundary are also selected. Remember, the default is st_intersect(). If you would like the lines beyond the state boundary to be cut out but the intersecting parts of those lines to remain, use st_intersection(), which is explained in Chapter @ref(st_intersection). tm_shape(railroads_KS) + tm_lines(col = &quot;blue&quot;) + tm_shape(KS_counties) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 3.10: Railroads that intersect Kansas county boundaries Unlike the previous two cases, multiple objects (lines) are checked against multiple objects (polygons) for intersection63. Therefore, we cannot use the strategy we took above of returning a vector of true or false using sparse = TRUE option. Here, we need to count the number of intersecting counties and then assign TRUE if the number is greater than 0. #--- check the number of intersecting KS counties ---# int_mat &lt;- st_intersects(rail_roads, KS_counties) %&gt;% lapply(length) %&gt;% unlist() #--- railroads ---# rail_roads &lt;- mutate(rail_roads, intersect_ks = int_mat &gt; 0) #--- take a look ---# dplyr::select(rail_roads, LINEARID, intersect_ks) Simple feature collection with 180958 features and 2 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 geographic CRS: NAD83 First 10 features: LINEARID intersect_ks geometry 1 11020239500 FALSE MULTILINESTRING ((-79.47058... 2 11020239501 FALSE MULTILINESTRING ((-79.46687... 3 11020239502 FALSE MULTILINESTRING ((-79.66819... 4 11020239503 FALSE MULTILINESTRING ((-79.46687... 5 11020239504 FALSE MULTILINESTRING ((-79.74031... 6 11020239575 FALSE MULTILINESTRING ((-79.43695... 7 11020239576 FALSE MULTILINESTRING ((-79.47852... 8 11020239577 FALSE MULTILINESTRING ((-79.43695... 9 11020239589 FALSE MULTILINESTRING ((-79.38736... 10 11020239591 FALSE MULTILINESTRING ((-79.53848... 3.2.4 polygons (source) vs points (target) The following map (Figure 3.11) shows the Kansas counties and irrigation wells in Kansas that overlie HPA. tm_shape(KS_counties) + tm_polygons(alpha = 0) + tm_shape(KS_wells_in_hpa) + tm_symbols(size = 0.1) + tm_layout(frame = FALSE) Figure 3.11: U.S. railroads and Kansas county boundaries We can select only counties that intersect with at least one well. KS_counties_intersected &lt;- KS_counties[KS_wells_in_hpa, ] As you can see in Figure 3.12 below, only the counties that intersect with at least one well remained. tm_shape(KS_counties) + tm_polygons(col = NA) + tm_shape(KS_counties_intersected) + tm_polygons(col =&quot;blue&quot;, alpha = 0.6) + tm_layout(frame = FALSE) Figure 3.12: Counties that do not have any wells To flag counties that have at least one well, use st_intersects() as follows: int_mat &lt;- st_intersects(KS_counties, KS_wells_in_hpa) %&gt;% lapply(length) %&gt;% unlist() #--- railroads ---# KS_counties &lt;- mutate(KS_counties, intersect_wells = int_mat &gt; 0) #--- take a look ---# dplyr::select(KS_counties, NAME, COUNTYFP, intersect_wells) Simple feature collection with 105 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: NAME COUNTYFP intersect_wells geometry 1 Neosho 133 FALSE MULTIPOLYGON (((-95.5255 37... 2 Hamilton 075 TRUE MULTIPOLYGON (((-102.0446 3... 3 Mitchell 123 FALSE MULTIPOLYGON (((-98.48738 3... 4 Stevens 189 TRUE MULTIPOLYGON (((-101.5566 3... 5 Reno 155 TRUE MULTIPOLYGON (((-98.47279 3... 6 Morton 129 TRUE MULTIPOLYGON (((-102.0419 3... 7 Greenwood 073 FALSE MULTIPOLYGON (((-96.52278 3... 8 Cheyenne 023 TRUE MULTIPOLYGON (((-102.0517 4... 9 Jewell 089 TRUE MULTIPOLYGON (((-98.50445 4... 10 Franklin 059 FALSE MULTIPOLYGON (((-95.50827 3... 3.2.5 Subsetting to a geograpchic extent (bounding box) We can use st_crop() to subset (crop) spatial objects to a spatial bounding box (extent) of a spatial object. The bounding box of an sf is a rectangle represented by the minimum and maximum of x and y that encompass/contain all the spatial objects in the sf. You can use st_bbox() to find the bounding box of an sf object. Let’s get the bounding box of KS_wells_in_hpa (irrigation wells in Kansas that overlie HPA). #--- get the bounding box of KS_wells ---# ( bbox_KS_wells_in_hpa &lt;- st_bbox(KS_wells_in_hpa) ) xmin ymin xmax ymax -102.04953 36.99552 -97.33193 40.00199 #--- check the class ---# class(bbox_KS_wells_in_hpa) [1] &quot;bbox&quot; Visualizing the bounding box (Figure 3.13): tm_shape(KS_counties) + tm_polygons(alpha = 0) + tm_shape(KS_wells_in_hpa) + tm_symbols(size = 0.1) + tm_shape(st_as_sfc(bbox_KS_wells_in_hpa)) + tm_borders(col = &quot;red&quot;) + tm_layout(frame = NA) Figure 3.13: The bounding box of the irrigation wells in Kansas that overlie HPA When you use a bounding box to crop an sf objects, you can consider the bounding box as a single polygon. Let’s crop KS_counties using the bbox of the irrigation wells. KS_cropped &lt;- st_crop(KS_counties, bbox_KS_wells_in_hpa) Here is what the cropped data looks like (Figure 3.14): tm_shape(KS_counties) + tm_polygons(col = NA) + tm_shape(KS_cropped) + tm_polygons(col =&quot;blue&quot;, alpha = 0.6) + tm_layout(frame = NA) Figure 3.14: The bounding box of the irrigation wells in Kansas that overlie HPA As you can see, the st_crop() operation cut some counties at the right edge of the bounding box. So, st_crop() is invasive. If you do not like this to happen and want the complete original counties that have at least one well, you can use the subset approach using [, ] after converting the bounding box to an sfc as follows: KS_complete_counties &lt;- KS_counties[st_as_sfc(bbox_KS_wells_in_hpa), ] Here is what the subsetted Kansas county data looks like (Figure 3.15): tm_shape(KS_counties) + tm_polygons(col = NA) + tm_shape(KS_complete_counties) + tm_polygons(col =&quot;blue&quot;, alpha = 0.6) + tm_layout(frame = NA) Figure 3.15: The bounding box of the irrigation wells in Kansas that overlie HPA Notice the difference between the result of this operation and the case where we used KS_wells_in_hpa directly to subset KS_counties as shown in Figure 3.12. The current approach includes counties that do not have any irrigation wells inside them. If you are a Windows user, you may find that only the KS_counties are shown on the created map. There seems to be a limitation to graphics in Windows. See here.↩︎ Of course, this situation arises for a polygons-polygons case as well. The above polygons-polygons example was an exception because hpa had only one polygon object.↩︎ "],
["sp-join.html", "3.3 Spatial Join", " 3.3 Spatial Join By spatial join, we mean spatial operations that involve all of the following: overlay one spatial layer (target layer) onto another spatial layer (source layer) for each of the observation in the target layer identify which objects in the source layer it geographically intersects (or a different topological relation) with extract values associated with the intersecting objects in the source layer (and summarize if necessary), assign the extracted value to the object in the target layer For economists, this is probably the most common motivation for using GIS software, with the ultimate goal being to include the spatially joined variables as covariates in regression analysis. We can classify spatial join into four categories by the type of the underlying spatial objects: vector-vector: vector data (target) against vector data (source) vector-raster: vector data (target) against raster data (source) raster-vector: raster data (target) against vector data (source) raster-raster: raster data (target) against raster data (source) Among the four, our focus here is the first case. The second case will be discussed in Chapter 5. We will not cover the third and fourth cases in this course because it is almost always the case that our target data is a vector data (e.g., city or farm fields as points, political boundaries as polygons, etc). Category 1 can be further broken down into different sub categories depending on the type of spatial object (point, line, and polygon). Here, we will ignore any spatial joins that involve lines. This is because objects represented by lines are rarely observation units in econometric analysis nor the source data from which we will extract values.64 Here is the list of the types of spatial joins we will learn. points (target) against polygons (source) polygons (target) against points (source) polygons (target) against polygons (source) 3.3.1 Case 1: points (target) vs polygons (source) Case 1, for each of the observations (points) in the target data, finds which polygon in the source file it intersects, and then assign the value associated with the polygon to the point65. In order to achieve this, we can use the st_join() function, whose syntax is as follows: #--- NOT RUN ---# st_join(target_sf, source_sf) Similar to spatial subsetting, the default topological relation is st_intersects()66. We use the Kansas irrigation well data (points) and Kansas county boundary data (polygons) for a demonstration. Our goal is to assign the county-level corn price information from the Kansas county data to wells. First let me create and add a fake county-level corn price variable to the Kansas county data. KS_corn_price &lt;- KS_counties %&gt;% mutate( corn_price = seq(3.2, 3.9, length = nrow(.)) ) %&gt;% dplyr::select(COUNTYFP, corn_price) Here is the map of Kansas counties color-differentiated by fake corn price (Figure 3.16): tm_shape(KS_corn_price) + tm_polygons(col = &quot;corn_price&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.16: Map of county-level fake corn price For this particular context, the following code will do the job: #--- spatial join ---# ( KS_wells_County &lt;- st_join(KS_wells, KS_corn_price) ) Simple feature collection with 37647 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199 geographic CRS: NAD83 First 10 features: site af_used in_hpa COUNTYFP corn_price geometry 1 1 232.099948 TRUE 069 3.556731 POINT (-100.4423 37.52046) 2 3 13.183940 TRUE 039 3.449038 POINT (-100.7118 39.91526) 3 5 99.187052 TRUE 165 3.287500 POINT (-99.15168 38.48849) 4 7 0.000000 TRUE 199 3.644231 POINT (-101.8995 38.78077) 5 8 145.520499 TRUE 055 3.832692 POINT (-100.7122 38.0731) 6 9 3.614535 FALSE 143 3.799038 POINT (-97.70265 39.04055) 7 11 188.423543 TRUE 181 3.590385 POINT (-101.7114 39.55035) 8 12 77.335960 FALSE 177 3.550000 POINT (-95.97031 39.16121) 9 15 0.000000 TRUE 159 3.610577 POINT (-98.30759 38.26787) 10 17 167.819034 TRUE 069 3.556731 POINT (-100.2785 37.71539) You can see from Figure 3.17 below that all the wells inside the same county have the same corn price value. tm_shape(KS_counties) + tm_polygons() + tm_shape(KS_wells_County) + tm_symbols(col = &quot;corn_price&quot;, size = 0.1) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.17: Map of wells color-differentiated by corn price 3.3.2 Case 2: polygons (target) vs points (source) Case 2, for each of the observations (polygons) in the target data, find which observations (points) in the source file it intersects, and then assign the values associated with the points to the polygon. We use the same function: st_join()67. Suppose you are now interested in county-level analysis and you would like to get county-level total groundwater pumping. The target file is KS_counties, and the source file is KS_wells. #--- spatial join ---# KS_County_wells &lt;- st_join(KS_counties, KS_wells) #--- take a look ---# dplyr::select(KS_County_wells, COUNTYFP, site, af_used) Simple feature collection with 37652 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: COUNTYFP site af_used geometry 1 133 53861 17.01790 MULTIPOLYGON (((-95.5255 37... 1.1 133 70592 0.00000 MULTIPOLYGON (((-95.5255 37... 2 075 328 394.04513 MULTIPOLYGON (((-102.0446 3... 2.1 075 336 80.65036 MULTIPOLYGON (((-102.0446 3... 2.2 075 436 568.25359 MULTIPOLYGON (((-102.0446 3... 2.3 075 1007 215.80416 MULTIPOLYGON (((-102.0446 3... 2.4 075 1170 0.00000 MULTIPOLYGON (((-102.0446 3... 2.5 075 1192 77.39120 MULTIPOLYGON (((-102.0446 3... 2.6 075 1249 0.00000 MULTIPOLYGON (((-102.0446 3... 2.7 075 1300 320.22612 MULTIPOLYGON (((-102.0446 3... As you can see in the resulting dataset, all the unique polygon - point intersecting combinations comprise the observations. For each of the polygons, you will have as many observations as the number of wells that intersect with the polygon. Once you join the two layers, you can find statistics by polygon (county here). Since we want groundwater extraction by county, the following does the job. KS_County_wells %&gt;% group_by(COUNTYFP) %&gt;% summarize(af_used = sum(af_used, na.rm = TRUE)) Simple feature collection with 105 features and 2 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 # A tibble: 105 x 3 COUNTYFP af_used geometry &lt;fct&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; 1 001 0 (((-95.51931 37.82026, -95.51897 38.03823, -95.07788 38.037… 2 003 0 (((-95.50833 38.39028, -95.06583 38.38994, -95.07788 38.037… 3 005 771. (((-95.56413 39.65287, -95.33974 39.65298, -95.11519 39.652… 4 007 4972. (((-99.0126 37.47042, -98.46466 37.47101, -98.46493 37.3841… 5 009 61083. (((-99.03297 38.69676, -98.48611 38.69688, -98.47991 38.681… 6 011 0 (((-95.08808 37.73248, -95.07969 37.8198, -95.07788 38.0377… 7 013 480. (((-95.78811 40.00047, -95.78457 40.00046, -95.3399 40.0000… 8 015 343. (((-97.15248 37.91273, -97.15291 38.0877, -96.84077 38.0856… 9 017 0 (((-96.83765 38.34864, -96.81951 38.52245, -96.35378 38.521… 10 019 0 (((-96.52487 37.30273, -95.9644 37.29923, -95.96427 36.9992… # … with 95 more rows Of course, it is just as easy to get other types of statistics by simply modifying the summarize() part. However, this two-step process can actually be done in one step using aggregate(), in which you specify how you want to aggregate with the FUN option as follows: #--- mean ---# aggregate(KS_wells, KS_counties, FUN = mean) Simple feature collection with 105 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: site af_used in_hpa geometry 1 62226.50 8.508950 0.0000000 MULTIPOLYGON (((-95.5255 37... 2 35184.64 176.390742 0.4481793 MULTIPOLYGON (((-102.0446 3... 3 40086.82 35.465123 0.0000000 MULTIPOLYGON (((-98.48738 3... 4 40179.41 285.672916 1.0000000 MULTIPOLYGON (((-101.5566 3... 5 51249.39 46.048048 0.9743783 MULTIPOLYGON (((-98.47279 3... 6 33033.13 202.612377 1.0000000 MULTIPOLYGON (((-102.0419 3... 7 29840.40 0.000000 0.0000000 MULTIPOLYGON (((-96.52278 3... 8 28235.82 94.585634 0.9736842 MULTIPOLYGON (((-102.0517 4... 9 36180.06 44.033911 0.3000000 MULTIPOLYGON (((-98.50445 4... 10 40016.00 1.142775 0.0000000 MULTIPOLYGON (((-95.50827 3... #--- sum ---# aggregate(KS_wells, KS_counties, FUN = sum) Simple feature collection with 105 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: site af_used in_hpa geometry 1 124453 1.701790e+01 0 MULTIPOLYGON (((-95.5255 37... 2 12560917 6.297149e+04 160 MULTIPOLYGON (((-102.0446 3... 3 1964254 1.737791e+03 0 MULTIPOLYGON (((-98.48738 3... 4 42389277 3.013849e+05 1055 MULTIPOLYGON (((-101.5566 3... 5 68007942 6.110576e+04 1293 MULTIPOLYGON (((-98.47279 3... 6 15756801 9.664610e+04 477 MULTIPOLYGON (((-102.0419 3... 7 149202 0.000000e+00 0 MULTIPOLYGON (((-96.52278 3... 8 17167377 5.750807e+04 592 MULTIPOLYGON (((-102.0517 4... 9 1809003 2.201696e+03 15 MULTIPOLYGON (((-98.50445 4... 10 160064 4.571102e+00 0 MULTIPOLYGON (((-95.50827 3... Notice that the mean() function was applied to all the columns in KS_wells, including site id number. So, you might want to select variables you want to join before you apply the aggregate() function like this: aggregate(dplyr::select(KS_wells, af_used), KS_counties, FUN = mean) Simple feature collection with 105 features and 1 field geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 geographic CRS: NAD83 First 10 features: af_used geometry 1 8.508950 MULTIPOLYGON (((-95.5255 37... 2 176.390742 MULTIPOLYGON (((-102.0446 3... 3 35.465123 MULTIPOLYGON (((-98.48738 3... 4 285.672916 MULTIPOLYGON (((-101.5566 3... 5 46.048048 MULTIPOLYGON (((-98.47279 3... 6 202.612377 MULTIPOLYGON (((-102.0419 3... 7 0.000000 MULTIPOLYGON (((-96.52278 3... 8 94.585634 MULTIPOLYGON (((-102.0517 4... 9 44.033911 MULTIPOLYGON (((-98.50445 4... 10 1.142775 MULTIPOLYGON (((-95.50827 3... 3.3.3 Case 3: polygons (target) vs polygons (source) For this case, st_join(target_sf, source_sf) will return all the unique intersecting polygon-polygon combinations with the information of the polygon from source_sf attached. We will use county-level corn acres in Iowa in 2018 from USDA NASS68 and Hydrologic Units69 Our objective here is to find corn acres by HUC units based on the county-level corn acres data70. We first import the Iowa corn acre data: #--- IA boundary ---# IA_corn &lt;- readRDS(&quot;./Data/IA_corn.rds&quot;) #--- take a look ---# IA_corn Simple feature collection with 93 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687 projected CRS: NAD83 / UTM zone 15N First 10 features: county_code year acres geometry 1 083 2018 183500 MULTIPOLYGON (((458997 4711... 2 141 2018 167000 MULTIPOLYGON (((267700.8 47... 3 081 2018 184500 MULTIPOLYGON (((421231.2 47... 4 019 2018 189500 MULTIPOLYGON (((575285.6 47... 5 023 2018 165500 MULTIPOLYGON (((497947.5 47... 6 195 2018 111500 MULTIPOLYGON (((459791.6 48... 7 063 2018 110500 MULTIPOLYGON (((345214.3 48... 8 027 2018 183000 MULTIPOLYGON (((327408.5 46... 9 121 2018 70000 MULTIPOLYGON (((396378.1 45... 10 077 2018 107000 MULTIPOLYGON (((355180.1 46... Here is the map of Iowa counties color-differentiated by corn acres (Figure 3.18): #--- here is the map ---# tm_shape(IA_corn) + tm_polygons(col = &quot;acres&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.18: Map of Iowa counties color-differentiated by corn planted acreage Now import the HUC units data: #--- import HUC units ---# HUC_IA &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;huc250k&quot;) %&gt;% dplyr::select(HUC_CODE) %&gt;% #--- reproject to the CRS of IA ---# st_transform(st_crs(IA_corn)) %&gt;% #--- select HUC units that overlaps with IA ---# .[IA_corn, ] Here is the map of HUC units (Figure 3.19): tm_shape(HUC_IA) + tm_polygons() + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.19: Map of HUC units that intersect with Iowa state boundary Here is a map of Iowa counties with HUC units superimposed on top (Figure 3.20): tm_shape(IA_corn) + tm_polygons(col = &quot;acres&quot;) + tm_shape(HUC_IA) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.20: Map of HUC units superimposed on the counties in Iowas Spatial joining will produce the following. ( HUC_joined &lt;- st_join(HUC_IA, IA_corn) ) Simple feature collection with 349 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 154941.7 ymin: 4346327 xmax: 773299.8 ymax: 4907735 projected CRS: NAD83 / UTM zone 15N First 10 features: HUC_CODE county_code year acres geometry 608 10170203 149 2018 226500 POLYGON ((235551.4 4907513,... 608.1 10170203 167 2018 249000 POLYGON ((235551.4 4907513,... 608.2 10170203 193 2018 201000 POLYGON ((235551.4 4907513,... 608.3 10170203 119 2018 184500 POLYGON ((235551.4 4907513,... 621 07020009 063 2018 110500 POLYGON ((408580.7 4880798,... 621.1 07020009 109 2018 304000 POLYGON ((408580.7 4880798,... 621.2 07020009 189 2018 120000 POLYGON ((408580.7 4880798,... 627 10170204 141 2018 167000 POLYGON ((248115.2 4891652,... 627.1 10170204 143 2018 116000 POLYGON ((248115.2 4891652,... 627.2 10170204 167 2018 249000 POLYGON ((248115.2 4891652,... Each of the intersecting HUC-county combinations becomes an observation with its resulting geometry the same as the geometry of the HUC unit. To see this, let’s take a look at one of the HUC units. The HUC unit with HUC_CODE ==10170203 intersects with four County. #--- get the HUC unit with `HUC_CODE ==10170203` ---# ( temp_HUC_county &lt;- filter(HUC_joined, HUC_CODE == 10170203) ) Simple feature collection with 4 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 154941.7 ymin: 4709628 xmax: 248115.2 ymax: 4907735 projected CRS: NAD83 / UTM zone 15N HUC_CODE county_code year acres geometry 608 10170203 149 2018 226500 POLYGON ((235551.4 4907513,... 608.1 10170203 167 2018 249000 POLYGON ((235551.4 4907513,... 608.2 10170203 193 2018 201000 POLYGON ((235551.4 4907513,... 608.3 10170203 119 2018 184500 POLYGON ((235551.4 4907513,... Figure 3.21 shows the map of the four observations. tm_shape(temp_HUC_county) + tm_polygons() + tm_layout(frame = FALSE) Figure 3.21: Map of the HUC unit So, all of the four observations have identical geometry, which is the geometry of the HUC unit, meaning that the st_join() did not leave the information about the nature of the intersection of the HUC unit and the four counties. Again, remember that the default option is st_intersects(), which checks whether spatial objects intersect or not, nothing more. If you are just calculating the simple average of corn acres ignoring the degree of spatial overlaps, this is just fine. However, if you would like to calculate area-weighted average, you do not have sufficient information. You will see how to find area-weighted average below. Note that we did not extract any attribute values of the railroads in Chapter 1.4. We just calculated the travel length of the railroads, meaning that the geometry of railroads themselves were of interest instead of values associated with the railroads.↩︎ You can see a practical example of this case in action in Demonstration 1 of Chapter 1.↩︎ While it is unlikely you will face the need to change the topological relation, you could do so using the join option.↩︎ You can see a practical example of this case in action in Demonstration 2 of Chapter 1.↩︎ See Chapter 9.1 for how to download Quick Stats data from within R.↩︎ See here for an explanation of what they are. You do not really need to know what HUC units are to understand what’s done in this section.↩︎ Yes, there will be substantial measurement errors as the source polygons (corn acres by county) are large relative to the target polygons (HUC units). But, this serves as a good illustration of a polygon-polygon join.↩︎ "],
["spatial-intersection-cropping-join.html", "3.4 Spatial Intersection (cropping join)", " 3.4 Spatial Intersection (cropping join) Sometimes you face the need to crop spatial objects by polygon boundaries. For example, we found the total length of the railroads inside of each county in Demonstration 4 in Chapter 1.4 by cutting off the parts of the railroads that extend beyond the boundary of counties. Also, we just saw that area-weighted averages cannot be found using st_join() because it does not provide information about how much area of each HUC unit is intersecting with each of its intersecting counties. If we can get the geometry of the intersecting part of the HUC unit and the county, then we can calculate its area, which in turn allows us to find area-weighted averages of joined attributes. For these purposes, we can use sf::st_intersection(). Below, we first illustrate how st_intersection() works for lines-polygons and polygons-polygons intersections (Note that we use the data we generated in Chapter 3.1). Intersections that involve points using st_intersection() is the same as using st_join() because points are length-less and area-less (nothing to cut). Thus, it is not discussed here. 3.4.1 st_intersection() While st_intersects() returns the indices of intersecting objects, st_intersection() returns intersecting spatial objects with the non-intersecting parts of the sf objects cut out. Moreover, attribute values of the source sf will be merged to its intersecting sfg in the target sf. We will see how it works for lines-polygons and polygons-polygons cases using the toy examples we used to explain how st_intersects() work. Here is the figure of the lines and polygons (Figure 3.22): ggplot() + geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) + scale_fill_discrete(name = &quot;Polygons&quot;) + geom_sf(data = lines, aes(color = line_name)) + scale_color_discrete(name = &quot;Lines&quot;) Figure 3.22: Visualization of the points, lines, and polygons lines and polygons The following code gets the intersection of the lines and the polygons. ( intersections_lp &lt;- st_intersection(lines, polygons) %&gt;% mutate(int_name = paste0(line_name, &quot;-&quot;, polygon_name)) ) Simple feature collection with 3 features and 3 fields geometry type: LINESTRING dimension: XY bbox: xmin: 0 ymin: 0 xmax: 2.5 ymax: 2 CRS: NA line_name polygon_name x int_name 1 line 1 polygon 1 LINESTRING (0 0, 2 0.4) line 1-polygon 1 2 line 2 polygon 1 LINESTRING (1.5 0.5, 2 1.25) line 2-polygon 1 3 line 2 polygon 2 LINESTRING (2.166667 1.5, 2... line 2-polygon 2 As you can see in the output, each instance of the intersections of the lines and polygons become an observation (line 1-polygon 1, line 2-polygon 1, and line 2-polygon 2). The part of the lines that did not intersect with any of the polygons is cut out and does not remain in the returned sf. To see this, see Figure 3.23 below: ggplot() + #--- here are all the original polygons ---# geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) + #--- here is what is returned after st_intersection ---# geom_sf(data = intersections_lp, aes(color = int_name), size = 1.5) Figure 3.23: The outcome of the intersections of the lines and polygons This further allows us to calculate the length of the part of the lines that are completely contained in polygons, just like we did in Chapter 1.4. Note also that the attribute (polygon_name) of the source sf (the polygons) are merged to their intersecting lines. Therefore, st_intersection() is transforming the original geometries while joining attributes (this is why I call this cropping join). polygons and polygons The following code gets the intersection of polygon 1 and polygon 3 with polygon 2. ( intersections_pp &lt;- st_intersection(polygons[c(1,3), ], polygons[2, ]) %&gt;% mutate(int_name = paste0(polygon_name, &quot;-&quot;, polygon_name.1)) ) Simple feature collection with 2 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 0.5 ymin: 1.5 xmax: 2.3 ymax: 3.2 CRS: NA polygon_name polygon_name.1 x 1 polygon 1 polygon 2 POLYGON ((0.5 2, 2 2, 2 1.5... 2 polygon 3 polygon 2 POLYGON ((0.5 2.5, 0.5 3.2,... int_name 1 polygon 1-polygon 2 2 polygon 3-polygon 2 As you can see in Figure 3.24, each instance of the intersections of polygons 1 and 3 against polygon 2 becomes an observation (polygon 1-polygon 2 and polygon 3-polygon 2). Just like the lines-polygons case, the non-intersecting part of polygons 1 and 3 are cut out and do not remain in the returned sf. We will see later that st_intersection() can be used to find area-weighted values from the intersecting polygons with help from st_area(). ggplot() + #--- here are all the original polygons ---# geom_sf(data = polygons, aes(fill = polygon_name), alpha = 0.3) + #--- here is what is returned after st_intersection ---# geom_sf(data = intersections_pp, aes(fill = int_name)) Figure 3.24: The outcome of the intersections of polygon 2 and polygons 1 and 3 3.4.2 Area-weighted average Let’s now get back to the example of HUC units and county-level corn acres data we saw in Chapter 3.3. We would like to find area-weighted average of corn acres instead of the simple average of corn acres. Using st_intersection(), for each of the HUC polygons, we find the intersecting counties, and then divide it into parts based on the boundary of the intersecting polygons. ( HUC_intersections &lt;- st_intersection(HUC_IA, IA_corn) %&gt;% mutate(huc_county = paste0(HUC_CODE, &quot;-&quot;, county_code)) ) Simple feature collection with 349 features and 5 fields geometry type: GEOMETRY dimension: XY bbox: xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687 projected CRS: NAD83 / UTM zone 15N First 10 features: HUC_CODE county_code year acres geometry huc_county 1 07080207 083 2018 183500 POLYGON ((482898.9 4711686,... 07080207-083 2 07080205 083 2018 183500 POLYGON ((499779.5 4696819,... 07080205-083 3 07080105 083 2018 183500 POLYGON ((461843.7 4683106,... 07080105-083 4 10170204 141 2018 167000 POLYGON ((269413.3 4793330,... 10170204-141 5 10230003 141 2018 167000 POLYGON ((271582.4 4754543,... 10230003-141 6 10230002 141 2018 167000 POLYGON ((267630.3 4790946,... 10230002-141 7 07100003 081 2018 184500 POLYGON ((436126.2 4789504,... 07100003-081 8 07080203 081 2018 184500 MULTIPOLYGON (((459473.1 47... 07080203-081 9 07080207 081 2018 184500 POLYGON ((429583.2 4779598,... 07080207-081 10 07100005 081 2018 184500 POLYGON ((420999.4 4772213,... 07100005-081 The key difference from the st_join() example is that each observation of the returned data is a unique HUC-county intersection. Figure 3.25 below is a map of all the intersections of the HUC unit with HUC_CODE ==10170203 and the four intersecting counties. tm_shape(filter(HUC_intersections, HUC_CODE == &quot;10170203&quot;)) + tm_polygons(col = &quot;huc_county&quot;) + tm_layout(frame = FALSE) Figure 3.25: Intersections of a HUC unit and Iowa counties Note also that the attributes of county data are joined as you can see acres in the output above. As I said earlier, st_intersection() is a spatial kind of spatial join where the resulting observations are the intersections of the target and source sf objects. In order to find the area-weighted average of corn acres, you can use st_area() first to calculate the area of the intersections, and then find the area-weighted average as follows: ( HUC_aw_acres &lt;- HUC_intersections %&gt;% #--- get area ---# mutate(area = as.numeric(st_area(.))) %&gt;% #--- get area-weight by HUC unit ---# group_by(HUC_CODE) %&gt;% mutate(weight = area / sum(area)) %&gt;% #--- calculate area-weighted corn acreage by HUC unit ---# summarize(aw_acres = sum(weight * acres)) ) Simple feature collection with 55 features and 2 fields geometry type: GEOMETRY dimension: XY bbox: xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687 projected CRS: NAD83 / UTM zone 15N # A tibble: 55 x 3 HUC_CODE aw_acres geometry &lt;chr&gt; &lt;dbl&gt; &lt;GEOMETRY [m]&gt; 1 07020009 251185. POLYGON ((421317.6 4797774, 421160.2 4797630, 421060.3 479… 2 07040008 165000 POLYGON ((602931.1 4817205, 602922.4 4817166, 602862.4 481… 3 07060001 105234. MULTIPOLYGON (((649942.7 4761635, 649613.8 4761791, 649473… 4 07060002 140201. MULTIPOLYGON (((593265.8 4817066, 593390.7 4817045, 593570… 5 07060003 149000 MULTIPOLYGON (((692249.2 4712890, 692081.6 4712936, 692051… 6 07060004 162123. POLYGON ((653200.5 4718418, 652956.5 4718503, 652446.5 471… 7 07060005 142428. POLYGON ((735347.9 4642386, 734770.6 4642296, 734450.5 464… 8 07060006 159635. POLYGON ((721064.4 4656959, 721056 4656900, 721026 4656760… 9 07080101 115572. POLYGON ((667461.7 4558779, 667381 4558691, 667210.9 45585… 10 07080102 160008. POLYGON ((635032.9 4675777, 635236 4675644, 635356.1 46756… # … with 45 more rows "],
["raster-basics.html", "Chapter 4 Raster Data Handling ", " Chapter 4 Raster Data Handling "],
["before-you-start-3.html", "Before you start", " Before you start In this chapter, we will learn how to use the terra package to handle raster data. The raster package has been (and I must say still is) THE package for raster data handling. However, we are in the period of transitioning from the raster package to the terra package. The terra package has been under active development to replace the raster package, and its first beta version71 was just released on CRAN on March 20, 2020. terra is written in C++ and thus is faster than the raster package in many raster data operations. The raster and terra packages share the same function name for most of the raster operations. Therefore, learning the terra package is almost the same as learning the raster package. Key differences will be discussed and will become clear later. For economists, raster data extraction for vector data will be by far the most common use case of raster data and also the most time-consuming part of the whole raster data handling experience. Therefore, we will introduce only the essential knowledge of raster data operation required to effectively implement the task of extracting values, which will be covered extensively in Chapter 5. For example, we do not cover raster arithmetic, focal operations, or aggregation. Those who are interested in a fuller treatment of the terra or raster package are referred to Spatial Data Science with R and “terra” or Chapters 3, 4, and 5 of Geocomputation with R, respectively. We still learn the raster object classes defined by the raster package and how to switch between the raster and terra object classes. This is because other useful packages for us economists were written to work with the raster object classes and have still not been adapted to support terra object classes at the moment. In particular, exactextractr is critical for economists who regularly use large spatially fine raster datasets with many temporal dimensions because of its speed advantage over the terra package. terra::extract() is much faster than raster::extract(), which is unbearably slow for large datasets. Unfortunately, terra::extract() is still much slower than the extraction function provided by exactextractr packages for large datasets72. Since exactextractr works only with objects defined by the raster package, you need to convert a terra object to a raster object if you would like to take advantage of the function. This also means that we need to learn the difference in raster object classes between the two packages. This problem should be resolved in a matter of a year, and most of the spatial packages will add support for terra. Finally, you might benefit from learning the stars package for raster data handling and operations (covered in Chapter 7), particularly if you often work with raster data with the temporal dimension (e.g., PRISM, Daymet). It provides a data model that makes working with raster data with temporal dimensions easier. It also allows you to apply dplyr verbs for data wrangling. Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( terra, # handle raster data raster, # handle raster data cdlTools, # download CDL data mapview, # create interactive maps dplyr, # data wrangling sf # vector data handling ) github page↩︎ CDL data is a good example↩︎ "],
["raster-data-object-classes.html", "4.1 Raster data object classes", " 4.1 Raster data object classes 4.1.1 raster package: RasterLayer, RasterStack, and RasterBrick Let’s start with taking a look at raster data. We will download CDL data for Iowa in 2015. library(cdlTools) #--- download the CDL data for Iowa in 2015 ---# ( IA_cdl_2015 &lt;- getCDL(&quot;Iowa&quot;, 2015)$IA2015 ) class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data/IA_cdl_2015.tif names : IA_cdl_2015 values : 0, 229 (min, max) Evaluating the imported raster object provides you with information about the raster data, such as dimensions (number of cells, number of columns, number of cells), spatial resolution (30 meter by 30 meter for this raster data), extent, CRS and the minimum and maximum values recorded in this raster layer. The class of the downloaded data is RasterLayer, which is a raster data class defined by the raster package.73 A RasterLayer consists of only one layer, meaning that only a single variable is associated with the cells (here it is land use category code in integer). Among these spatial characteristics, you often need to extract the CRS of a raster object before you interact it with vector data74, which can be done using projection(): projection(IA_cdl_2015) [1] &quot;+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; You can stack multiple raster layers of the same spatial resolution and extent to create a RasterStack using raster::stack(). Often times, processing a multi-layer object has computational advantages over processing multiple single-layer one by one75. To create a RasterStack and RasterBrick, let’s download the CDL data for IA in 2016 and stack it with the 2015 data. #--- download the CDL data for Iowa in 2016 ---# IA_cdl_2016 &lt;- getCDL(&quot;Iowa&quot;, 2016)$IA2016 #--- stack the two ---# ( IA_cdl_stack &lt;- stack(IA_cdl_2015, IA_cdl_2016) ) class : RasterStack dimensions : 11671, 17795, 207685445, 2 (nrow, ncol, ncell, nlayers) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs names : IA_cdl_2015, IA_cdl_2016 min values : 0, 0 max values : 229, 241 IA_cdl_stack is of class RasterStack, and it has two layers of variables: CDL for 2015 and 2016. You can make it a RasterBrick using raster::brick(): #--- stack the two ---# IA_cdl_brick &lt;- brick(IA_cdl_stack) #--- or this works as well ---# # IA_cdl_brick &lt;- brick(IA_cdl_2015, IA_cdl_2016) #--- take a look ---# IA_cdl_brick class : RasterBrick dimensions : 11671, 17795, 207685445, 2 (nrow, ncol, ncell, nlayers) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /private/var/folders/t4/5gnqprbn38nftyxkyk5hdwmd8hnypy/T/Rtmp6644mS/raster/r_tmp_2020-07-31_205103_69893_89224.grd names : IA_cdl_2015, IA_cdl_2016 min values : 0, 0 max values : 229, 241 You probably noticed that it took some time to create the RasterBrick object76. While spatial operations on RasterBrick are supposedly faster than RasterStack, the time to create a RasterBrick object itself is often long enough to kill the speed advantage entirely77. Often, the three raster object types are collectively referred to as Raster\\(^*\\) objects for shorthand in the documentation of the raster and other related packages. 4.1.2 terra package: SpatRaster terra package has only one object class for raster data, SpatRaster and no distinctions between one-layer and multi-layer rasters are necessary. Let’s first convert a RasterLayer to a SpatRaster using terra::rast() function. #--- convert to a SpatRaster ---# IA_cdl_2015_sr &lt;- rast(IA_cdl_2015) #--- take a look ---# IA_cdl_2015_sr class : SpatRaster dimensions : 11671, 17795, 1 (nrow, ncol, nlyr) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs data source : IA_cdl_2015.tif names : Layer_1 min values : 0 max values : 229 You can see that the number of layers (nlyr) is \\(1\\) because the original object is a RasterLayer, which by definition has only one layer. Now, let’s convert a RasterStack to a SpatRaster using terra::rast(). #--- convert to a SpatRaster ---# IA_cdl_stack_sr &lt;- rast(IA_cdl_stack) #--- take a look ---# IA_cdl_stack_sr class : SpatRaster dimensions : 11671, 17795, 2 (nrow, ncol, nlyr) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs source(s) : IA_cdl_2015.tif IA_cdl_2016.tif names : Layer_1, Layer_1 min values : 0, 0 max values : 229, 241 Again, it is a SpatRaster, and you now see that the number of layers is 2. We just confirmed that terra has only one class for raster data whether it is single-layer or multiple-layer ones. Instead of projection(), you use crs() to extract the CRS. crs(IA_cdl_2015_sr) [1] &quot;PROJCRS[\\&quot;unnamed\\&quot;,\\n BASEGEOGCRS[\\&quot;GRS 1980(IUGG, 1980)\\&quot;,\\n DATUM[\\&quot;unknown\\&quot;,\\n ELLIPSOID[\\&quot;GRS80\\&quot;,6378137,298.257222101,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]]],\\n PRIMEM[\\&quot;Greenwich\\&quot;,0,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433,\\n ID[\\&quot;EPSG\\&quot;,9122]]]],\\n CONVERSION[\\&quot;Albers Equal Area\\&quot;,\\n METHOD[\\&quot;Albers Equal Area\\&quot;,\\n ID[\\&quot;EPSG\\&quot;,9822]],\\n PARAMETER[\\&quot;Latitude of false origin\\&quot;,23,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8821]],\\n PARAMETER[\\&quot;Longitude of false origin\\&quot;,-96,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8822]],\\n PARAMETER[\\&quot;Latitude of 1st standard parallel\\&quot;,29.5,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8823]],\\n PARAMETER[\\&quot;Latitude of 2nd standard parallel\\&quot;,45.5,\\n ANGLEUNIT[\\&quot;degree\\&quot;,0.0174532925199433],\\n ID[\\&quot;EPSG\\&quot;,8824]],\\n PARAMETER[\\&quot;Easting at false origin\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8826]],\\n PARAMETER[\\&quot;Northing at false origin\\&quot;,0,\\n LENGTHUNIT[\\&quot;metre\\&quot;,1],\\n ID[\\&quot;EPSG\\&quot;,8827]]],\\n CS[Cartesian,2],\\n AXIS[\\&quot;easting\\&quot;,east,\\n ORDER[1],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]],\\n AXIS[\\&quot;northing\\&quot;,north,\\n ORDER[2],\\n LENGTHUNIT[\\&quot;metre\\&quot;,1,\\n ID[\\&quot;EPSG\\&quot;,9001]]]]&quot; 4.1.3 Converting a SpatRaster object to a Raster\\(^*\\) object. You can convert a SpatRaster object to a Raster\\(^*\\) object using raster(), stack(), and brick(). Keep in mind that if you use rater() even though SpatRaster has multiple layers, the resulting RasterLayer object has only the first of the multiple layers. #--- RasterLayer (only 1st layer) ---# IA_cdl_stack_sr %&gt;% raster() class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data/IA_cdl_2015.tif names : Layer_1 values : 0, 229 (min, max) #--- RasterLayer ---# IA_cdl_stack_sr %&gt;% stack() class : RasterStack dimensions : 11671, 17795, 207685445, 2 (nrow, ncol, ncell, nlayers) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs names : IA_cdl_2015, IA_cdl_2016 min values : 0, 0 max values : 229, 241 #--- RasterLayer (this takes some time) ---# IA_cdl_stack_sr %&gt;% brick() class : RasterStack dimensions : 11671, 17795, 207685445, 2 (nrow, ncol, ncell, nlayers) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs names : IA_cdl_2015, IA_cdl_2016 min values : 0, 0 max values : 229, 241 4.1.4 Vector data in the terra package terra package has its own class for vector data, called SpatVector. While we do not use any of the vector data functionality provided by the terra package, we learn how to convert an sf object to SpatVector because terra functions do not support sf as of now (this will likely be resolved very soon). We will see some use cases of this conversion in the next chapter when we learn raster value extractions for vector data using terra::extract(). As an example, let’s use Illinois county border data. library(maps) #--- Illinois county boundary ---# ( IL_county &lt;- st_as_sf(map(&quot;county&quot;, &quot;illinois&quot;, plot = FALSE, fill = TRUE)) ) Simple feature collection with 102 features and 1 field geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.50136 ymin: 37.00161 xmax: -87.49638 ymax: 42.50774 geographic CRS: WGS 84 First 10 features: ID geom 1 illinois,adams MULTIPOLYGON (((-91.49563 4... 2 illinois,alexander MULTIPOLYGON (((-89.21526 3... 3 illinois,bond MULTIPOLYGON (((-89.27828 3... 4 illinois,boone MULTIPOLYGON (((-88.94024 4... 5 illinois,brown MULTIPOLYGON (((-90.91121 3... 6 illinois,bureau MULTIPOLYGON (((-89.63351 4... 7 illinois,calhoun MULTIPOLYGON (((-90.93414 3... 8 illinois,carroll MULTIPOLYGON (((-89.91999 4... 9 illinois,cass MULTIPOLYGON (((-90.51014 3... 10 illinois,champaign MULTIPOLYGON (((-88.46468 4... You cannot convert an sf object directly to SpatVector. You first need to turn an sf into a spatial object class supported by the sp package, and then turn that into a SpatVector object using terra::vect(). IL_county_sv &lt;- as(IL_county, &quot;Spatial&quot;) %&gt;% # to SpatialPolygonsDataFrame #--- to SpatVectgor ---# vect() You just need to put the name of your sf object in place of IL_county. You do not have to understand what SpatialPolygonsDataFrame is if you are not familiar with the sp package. It is just an intermediate object that you do not really need to understand. Indeed, when the next version of the terra packages comes out (The version currently available on CRAN is 0.6-9 at the time of writing), vect() will be able to convert a SpatVector object to an sf object directly without the intermediate step like this (see here): #--- NOT RUN ---# IL_county_sv &lt;- vect(IL_county) To install the development version, visit here and follow the direction. This is what I meant by the raster being THE package for raster data handling. The default object class for many raster-related packages is a raster object class, instead of a terra object class.↩︎ e.g., extracting values from a raster layer to vector data, or cropping a raster layer to the spatial extent of vector data.↩︎ You will see this in Chapter 5 where we learn how to extract values from a raster layer for a vector data.↩︎ Read here for the subtle difference between RasterStack and RasterBrick↩︎ We will see this in Chapter , where we compare the speed of data extraction from RasterStack and RasterBrick objects.↩︎ "],
["read-and-write-a-raster-data-file.html", "4.2 Read and write a raster data file", " 4.2 Read and write a raster data file Sometimes we can download raster data as we saw in Section 3.1. But, most of the time you need to read raster data stored in a file. Raster data files come in numerous formats. For example, PRPISM comes in the Band interleaved by line (BIL) format, some of the Daymet data comes in netCDF format. Other popular formats include GeoTiff, SAGA, ENVI, and many others. 4.2.1 Read raster file(s) You use terra::rast() to read raster data of many common formats, and it should be almost always the case that the raster data you got can be read using this function. Here, we read a GeoTiff file (a file with .tif extension): ( IA_cdl_2015_sr &lt;- rast(&quot;./Data/IA_cdl_2015.tif&quot;) ) class : SpatRaster dimensions : 11671, 17795, 1 (nrow, ncol, nlyr) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs data source : IA_cdl_2015.tif names : Layer_1 min values : 0 max values : 229 One important thing to note here is that the cell values of the raster data are actually not in memory when you “read” raster data from a file. You basically just established a connection to the file. This helps to reduce the memory footprint of raster data handling. You can check this by raster::inMemory() function for Raster\\(^*\\) objects, but the same function has not been implemented for terra yet. You can read multiple single-layer raster datasets of the same spatial extent and resolution at the same time to have a multi-layer SpatRaster object. Here, we import two single-layer raster datasets (IA_cdl_2015.tif and IA_cdl_2016.tif) to create a two-layer SpatRaster object. #--- the list of path to the files ---# files_list &lt;- c(&quot;./Data/IA_cdl_2015.tif&quot;, &quot;./Data/IA_cdl_2016.tif&quot;) #--- read the two at the same time ---# ( multi_layer_sr &lt;- rast(files_list) ) class : SpatRaster dimensions : 11671, 17795, 2 (nrow, ncol, nlyr) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) coord. ref. : +proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +ellps=GRS80 +units=m +no_defs source(s) : IA_cdl_2015.tif IA_cdl_2016.tif names : Layer_1, Layer_1 min values : 0, 0 max values : 229, 241 Of course, this only works because the two datasets have the identical spatial extent and resolution. There are, however, no restrictions on what variable each of the raster layers represent. For example, you can combine PRISM temperature and precipitation raster layers. 4.2.2 Write raster files You can write a SpatRaster object using terra::writeRaster(). terra::writeRaster(IA_cdl_2015_sr, &quot;./Data/IA_cdl_stack.tif&quot;, format = &quot;GTiff&quot;, overwrite = TRUE) The above code saves IA_cdl_2015_sr (a SpatRaster object) as a GeoTiff file.78 The format option can be dropped as writeRaster() infers the format from the extension of the file name. The overwrite = TRUE option is necessary if a file with the same name already exists and you are overwriting it. This is one of the many areas terra is better than raster. raster::writeRaster() can be frustratingly slow for a large Raster\\(^*\\) object. terra::writeRaster() is much faster. You can also save a multi-layer SpatRster object just like you save a single-layer SpatRster object. terra::writeRaster(IA_cdl_stack_sr, &quot;./Data/IA_cdl_stack.tif&quot;, format = &quot;GTiff&quot;, overwrite = TRUE) The saved file is a multi-band raster datasets. So, if you have many raster files of the same spatial extent and resolution, you can “stack” them on R and then export it to a single multi-band raster datasets, which cleans up your data folder. There are many other alternative formats (see here)↩︎ "],
["access-values.html", "4.3 Access values", " 4.3 Access values Sometime, it is nice to be able to see the data values in a raster dataset or visualize the data for various kinds of checks. You can access the values stored in a SpatRaster object using values() function: #--- terra::values ---# values_from_rs &lt;- values(IA_cdl_stack_sr) #--- take a look ---# head(values_from_rs) Layer_1 Layer_1 [1,] 0 0 [2,] 0 0 [3,] 0 0 [4,] 0 0 [5,] 0 0 [6,] 0 0 The returned values come in a matrix form of two columns because we are getting values from a two-layer SpatRaster object (one column for each layer). "],
["quick-plot-and-interactive-map.html", "4.4 Quick plot and interactive map", " 4.4 Quick plot and interactive map To have a quick visualization of the data values of SpatRaster objects, you can simply use plot(): plot(IA_cdl_stack_sr) If you would like to have an interactive view of raster datasets, you can use the mapView() function from the mapview package just like we did for sf objects in Chapter 3. It sill does not support terra objects, so you first need to convert a SpatRaster object into Raster\\(^*\\) objects first and then apply mapView(). mapView(stack(IA_cdl_stack_sr)) By clicking on the white box beneath the \\(+\\) and \\(-\\) signs, you can pick the layers to display (here, CDL for Iowa in 2015 and 2016). Note that, in the map above, the raster values are modified because the number of cells of the raster layers are too large. mapView() reduced the resolution when creating the map, changing the values during the process. You can specify the maximum number of cells using the mapview.maxpixels option. "],
["int-RV.html", "Chapter 5 Spatial Interactions of Vector and Raster Data ", " Chapter 5 Spatial Interactions of Vector and Raster Data "],
["before-you-start-4.html", "Before you start", " Before you start In this chapter we learn the spatial interactions of a vector and raster dataset. We first look at how to crop (spatially subset) a raster dataset based on the geographic extent of a vector dataset. We then cover how to extract values from raster data for points and polygons. To be precise, here is what we mean by raster data extraction and what it does for points and polygons data: Points: For each of the points, find which raster cell it is located within, and assign the value of the cell to the point. Polygons: For each of the polygons, identify all the raster cells that intersect with the polygon, and assign a vector of the cell values to the polygon This is probably the most important operation economists run on raster datasets. We will show how we can use terra::extract() for both cases. But, we will also see that for polygons, exact_extract() from the exactextractr package is often considerably faster than terra::extract(). Finally, you will see conversions between Raster\\(^*\\) (raster package) objects and SpatRaster object (terra package) because of the incompatibility of object classes across the key packages. I believe that these hassles will go away soon when they start supporting each other. Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( terra, # handle raster data raster, # handle raster data exactextractr, # fast extractions sf, # vector data operations dplyr, # data wrangling tidyr, # data wrangling data.table, # data wrangling prism, # download PRISM data tictoc # timing codes ) (Important as of 06//15/2020) You can make exactextractr even faster by installing the most recent versions of the raster (which exactextractr partially depends on) and exactextractr packages available on their respective github pages as follows: library(remotes) install_github(&quot;isciences/exactextractr&quot;) install_github(&quot;rspatial/raster&quot;) This will alleviate the significant overhead associated with many getValuesBlock() calls from exactextractr::exact_extract, and makes it much faster. See some benchmarks here. But, the installation of this version of the raster package causes an error with mapview() function. Therefore, this book is compiled with the CRAN version of the raster package to make all the codes in this book run without errors. This of course means that the relative performance of exact_extract() over other options can be even more impressive than what you will see in this book. "],
["raster-crop.html", "5.1 Cropping (Spatial subsetting) to the Area of Interest", " 5.1 Cropping (Spatial subsetting) to the Area of Interest Here we use PRISM maximum temperature (tmax) data as a raster dataset and Kansas county boundaries as a vector dataset. Let’s download the tmax data for July 1, 2018 (Figure 5.1). #--- set the path to the folder to which you save the downloaded PRISM data ---# # This code sets the current working directory as the designated folder options(prism.path = &quot;./Data&quot;) #--- download PRISM precipitation data ---# get_prism_dailys( type = &quot;tmax&quot;, date = &quot;2018-07-01&quot;, keepZip = FALSE ) #--- the file name of the PRISM data just downloaded ---# prism_file &lt;- &quot;./Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil&quot; #--- read in the prism data ---# prism_tmax_0701 &lt;- raster(prism_file) Figure 5.1: Map of PRISM tmax data on July 1, 2018 We now get Kansas county border data from the tigris package (Figure 5.2) as sf. library(tigris) #--- Kansas boundary (sf) ---# KS_county_sf &lt;- counties(state = &quot;Kansas&quot;, cb = TRUE) %&gt;% #--- sp to sf ---# st_as_sf() %&gt;% #--- transform using the CRS of the PRISM tmax data ---# st_transform(projection(prism_tmax_0701)) Figure 5.2: Kansas county boundaries Sometimes, it is convenient to crop a raster layer to the specific area of interest so that you do not have to carry around unnecessary parts of the raster layer. Moreover, it takes less time to extract values from a raster layer when the size of the raster layer is smaller. You can crop a raster layer by using raster::crop(). It works like this: #--- syntax (NOT RUN) ---# crop(raster object, geographic extent) To find the geographic extent of a vector data, you can use raster::extent(). KS_extent &lt;- raster::extent(KS_county_sf) As you can see, it consists of four points. Four pairs of these values (xmin, ymin), (xmin, ymax), (xmax, ymin), and (xmax, ymax) form a rectangle that encompasses the Kansas state boundary. We will crop the PRISM raster layer to the rectangle: #--- crop the entire PRISM to its KS portion---# prism_tmax_0701_KS_rl &lt;- crop(prism_tmax_0701, KS_extent) The figure below (Figure 5.3) shows the PRISM tmax raster data cropped to the geographic extent of Kansas. Notice that the cropped raster layer extends beyond the outer boundary of Kansas state boundary (it is a bit hard to see, but look at the upper right corner). Figure 5.3: PRISM tmax raster data cropped to the geographic extent of Kansas "],
["extracting-values-from-raster-layers-for-vector-data.html", "5.2 Extracting Values from Raster Layers for Vector Data", " 5.2 Extracting Values from Raster Layers for Vector Data In this section, we will learn how to extract information from raster layers for spatial units represented as vector data (points and polygons). For the illustrations in this section, we use the following datasets: Raster: PRISM tmax data cropped to Kansas state border for 07/01/2018 (obtained in 5.1) and 07/02/2018 (downloaded below) Polygons: Kansas county boundaries (obtained in 5.1) Points: Irrigation wells in Kansas (imported below) PRISM tmax data for 07/02/2018 #--- download PRISM precipitation data ---# get_prism_dailys( type = &quot;tmax&quot;, date = &quot;2018-07-02&quot;, keepZip = FALSE ) #--- the file name of the PRISM data just downloaded ---# prism_file &lt;- &quot;Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil&quot; #--- read in the prism data and crop it to Kansas state border ---# prism_tmax_0702_KS_sr &lt;- rast(prism_file) %&gt;% terra::crop(KS_extent) Irrigation wells in Kansas: #--- read in the KS points data ---# ( KS_wells &lt;- readRDS(&quot;./Data/Chap_5_wells_KS.rds&quot;) ) Simple feature collection with 37647 features and 1 field geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199 geographic CRS: NAD83 First 10 features: well_id geometry 1 1 POINT (-100.4423 37.52046) 2 3 POINT (-100.7118 39.91526) 3 5 POINT (-99.15168 38.48849) 4 7 POINT (-101.8995 38.78077) 5 8 POINT (-100.7122 38.0731) 6 9 POINT (-97.70265 39.04055) 7 11 POINT (-101.7114 39.55035) 8 12 POINT (-95.97031 39.16121) 9 15 POINT (-98.30759 38.26787) 10 17 POINT (-100.2785 37.71539) Here is how the wells are spatially distributed over the PRISM grids and Kansas county borders (Figure 5.4): Figure 5.4: Map of Kansas county borders, irrigation wells, and PRISM tmax 5.2.1 Points You can extract values from raster layers to points using terra::extract(). terra::extract() finds which raster cell each of the points is located within and assigns the value of the cell to the point. One complication that we have to deal with at the moment is the fact that terra does not support sf yet. However, terra::extract() accepts a longitude and latitude matrix. Therefore, the following works:79 #--- syntax (NOT RUN) ---# terra::extract(raster object, st_coordinates(sf object)) Let’s extract tmax values from the PRISM tmax layer (prism_tmax_0701_KS_rl) to the irrigation wells: Since prism_tmax_0701_KS_rl is a RasterLayer, let’s first convert it into a SpatRaster object. prism_tmax_0701_KS_sr &lt;- rast(prism_tmax_0701_KS_rl) #--- extract tmax values ---# tmax_from_prism &lt;- terra::extract(prism_tmax_0701_KS_sr, st_coordinates(KS_wells)) #--- take a look ---# head(tmax_from_prism) [1] 34.241 29.288 32.585 30.104 34.232 35.168 terra::extract() returns the extracted values as a vector when the raster object is single-layer raster data. Since the order of the values are consistent with the order of the observations in the points data, you can simply assign the vector as a new variable of the points data as follows: KS_wells$tmax_07_01 &lt;- tmax_from_prism Extracting values from a multi-layer SpatRaster works the same way. Here, we combine prism_tmax_0701_KS_sr and prism_tmax_0702_KS_sr to create a multi-layer SpatRaster. #--- create a multi-layer SpatRaster ---# prism_tmax_stack &lt;- c(prism_tmax_0701_KS_sr, prism_tmax_0702_KS_sr) #--- extract tmax values ---# tmax_from_prism_stack &lt;- terra::extract(prism_tmax_stack, st_coordinates(KS_wells)) #--- take a look ---# head(tmax_from_prism_stack) ID PRISM_tmax_stable_4kmD2_20180701_bil [1,] 1 34.241 [2,] 2 29.288 [3,] 3 32.585 [4,] 4 30.104 [5,] 5 34.232 [6,] 6 35.168 PRISM_tmax_stable_4kmD2_20180702_bil [1,] 30.544 [2,] 29.569 [3,] 29.866 [4,] 29.819 [5,] 30.481 [6,] 30.640 Instead of a vector, the returned object is a matrix with each of the raster layers forming a column. 5.2.2 Polygons (terra way) Caution: Recently, terra::extract() crashed R sessions on RStudio several times when I tried to extract values from a large raster dataset (1.6 GB) for polygons. I did not see any problem when extracting for points data even if the raster data is very large, For now, I recommend exact_extract() to extract values for polygons, which is detailed in the next section. exact_extract() is faster for a large raster dataset anyway. Remember that the terra packages does not support an sf object yet. So, an sf object of polygons needs to be converted to a SpatVector object before we use any functions from the terra packages.80 #--- Kansas boundary (SpatVector) ---# KS_county_sv &lt;- KS_county_sf %&gt;% #--- convert to a SpatVector object ---# as(., &quot;Spatial&quot;) %&gt;% vect() You can use the same terra::extract() function to extract values from a raster layer for polygons. For each of the polygons, it will identify all the raster cells whose center lies inside the polygon and assign the vector of values of the cells to the polygon (You can change this to the cells that intersect with polygons using the touch = TRUE option). #--- extract values from the raster for each county ---# tmax_by_county &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_county_sv) #--- take a look at the first 2 elements of the list ---# tmax_by_county[1:2] [1] 1 1 terra::extract() returns a list, where its \\(i\\)th element corresponds to the \\(i\\)th row of observation in the polygon data (KS_county_sv). Each of the list elements is also a list, and the list has a vector of extracted values for the corresponding polygon. #--- see the first element of the list ---# tmax_by_county[[1]] [1] 1 #--- check the class ---# tmax_by_county[[1]] %&gt;% class() [1] &quot;numeric&quot; In order to make the results usable, you can process them to get a single data.frame, taking advantage of dplyr::bind_rows() to combine the list of the datasets into one dataset. In doing so, you can use .id option to create a new identifier column that links each row to its original data (data.table users can use rbindlist() with the idcol option). ( tmax_by_county_df &lt;- tmax_by_county %&gt;% #--- apply unlist to the lists to have vectors as the list elements ---# lapply(unlist) %&gt;% #--- convert vectors to data.frames ---# lapply(as_tibble) %&gt;% #--- combine the list of data.frames ---# bind_rows(., .id = &quot;rowid&quot;) %&gt;% #--- rename the value variable ---# rename(tmax = value) ) # A tibble: 25,690 x 2 rowid tmax &lt;chr&gt; &lt;dbl&gt; 1 1 1 2 2 1 3 3 1 4 4 1 5 5 1 6 6 1 7 7 1 8 8 1 9 9 1 10 10 1 # … with 25,680 more rows Note that rowid represents the row number of polygons in KS_county_sv. Now, we can easily summarize the data by polygon (county). For example, the code below finds a simple average of tmax by county. tmax_by_county_df %&gt;% group_by(rowid) %&gt;% summarize(tmax = mean(tmax)) # A tibble: 25,690 x 2 rowid tmax &lt;chr&gt; &lt;dbl&gt; 1 1 1 2 10 1 3 100 1 4 1000 8 5 10000 82 6 10001 82 7 10002 82 8 10003 83 9 10004 83 10 10005 83 # … with 25,680 more rows For data.table users, here is how you can do the same: tmax_by_county %&gt;% #--- apply unlist to the lists to have vectors as the list elements ---# lapply(unlist) %&gt;% #--- convert vectors to data.frames ---# lapply(data.table) %&gt;% #--- combine the list ---# rbindlist(., idcol = &quot;rowid&quot;) %&gt;% #--- rename the value variable ---# setnames(&quot;V1&quot;, &quot;tmax&quot;) %&gt;% #--- find the mean of tmax ---# .[, .(tmax = mean(tmax)), by = rowid] rowid tmax 1: 1 1.000 2: 2 1.000 3: 3 1.000 4: 4 1.000 5: 5 1.000 --- 25686: 25686 32.898 25687: 25687 32.998 25688: 25688 32.912 25689: 25689 32.792 25690: 25690 32.483 Extracting values from a multi-layer raster data works exactly the same way except that data processing after the value extraction is slightly more complicated. #--- extract from a multi-layer raster object ---# tmax_by_county_from_stack &lt;- terra::extract(prism_tmax_stack, KS_county_sv) #--- take a look at the first element ---# tmax_by_county_from_stack[[1]] [1] 1 Just like the single-layer case, \\(i\\)th element of the list corresponds to \\(i\\)th polygon. However, each element of the list has two lists of extracted values because we are extracting from a two-layer raster object. This makes it a bit complicated to process them to have nicely-formatted data. The following code transform the list to a single data.frame: #--- extraction from a multi-layer raster object ---# tmax_long_from_stack &lt;- tmax_by_county_from_stack %&gt;% lapply(., function(x) bind_rows(lapply(x, as_tibble), .id = &quot;layer&quot;)) %&gt;% bind_rows(., .id = &quot;rowid&quot;) #--- take a look ---# head(tmax_long_from_stack) # A tibble: 6 x 3 rowid layer value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 1 1 2 2 1 1 3 3 1 1 4 4 1 1 5 5 1 1 6 6 1 1 Note that this code works for a raster object with any number of layers including the single-layer case we saw above. We can then summarize the extracted data by polygon and raster layer. tmax_long_from_stack %&gt;% group_by(rowid, layer) %&gt;% summarize(tmax = mean(value)) # A tibble: 38,535 x 3 # Groups: rowid [38,535] rowid layer tmax &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 1 1 2 10 1 1 3 100 1 1 4 1000 1 8 5 10000 1 82 6 10001 1 82 7 10002 1 82 8 10003 1 83 9 10004 1 83 10 10005 1 83 # … with 38,525 more rows Here is the data.table way: ( tmax_by_county_layer &lt;- tmax_by_county_from_stack %&gt;% lapply(., function(x) rbindlist(lapply(x, data.table), idcol = &quot;layer&quot;)) %&gt;% rbindlist(., idcol = &quot;rowid&quot;) %&gt;% .[, .(tmax = mean(V1)), by = .(rowid, layer)] ) rowid layer tmax 1: 1 1 1.000 2: 2 1 1.000 3: 3 1 1.000 4: 4 1 1.000 5: 5 1 1.000 --- 38531: 38531 1 30.020 38532: 38532 1 30.030 38533: 38533 1 29.875 38534: 38534 1 29.747 38535: 38535 1 29.427 5.2.3 Polygons (exactextractr way) exact_extract() function from the exactextractr package is a faster alternative than terra::extract() for large raster data as we confirm later (exact_extract() does not work with points data at the moment).81 exact_extract() also provides a coverage fraction value for each of the cell-polygon intersections. However, as mentioned in Chapter 4, it only works with Raster\\(^*\\) objects. So, we first need to convert a SpatRaster object to a Raster\\(^*\\) object. The syntax of exact_extract() is very much similar to terra::extract(). #--- syntax (NOT RUN) ---# exact_extract(raster, sf) So, to get tmax values from the PRISM raster layer for Kansas county polygons, the following does the job: #--- convert to a RasterLayer ---# prism_tmax_0701_KS_rl &lt;- raster(prism_tmax_0701_KS_sr) library(&quot;exactextractr&quot;) #--- extract values from the raster for each county ---# tmax_by_county &lt;- exact_extract(prism_tmax_0701_KS_rl, KS_county_sf) #--- take a look at the first 6 rows of the first two list elements ---# tmax_by_county[1:2] %&gt;% lapply(function(x) head(x)) [[1]] value coverage_fraction 1 33.669 0.05508952 2 33.729 0.40033439 3 33.805 0.40245542 4 33.817 0.40013212 5 33.819 0.39828768 6 33.893 0.40295100 [[2]] value coverage_fraction 1 33.253 0.05477130 2 33.292 0.08201632 3 33.390 0.08075412 4 33.443 0.08251677 5 33.511 0.08167925 6 33.512 0.08369426 exact_extract() returns a list, where its \\(i\\)th element corresponds to the \\(i\\)th row of observation in the polygon data (KS_county_sf). For each element of the list, you see value and coverage_fraction. value is the tmax value of the intersecting raster cells, and coverage_fraction is the fraction of the intersecting area relative to the full raster grid, which can help find coverage-weighted summary of the extracted values. #--- combine ---# tmax_combined &lt;- bind_rows(tmax_by_county, .id = &quot;id&quot;) #--- take a look ---# head(tmax_combined) id value coverage_fraction 1 1 33.669 0.05508952 2 1 33.729 0.40033439 3 1 33.805 0.40245542 4 1 33.817 0.40013212 5 1 33.819 0.39828768 6 1 33.893 0.40295100 We can now summarize the data by id. Here, we calculate coverage-weighted mean of tmax. tmax_by_id &lt;- tmax_combined %&gt;% #--- convert from character to numeric ---# mutate(id = as.numeric(id)) %&gt;% #--- group summary ---# group_by(id) %&gt;% summarise(tmax = sum(value * coverage_fraction) / sum(coverage_fraction)) #--- take a look ---# head(tmax_by_id) # A tibble: 6 x 2 id tmax &lt;dbl&gt; &lt;dbl&gt; 1 1 33.9 2 2 33.9 3 3 32.9 4 4 30.8 5 5 34.0 6 6 34.8 Remember that id values are row numbers in the polygon data (KS_county_sf). So, we can assign the tmax values to KS_county_sf as follows: KS_county_sf$tmax_07_01 &lt;- tmax_by_id$tmax Extracting values from RasterStack works in exactly the same manner as RasterLayer. Do not forget that you need to use stack() instead of raster() to convert a multi-layer SpatRaster to RasterStack. tmax_by_county_stack &lt;- stack(prism_tmax_stack) %&gt;% # convert to RasterStack #--- extract from a stack ---# exact_extract(., KS_county_sf, progress = F) #--- take a look at the first 6 lines of the first element---# tmax_by_county_stack[[1]] %&gt;% head() PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil 1 33.669 30.281 2 33.729 30.311 3 33.805 30.333 4 33.817 30.334 5 33.819 30.340 6 33.893 30.374 coverage_fraction 1 0.05508952 2 0.40033439 3 0.40245542 4 0.40013212 5 0.39828768 6 0.40295100 As you can see above, exact_extract() appends additional columns for additional layers, unlike the results of terra::extract() that creates additional lists for additional layers. This makes the post-extraction processing much simpler. #--- combine them ---# tmax_all_combined &lt;- tmax_by_county_stack %&gt;% bind_rows(.id = &quot;id&quot;) #--- take a look ---# head(tmax_all_combined) id PRISM_tmax_stable_4kmD2_20180701_bil PRISM_tmax_stable_4kmD2_20180702_bil 1 1 33.669 30.281 2 1 33.729 30.311 3 1 33.805 30.333 4 1 33.817 30.334 5 1 33.819 30.340 6 1 33.893 30.374 coverage_fraction 1 0.05508952 2 0.40033439 3 0.40245542 4 0.40013212 5 0.39828768 6 0.40295100 In order to find the coverage-weighted tmax by date, you can first pivot it to a long format using dplyr::pivot_longer(). #--- pivot to a longer format ---# ( tmax_long &lt;- pivot_longer( tmax_all_combined, -c(id, coverage_fraction), names_to = &quot;date&quot;, values_to = &quot;tmax&quot; ) ) # A tibble: 30,298 x 4 id coverage_fraction date tmax &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 0.0551 PRISM_tmax_stable_4kmD2_20180701_bil 33.7 2 1 0.0551 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 3 1 0.400 PRISM_tmax_stable_4kmD2_20180701_bil 33.7 4 1 0.400 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 5 1 0.402 PRISM_tmax_stable_4kmD2_20180701_bil 33.8 6 1 0.402 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 7 1 0.400 PRISM_tmax_stable_4kmD2_20180701_bil 33.8 8 1 0.400 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 9 1 0.398 PRISM_tmax_stable_4kmD2_20180701_bil 33.8 10 1 0.398 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 # … with 30,288 more rows And then find coverage-weighted tmax by date: ( tmax_long %&gt;% group_by(id, date) %&gt;% summarize(tmax = sum(tmax * coverage_fraction) / sum(coverage_fraction)) ) # A tibble: 210 x 3 # Groups: id [105] id date tmax &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 PRISM_tmax_stable_4kmD2_20180701_bil 33.9 2 1 PRISM_tmax_stable_4kmD2_20180702_bil 30.3 3 10 PRISM_tmax_stable_4kmD2_20180701_bil 32.8 4 10 PRISM_tmax_stable_4kmD2_20180702_bil 30.4 5 100 PRISM_tmax_stable_4kmD2_20180701_bil 34.1 6 100 PRISM_tmax_stable_4kmD2_20180702_bil 29.3 7 101 PRISM_tmax_stable_4kmD2_20180701_bil 35.3 8 101 PRISM_tmax_stable_4kmD2_20180702_bil 29.6 9 102 PRISM_tmax_stable_4kmD2_20180701_bil 33.8 10 102 PRISM_tmax_stable_4kmD2_20180702_bil 29.2 # … with 200 more rows For data.table users, this does the same: ( tmax_all_combined %&gt;% data.table() %&gt;% melt(id.var = c(&quot;id&quot;, &quot;coverage_fraction&quot;)) %&gt;% .[, .(tmax = sum(value * coverage_fraction) / sum(coverage_fraction)), by = .(id, variable)] ) id variable tmax 1: 1 PRISM_tmax_stable_4kmD2_20180701_bil 33.86211 2: 2 PRISM_tmax_stable_4kmD2_20180701_bil 33.89363 3: 3 PRISM_tmax_stable_4kmD2_20180701_bil 32.86976 4: 4 PRISM_tmax_stable_4kmD2_20180701_bil 30.75306 5: 5 PRISM_tmax_stable_4kmD2_20180701_bil 34.02482 --- 206: 101 PRISM_tmax_stable_4kmD2_20180702_bil 29.59724 207: 102 PRISM_tmax_stable_4kmD2_20180702_bil 29.17017 208: 103 PRISM_tmax_stable_4kmD2_20180702_bil 29.59134 209: 104 PRISM_tmax_stable_4kmD2_20180702_bil 30.22309 210: 105 PRISM_tmax_stable_4kmD2_20180702_bil 29.85573 I believe this issue will be resolved soon and you can just supply an sf object instead of coordinates.↩︎ See Chapter 4 to learn what SpatVector is and how to convert sf to SpatRaster.↩︎ See here for how it does extraction tasks differently from other major GIS software.↩︎ "],
["extract-speed.html", "5.3 Extraction speed comparison", " 5.3 Extraction speed comparison Here we compare the extraction speed of raster::extract(), terra::extract(), and exact_extract(). 5.3.1 Points: terra::extract() and raster::extract() exact_extract() uses C++ as the backend. Therefore, it is considerably faster than raster::extract(). #--- terra ---# tic() temp &lt;- terra::extract(prism_tmax_0701_KS_sr, st_coordinates(KS_wells)) toc() 0.008 sec elapsed #--- raster ---# tic() temp &lt;- raster::extract(raster(prism_tmax_0701_KS_sr), KS_wells) toc() 0.317 sec elapsed As you can see, terra::extract() is much faster. The time differential between the two packages can be substantial as the raster data becomes larger. 5.3.2 Polygons: exact_extract(), terra::extract(), and raster::extract() terra::extract() is faster than exact_extract() for a relatively small raster data. Let’s time them and see the difference. library(tictoc) #--- terra extract ---# tic() terra_extract_temp &lt;- terra::extract(prism_tmax_0701_KS_sr, KS_county_sv, progress = FALSE) toc() 0.479 sec elapsed #--- exact extract ---# tic() exact_extract_temp &lt;- exact_extract(prism_tmax_0701_KS_rl, KS_county_sf, progress = FALSE) toc() 0.06 sec elapsed #--- raster::extract ---# tic() raster_extract_temp &lt;- raster::extract(prism_tmax_0701_KS_rl, KS_county_sf) toc() 2.368 sec elapsed As you can see, raster::extract() is by far the slowest. terra::extract() is faster than exact_extract(). However, once the raster data becomes larger (or spatially finer), then exact_extact() starts to shine. Let’s disaggregate the prism data by a factor of 10 to create a much larger raster data.82 #--- disaggregate ---# ( prism_tmax_0701_KS_sr_10 &lt;- terra::disaggregate(prism_tmax_0701_KS_sr, fact = 10) ) class : SpatRaster dimensions : 730, 1790, 1 (nrow, ncol, nlyr) resolution : 0.004166667, 0.004166667 (x, y) extent : -102.0625, -94.60417, 36.97917, 40.02083 (xmin, xmax, ymin, ymax) coord. ref. : +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs data source : memory names : PRISM_tmax_stable_4kmD2_20180701_bil min values : 27.711 max values : 37.656 #--- convert the disaggregated PRISM data to RasterLayer ---# prism_tmax_0701_KS_rl_10 &lt;- raster(prism_tmax_0701_KS_sr_10) The disaggregated PRISM data now has 10 times more rows and columns (see below). #--- original ---# dim(prism_tmax_0701_KS_sr) [1] 73 179 1 #--- disaggregated ---# dim(prism_tmax_0701_KS_sr_10) [1] 730 1790 1 Now, let’s compare terra::extrct() and exact_extrct() using the disaggregated data. #--- terra extract ---# tic() terra_extract_temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) toc() 4.061 sec elapsed #--- exact extract ---# tic() exact_extract_temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) toc() 0.301 sec elapsed As you can see, exact_extract() is considerably faster. The difference in time becomes even more pronounced as the size of the raster data becomes larger and the number of polygons are greater. The time difference of several seconds seem nothing, but imagine processing PRISM files for the entire US over 20 years, then you would appreciate the speed of exact_extract(). 5.3.3 Single-layer vs multi-layer Pretend that you have five dates of PRISM tmax data (here we repeat the same file five times) and would like to extract values from all of them. Extracting values from a multi-layer raster objects (RasterStack for raster package) takes less time than extracting values from the individual layers one at a time. This can be observed below. terra::extract() #--- extract from 5 layers one at a time ---# tic() temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) temp &lt;- terra::extract(prism_tmax_0701_KS_sr_10, KS_county_sv) toc() 20.879 sec elapsed #--- extract from a 5-layer stack ---# prism_tmax_ml_5 &lt;- c( prism_tmax_0701_KS_sr_10, prism_tmax_0701_KS_sr_10, prism_tmax_0701_KS_sr_10, prism_tmax_0701_KS_sr_10, prism_tmax_0701_KS_sr_10 ) tic() temp &lt;- terra::extract(prism_tmax_ml_5, KS_county_sv) toc() 4.932 sec elapsed exact_extract() #--- extract from 5 layers one at a time ---# tic() temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) temp &lt;- exact_extract(prism_tmax_0701_KS_rl_10, KS_county_sf, progress = FALSE) toc() 1.557 sec elapsed #--- extract from from a 5-layer stack ---# prism_tmax_stack_5 &lt;- stack( prism_tmax_0701_KS_rl_10, prism_tmax_0701_KS_rl_10, prism_tmax_0701_KS_rl_10, prism_tmax_0701_KS_rl_10, prism_tmax_0701_KS_rl_10 ) tic() temp &lt;- exact_extract(prism_tmax_stack_5, KS_county_sf, progress = FALSE) toc() 0.581 sec elapsed The reduction in computation time for both methods makes sense. Since both layers have exactly the same geographic extent and resolution, finding the polygons-cells correspondence is done once and then it can be used repeatedly across the layers for the multi-layer SparRaster and RasterStack. This clearly suggests that when you are processing many layers of the same spatial resolution and extent, you should first stack them and then extract values at the same time instead of processing them one by one as long as your memory allows you to do so. We did not introduce this function as it is very rare that you need this function in research projects.↩︎ "],
["EE.html", "Chapter 6 Speed Things Up ", " Chapter 6 Speed Things Up "],
["before-you-start-5.html", "Before you start", " Before you start In this chapter, we learn how to parallelize raster data extraction for polygons data. We do not cover parallelization of raster data extraction for points data because it is very fast. Thus, the repeated raster data extractions for points is unlikely to be a bottleneck in your work. We first start with parallelizing data extraction from a single-layer raster data. We then move on to a multi-layer raster data case. There are different ways of parallelizing the same extraction process. We will discuss several parallelization approaches in terms of their speed and memory footprint. You will learn how to parallelize matters. A naive parallelization can actually increase the time of raster data extraction, while a clever parallelization approach can save you hours or even days (depending on the size of the extraction job, of course). We will use the future.apply and parallel packages for parallelization. Basic knowledge of parallelization using these packages is assumed. Those who are not familiar with parallelized looping using lapply() and parallelization using mclapply() (Mac and Linux users only) or future_lapply() (including Windows), see Chapter A first. Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Warning: the folder includes a series of daily PRISM datasets stored by month for 10 years. They amount to \\(12.75\\) GB of data. Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( parallel, # for parallelization future.apply, # for parallelization terra, # handle raster data raster, # handle raster data exactextractr, # fast extractions sf, # vector data operations dplyr, # data wrangling data.table, # data wrangling prism # download PRISM data ) "],
["single-raster-layer.html", "6.1 Single raster layer", " 6.1 Single raster layer Let’s prepare for parallel processing for the rest of the section. library(parallel) #--- get the number of logical cores to use ---# ( num_cores &lt;- detectCores() - 1 ) [1] 15 6.1.1 Datasets We will use the following datasets: raster: Iowa Cropland Data Layer (CDL) data in 2015 polygons: Regular polygon grids over Iowa Iowa CDL data in 2015 #--- Iowa CDL in 2015 ---# ( IA_cdl_15 &lt;- raster(&quot;./Data/IA_cdl_2015.tif&quot;) ) class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data/IA_cdl_2015.tif names : IA_cdl_2015 values : 0, 229 (min, max) Values recorded in the raster data are integers representing land use type. Regularly-sized grids over Iowa #--- regular grids over Iowa ---# ( IA_grids &lt;- st_as_sf(map(&quot;state&quot;, &quot;iowa&quot;, plot = FALSE, fill = TRUE)) %&gt;% #--- create regularly-sized grids ---# st_make_grid(n = c(50, 50)) %&gt;% #--- project to the CRS of the CDL data ---# st_transform(projection(IA_cdl_15)) %&gt;% #--- convert to sf from sfc ---# st_as_sf() ) Simple feature collection with 2035 features and 0 fields geometry type: POLYGON dimension: XY bbox: xmin: -49480.27 ymin: 1937867 xmax: 483649.6 ymax: 2289564 CRS: +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs First 10 features: x 1 POLYGON ((373548.7 1937867,... 2 POLYGON ((384427.1 1938383,... 3 POLYGON ((362351.4 1944380,... 4 POLYGON ((373220.9 1944881,... 5 POLYGON ((384089.6 1945396,... 6 POLYGON ((362033.1 1951394,... 7 POLYGON ((372893 1951894, 3... 8 POLYGON ((383752.2 1952409,... 9 POLYGON ((14258.36 1950217,... 10 POLYGON ((25120.22 1950244,... Here is how they look (Figure 6.1): tm_shape(IA_cdl_15) + tm_raster(title = &quot;Land Use &quot;) + tm_shape(IA_grids) + tm_polygons(alpha = 0) + tm_layout(legend.outside = TRUE) Figure 6.1: Regularly-sized grids and land use type in Iowa in 2105 6.1.2 Parallelization Here is how long it takes to extract raster data values for the polygon grids using exact_extract(). tic() temp &lt;- exact_extract(IA_cdl_15, IA_grids) toc() elapsed 47.313 One way to parallelize this process is to let each core work on one polygon at a time. Let’s first define the function to extract values for one polygon and then run it for all the polygons parallelized. #--- function to extract raster values for a single polygon ---# get_values_i &lt;- function(i){ temp &lt;- exact_extract(IA_cdl_15, IA_grids[i, ]) return(temp) } #--- parallelized ---# tic() temp &lt;- mclapply(1:nrow(IA_grids), get_values_i, mc.cores = num_cores) toc() elapsed 89.721 As you can see, this is a terrible way to parallelize the computation process. To see why, let’s look at the computation time of extracting from one polygon, two polygons, and up to five polygons. library(microbenchmark) mb &lt;- microbenchmark( &quot;p_1&quot; = { temp &lt;- exact_extract(IA_cdl_15, IA_grids[1, ]) }, &quot;p_2&quot; = { temp &lt;- exact_extract(IA_cdl_15, IA_grids[1:2, ]) }, &quot;p_3&quot; = { temp &lt;- exact_extract(IA_cdl_15, IA_grids[1:3, ]) }, &quot;p_4&quot; = { temp &lt;- exact_extract(IA_cdl_15, IA_grids[1:4, ]) }, &quot;p_5&quot; = { temp &lt;- exact_extract(IA_cdl_15, IA_grids[1:5, ]) }, times = 100 ) mb %&gt;% data.table() %&gt;% .[, expr := gsub(&quot;p_&quot;, &quot;&quot;, expr)] %&gt;% ggplot(.) + geom_boxplot(aes(y = time/1e9, x = expr)) + ylim(0, NA) + ylab(&quot;seconds&quot;) + xlab(&quot;number of polygons to process&quot;) Figure 6.2: Comparison of the computation time of raster data extractions As you can see in Figure 6.2, there is a significant overhead (about 0.23 seconds) irrespective of the number of the polygons to extract data for. Once the process is initiated and ready to start extracting values for polygons, it does not spend much time processing for additional units of polygon. So, this is a typical example of how you should NOT parallelize. Since each core processes about \\(136\\) polygons, a very simple math suggests that you would spend at least 31.28 (0.23 \\(\\times\\) 136) seconds just for preparing extraction jobs. We can minimize this overhead as much as possible by having each core use exact_extract() only once in which multiple polygons are processed in the single call. Specifically, we will split the collection of the polygons into 15 groups and have each core extract for one group. #--- number of polygons in a group ---# num_in_group &lt;- floor(nrow(IA_grids)/num_cores) #--- assign group id to polygons ---# IA_grids &lt;- IA_grids %&gt;% mutate( #--- create grid id ---# grid_id = 1:nrow(.), #--- assign group id ---# group_id = grid_id %/% num_in_group + 1 ) tic() #--- parallelized processing by group ---# temp &lt;- mclapply( 1:num_cores, function(i) exact_extract(IA_cdl_15, filter(IA_grids, group_id == i)), mc.cores = num_cores ) toc() elapsed 11.901 Great, this is much better.83 Now, we can further reduce the processing time by reducing the size of the object that is returned from each core to be collated into one. In the code above, each core returns a list of data.frames where each grid of the same group has multiple values from the intersecting raster cells. #--- take a look at the the values extracted for the 1st polygon of the 1st group---# head(temp[[1]][[1]]) [1] 176 176 141 141 141 141 #--- the size of the list of data returned by the first core ---# object.size(temp[[1]]) %&gt;% format(units = &quot;GB&quot;) [1] &quot;0 Gb&quot; In total, about 3GB of data has to be collated into one list from 15 cores. It turns out, this process is costly. To see this, take a look at the following example where the same exact_extrct() processes are run, yet nothing is returned by each core. #--- define the function to extract values by block of polygons ---# extract_by_group &lt;- function(i){ temp &lt;- exact_extract(IA_cdl_15, filter(IA_grids, group_id == i)) #--- returns nothing! ---# return(NULL) } #--- parallelized processing by group ---# tic() temp &lt;- mclapply( 1:num_cores, function(i) extract_by_group(i), mc.cores = num_cores ) toc() elapsed 6.689 Approximately 5.212 seconds were used just to collect the 3GB worth of data from the cores into one. In most cases, we do not have to carry around all the individual cell values of landuse types for our subsequent analysis. For example, in Demonstration 3 (Chapter 1.3) we just need a summary (count) of each unique landuse type by polygon. So, let’s get the summary before we have the computer collect the objects returned from each core as follows: extract_by_group_reduced &lt;- function(i){ temp_return &lt;- exact_extract( IA_cdl_15, filter(IA_grids, group_id == i) ) %&gt;% #--- combine the list of data.frames into one with polygon id ---# rbindlist(idcol = &quot;id_within_group&quot;) %&gt;% #--- find the count of land use type values by polygon ---# .[, .(num_value = .N), by = .(value, id_within_group)] return(temp_return) } tic() #--- parallelized processing by group ---# temp &lt;- mclapply( 1:num_cores, function(i) extract_by_group_reduced(i), mc.cores = num_cores ) toc() elapsed 8.514 It is of course slower than the one that returns nothing, but it is much faster than the one that does not reduce the size before the outcome collation. As you can see, the computation time of the fastest approach is now much less, but you still only gained 81.21. How much time did I spend writing the code to do the parallelized group processing? Three minutes. Obviously, what matters to you is the total time (coding time plus processing time) you spend to get the desired outcome. Indeed, the time you could save by a clever coding at the most is 89.72 seconds. Writing any kind of code in an attempt to make your code faster takes more time than that. So, don’t even try to make your code faster if the processing time is quite short in the first place. Before you start parallelizing things, go through what you need to go through in terms of coding in your head, and judge if it’s worth it or not. Imagine processing CDL data for all the states from 2009 to 2020. Then, the whole process will take roughly 14.95 (\\(50 \\times 12 \\times 89.721/60/60\\)) hours. Again, a super rough calculation tells us that the whole process would be done in 1.42 hours if parallelized in the same way as the best approach we saw above. Actually, 14.95 is still not terrible. You execute the program before you go to bed, and when you start working on the next day, all the data is there for you. 6.1.3 Summary Do not let each core runs small tasks over and over again (extracting raster values for one polygon at a time), or you will suffer from significant overhead. Blocking is one way to avoid the problem above. Reduce the size of the outcome of each core as much as possible to spend less time to simply collating them into one. Do not forget about the time you would spend on coding parallelized processes. To get the total time, I should include the codes to generate group id. But, they are so quick that I did not time them.↩︎ "],
["many-multi-layer.html", "6.2 Many multi-layer raster files", " 6.2 Many multi-layer raster files Here we discuss ways to parallelize the process of extracting values from many of multi-layer raster files. 6.2.1 Datasets We will use the following datasets: raster: daily PRISM data 2010 through 2019 stacked by month polygons: Regular polygon grids over Iowa daily PRISM precipitation 2010 through 2019 You can download all the prism files from here. For those who are interested in learning how to generate the series of daily PRISM data files stored by month, see section 9.3 for the code. US counties #--- regular grids over Iowa ---# ( US_county &lt;- st_as_sf(map(database = &quot;county&quot;, plot = FALSE, fill = TRUE)) %&gt;% #--- get state name from ID ---# mutate(state = str_split(ID, &quot;,&quot;) %&gt;% lapply(., `[[`, 1) %&gt;% unlist) %&gt;% #--- project to the CRS of the CDL data ---# st_transform(projection(brick(&quot;./Data/PRISM/PRISM_ppt_y2017_m7.tif&quot;))) ) Simple feature collection with 3076 features and 2 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -124.6813 ymin: 25.12993 xmax: -67.00742 ymax: 49.38323 CRS: +proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0 First 10 features: ID geom state 1 alabama,autauga MULTIPOLYGON (((-86.50517 3... alabama 2 alabama,baldwin MULTIPOLYGON (((-87.93757 3... alabama 3 alabama,barbour MULTIPOLYGON (((-85.42801 3... alabama 4 alabama,bibb MULTIPOLYGON (((-87.02083 3... alabama 5 alabama,blount MULTIPOLYGON (((-86.9578 33... alabama 6 alabama,bullock MULTIPOLYGON (((-85.66866 3... alabama 7 alabama,butler MULTIPOLYGON (((-86.8604 31... alabama 8 alabama,calhoun MULTIPOLYGON (((-85.74313 3... alabama 9 alabama,chambers MULTIPOLYGON (((-85.59416 3... alabama 10 alabama,cherokee MULTIPOLYGON (((-85.46812 3... alabama 6.2.2 Non-parallelized extraction We have already learned in Chapter 5.3 that extracting values from stacked raster layers is faster than doing so from multiple single-layer raster datasets one at a time. Here, daily precipitation datasets are stacked by year-month and saved as multi-layer GeoTIFF files. For example, PRISM_ppt_y2009_m1.tif stores the daily precipitation data for January, 2009. This is how long it takes to extract values for US counties from a month’s of daily PRISM precipitation data. tic() temp &lt;- exact_extract(stack(&quot;./Data/PRISM/PRISM_ppt_y2009_m1.tif&quot;), US_county, progress = F) toc() elapsed 26.571 Now, to process all the precipitation data from 2009-2018, we consider two approaches in this section are: parallelize over polygons and do regular loop over year-month parallelize over year-month 6.2.3 Approach 1: parallelize over polygons and do regular loop over year-month For this approach, let’s measure the time spent on processing one year-month PRISM dataset and then guess how long it would take to process 120 year-month PRISM datasets. #--- number of polygons in a group ---# num_in_group &lt;- floor(nrow(US_county)/num_cores) #--- define group id ---# US_county &lt;- US_county %&gt;% mutate( #--- create grid id ---# poly_id = 1:nrow(.), #--- assign group id ---# group_id = poly_id %/% num_in_group + 1 ) extract_by_group &lt;- function(i){ temp_return &lt;- exact_extract( stack(&quot;./Data/PRISM/PRISM_ppt_y2009_m1.tif&quot;), filter(US_county, group_id == i) ) %&gt;% #--- combine the list of data.frames into one with polygon id ---# rbindlist(idcol = &quot;id_within_group&quot;) %&gt;% #--- find the count of land use type values by polygon ---# melt(id.var = c(&quot;id_within_group&quot;, &quot;coverage_fraction&quot;)) %&gt;% .[, sum(value * coverage_fraction)/sum(coverage_fraction), by = .(id_within_group, variable)] return(temp_return) } tic() temp &lt;- mclapply(1:num_cores, extract_by_group, mc.cores = num_cores) toc() elapsed 83.694 Okay, so this approach does not really help. If we are to process 10 years of daily PRISM data, then it would take roughly 167.39 minutes. 6.2.4 Approach 2: parallelize over the temporal dimension (year-month) Instead of parallelize over polygons, let’s parallelize over time (year-month). To do so, we first create a data.frame that has all the year-month combinations we will work on. ( month_year_data &lt;- expand.grid(month = 1:12, year = 2009:2018) %&gt;% data.table() ) The following function extract data from a single year-month case: get_prism_by_month &lt;- function(i, vector){ temp_month &lt;- month_year_data[i, month] # month to work on temp_year &lt;- month_year_data[i, year] # year to work on #--- import raster data ---# temp_raster &lt;- stack(paste0(&quot;./Data/PRISM/PRISM_ppt_y&quot;, temp_year, &quot;_m&quot;, temp_month, &quot;.tif&quot;)) temp &lt;- exact_extract(temp_raster, vector) %&gt;% #--- combine the extraction results into one data.frame ---# rbindlist(idcol = &quot;row_id&quot;) %&gt;% #--- wide to long ---# melt(id.var = c(&quot;row_id&quot;, &quot;coverage_fraction&quot;)) %&gt;% #--- find coverage-weighted average ---# .[, sum(value*coverage_fraction)/sum(coverage_fraction), by = .(row_id, variable)] return(temp) gc() } We then loop over the rows of month_year_data in parallel. tic() temp &lt;- mclapply(1:nrow(month_year_data), function(x) get_prism_by_month(x, US_county), mc.cores = num_cores) toc() elapsed 451.477 It took 7.52 minutes. So, Approach 2 is the clear winner. 6.2.5 Memory consideration So far, we have paid no attention to the memory footprint of the parallelized processes. But, it is crucial when parallelizing many large datasets. Approaches 1 and 2 differ substantially in their memory footprints. Approach 1 divides the polygons into a group of polygons and parallelizes over the groups when extracting raster values. Approach 2 extracts and holds raster values for 15 of the whole U.S. polygons. So, Approach 1 clearly has a lesser memory footprint. Approach 2 used about 40 Gb of the computer’s memory, almost maxing out the 64 Gb RAM memory of my computer (it’s not just R or C++ that are consuming RAM memory at the time). If you do not go over the limit, it is perfectly fine. Approach 2 is definitely a better option for me. However, if I had 32 Gb RAM memory, Approach 2 would have suffered a significant loss in its performance, while Approach 1 would not have. Or, if the raster data had twice as many cells with the same spatial extent, then Approach 2 would have suffered a significant loss in its performance, while Approach 1 would not have. It is easy to come up with a case where Approach 1 is preferable. For example, suppose you have multiple 10-Gb raster layers and your computer has 16 Gb RAM memory. Then, Approach 2 clearly does not work, and Approach 1 is your only choice, which is better than not parallelizing at all. In summary, while letting each core process a larger amount of data, you need to be careful not to exceed the RAM memory limit of your computer. "],
["stars-basics.html", "Chapter 7 Spatiotemporal Raster Data Handling with stars ", " Chapter 7 Spatiotemporal Raster Data Handling with stars "],
["before-you-start-6.html", "Before you start", " Before you start In this Chapter, we introduce the stars package (Pebesma 2020) for raster data handling. It can be particularly useful for those who use spatiotemporal raster data often (like daily PRISM and Daymet data) because it brings a framework that provides a consistent treatment of raster data with temporal dimensions. Specifically, stars objects can have a time dimension in addition to the spatial 2D dimensions (longitude and latitude), where the time dimension can take Date values.84 This can be handy for several reasons as you will see below (e.g., filtering the data by date). Another advantage of the stars package is its compatibility with sf objects as the lead developer of the two packages is the same person. Therefore, unlike the terra package approach, we do not need any tedious conversions between sf and SpatVector. The stars package also allows dplyr-like data operations using functions like filter(), mutate() (see section 7.6). The stars package has its own set of functions to extract values from raster data for vector data (see section 7.10). They are not as fast as terra::extract() for points data or exact_extract() for polygons data (but, exact_extract() will be incorporated very soon). So, if the speed is an issue, you may want to convert stars object to SpatRaster or Raster\\(^*\\) objects so you can use terra::extract() for points and exact_extract() for polygons data (this will be discussed in section 7.10). In Chapters 4 and 5, we used the raster and terra packages to handle raster data and interact raster data with vector data. If you do not feel any inconvenience with the approach, you do not need to read on. Also, note that the stars package was not written to replace either raster or terra packages. Here is a good summary of how raster functions map to stars functions. As you can see, there are many functions that are available to the raster packages that cannot be implemented by the stars package. However, I must say the functionality of the stars package is rather complete at least for most economists, and it is definitely possible to use just the stars package for all the raster data work in most cases.85 Finally, this book does not cover the use of stars_proxy for big data that does not fit in your memory, which may be useful for some of you. This provides an introduction to stars_proxy for those interested. This book also does not cover irregular raster cells (e.g., curvelinear grids). Interested readers are referred to here. Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( stars, # spatiotemporal data handling sf, # vector data handling tidyverse, # data wrangling cubelyr, # handle raster data tmap, # make maps mapview, # make maps exactextractr, # fast raster data extraction lubridate, # handle dates prism # download PRISM data ) Run the following code to define the theme for map: theme_set(theme_bw()) theme_for_map &lt;- theme( axis.ticks = element_blank(), axis.text= element_blank(), axis.line = element_blank(), panel.border = element_blank(), panel.grid.major = element_line(color=&#39;transparent&#39;), panel.grid.minor = element_line(color=&#39;transparent&#39;), panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,color=&#39;transparent&#39;) ) References "],
["stars-structure.html", "7.1 Understanding the structure of a stars object", " 7.1 Understanding the structure of a stars object Let’s import a stars object of daily PRISM precipitation and tmax saved as an R dataset. #--- read PRISM prcp and tmax data ---# ( prcp_tmax_PRISM_m8_y09 &lt;- readRDS(&quot;./Data/prcp_tmax_PRISM_m8_y09_small.rds&quot;) ) stars object with 3 dimensions and 2 attributes attribute(s): ppt tmax Min. : 0.000 Min. : 1.833 1st Qu.: 0.000 1st Qu.:17.556 Median : 0.000 Median :21.483 Mean : 1.292 Mean :22.035 3rd Qu.: 0.011 3rd Qu.:26.543 Max. :30.851 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL This stars object has two attributes: ppt (precipitation) and tmax (maximum temperature). They are like variables in a regular data.frame. They hold information we are interested in using for our analysis. This stars object has three dimensions: x (longitude), y (latitude), and date. Each dimension has from, to, offset, delta, refsys, point, and values. Here are their definitions (except point86): from: beginning index to: ending index offset: starting value delta: step value refsys: GCS or CRS for x and y (and can be Date for a date dimension) values: values of the dimension when objects are not regular In order to understand the dimensions of stars objects, let’s first take a look at the figure below (Figure 7.1), which visualizes the tmax values on the 2D x-y surfaces of the stars objects at 10th date value (the spatial distribution of tmax at a particular date). Figure 7.1: Map of tmax values on August 10, 2009: 20 by 20 matrix of cells You can consider the 2D x-y surface as a matrix, where the location of a cell is defined by the row number and column number. Since the from for x is 1 and to for x is 20, we have 20 columns. Similarly, since the from for y is 1 and to for y is 20, we have 20 rows. The offset value of the x and y dimensions is the longitude and latitude of the upper-left corner point of the upper left cell of the 2D x-y surface, respectively (the red circle in Figure 7.1).87 As refsys indicates, they are in NAD83 GCS. The longitude of the upper-left corner point of all the cells in the \\(j\\)th column (from the left) of the 2D x-y surface is -121.7291667 + \\((j-1)\\times\\) 0.0416667, where -121.7291667 is offset for x and 0.0416667 is delta for x. Similarly, the latitude of the upper-left corner point of all the cells in the \\(i\\)th row (from the top) of the 2D x-y surface is 46.6458333 +\\((i-1)\\times\\) -0.0416667, where 46.6458333 is offset for y and -0.0416667 is delta for y. The dimension characteristics of x and y are shared by all the layers across the date dimension, and a particular combination of x and y indexes refers to exactly the same location on the earth in all the layers across dates (of course). In the date dimension, we have 10 date values since the from for date is 1 and to for date is 10. The refsys of the date dimension is Date. Since the offset is 2009-08-11 and delta is 1, \\(k\\)th layer represents tmax values for August \\(11+k-1\\), 2009. Putting all this information together, we have 20 by 20 x-y surfaces stacked over the date dimension (10 layers), thus making up a 20 by 20 by 10 three-dimensional array (or cube) as shown in the figure below (Figure 7.2). Figure 7.2: Visual illustration of stars data structure Remember that prcp_tmax_PRISM_m8_y09 also has another attribute (ppt) which is structured exactly the same way as tmax. So, prcp_tmax_PRISM_m8_y09 basically has four dimensions: attribute, x, y, and date. It is not guaranteed that all the dimensions are regularly spaced or timed. For an irregular dimension, dimension values themselves are stored in values instead of using indexes, offset, and delta to find dimension values. For example, if you observe satellite data with 6-day gaps sometimes and 7-day gaps other times, then the date dimension would be irregular. We will see a made-up example of irregular time dimension in Chapter 7.5. It is not clear what it really means. I have never had to pay attention to this parameter. So, its definition is not explained here. If you insist on learning what it is the best resource is probably this↩︎ This changes depending on the delta of x and y. If both of the delta for x and y are positive, then the offset value of the x and y dimensions are the longitude and latitude of the upper-left corner point of the lower-left cell of the 2D x-y surface.↩︎ "],
["some-basic-operations-on-stars-objects.html", "7.2 Some basic operations on stars objects", " 7.2 Some basic operations on stars objects 7.2.1 Subset a stars object by index In order to access the value of an attribute (say ppt) at a particular location at a particular time from the stars object (prcp_tmax_PRISM_m8_y09), you need to tell R that you are interested in the ppt attribute and specify the corresponding index of x, y, and date. Here, is how you get the ppt value of (3, 4) cell at date = 10. prcp_tmax_PRISM_m8_y09[&quot;ppt&quot;, 3, 4, 10] stars object with 3 dimensions and 1 attribute attribute(s): ppt Min. :0 1st Qu.:0 Median :0 Mean :0 3rd Qu.:0 Max. :0 dimension(s): from to offset delta refsys point values x 3 3 -121.729 0.0416667 NAD83 FALSE NULL [x] y 4 4 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 10 10 2009-08-11 1 days Date NA NULL This is very much similar to accessing a value from a matrix except that we have more dimensions. The first argument selects the attribute of interest, 2nd x, 3rd y, and 4th date. Of course, you can subset a stars object to access the value of multiple cells like this: prcp_tmax_PRISM_m8_y09[&quot;ppt&quot;, 3:6, 3:4, 5:10] stars object with 3 dimensions and 1 attribute attribute(s): ppt Min. :0.00000 1st Qu.:0.00000 Median :0.00000 Mean :0.04312 3rd Qu.:0.00000 Max. :0.54600 dimension(s): from to offset delta refsys point values x 3 6 -121.729 0.0416667 NAD83 FALSE NULL [x] y 3 4 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 5 10 2009-08-11 1 days Date NA NULL 7.2.2 Set attribute names When you read a raster dataset from a GeoTIFF file, the attribute name is the file name by default. So, you will often encounter cases where you want to change the attribute name. You can set the name of attributes using setNames(). prcp_tmax_PRISM_m8_y09_dif_names &lt;- setNames(prcp_tmax_PRISM_m8_y09, c(&quot;precipitation&quot;, &quot;maximum_temp&quot;)) Note that the attribute names of prcp_tmax_PRISM_m8_y09 have not changed after this operation (just like mutate() or other dplyr functions do): prcp_tmax_PRISM_m8_y09 stars object with 3 dimensions and 2 attributes attribute(s): ppt tmax Min. : 0.000 Min. : 1.833 1st Qu.: 0.000 1st Qu.:17.556 Median : 0.000 Median :21.483 Mean : 1.292 Mean :22.035 3rd Qu.: 0.011 3rd Qu.:26.543 Max. :30.851 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL If you want to reflect changes in the variable names while keeping the same object name, you need to assign the output of setNames() to the object as follows: #--- NOT RUN ---# # the codes in the rest of the chapter use &quot;ppt&quot; and &quot;tmax&quot; as the variables names, not these ones prcp_tmax_PRISM_m8_y09 &lt;- setNames(prcp_tmax_PRISM_m8_y09, c(&quot;precipitation&quot;, &quot;maximum_temp&quot;)) We will be using this function in Chapter 7.7.2. 7.2.3 Get the coordinate reference system You can get the CRS of a stars object using st_crs(), which is actually the same function name that we used to extract the CRS of an sf object. st_crs(prcp_tmax_PRISM_m8_y09) Coordinate Reference System: User input: GEOGCS[&quot;NAD83&quot;,DATUM[&quot;North_American_Datum_1983&quot;,SPHEROID[&quot;GRS 1980&quot;,6378137,298.2572221010042,AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]],PRIMEM[&quot;Greenwich&quot;,0],UNIT[&quot;degree&quot;,0.0174532925199433],AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]] wkt: GEOGCS[&quot;NAD83&quot;, DATUM[&quot;North_American_Datum_1983&quot;, SPHEROID[&quot;GRS 1980&quot;,6378137,298.2572221010042, AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]], PRIMEM[&quot;Greenwich&quot;,0], UNIT[&quot;degree&quot;,0.0174532925199433], AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]] You will use this to change the projection of vector datasets when you interact them: crop the stars raster dataset to the spatial extent of sf objects extract values from the stars raster dataset for sf objects as we will see in Chapter 5. Notice also that we used exactly the same function name (st_crs()) to get the CRS of sf objects (see Chapter 2). 7.2.4 Get the dimension characteristics and values You can access these dimension values using st_dimensions(). #--- get dimension characteristics ---# ( dim_prcp_tmin &lt;- st_dimensions(prcp_tmax_PRISM_m8_y09) ) from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL The following shows what we can get from this object. str(dim_prcp_tmin) List of 3 $ x :List of 7 ..$ from : num 1 ..$ to : num 20 ..$ offset: num -122 ..$ delta : num 0.0417 ..$ refsys:List of 2 .. ..$ input: chr &quot;GEOGCS[\\&quot;NAD83\\&quot;,DATUM[\\&quot;North_American_Datum_1983\\&quot;,SPHEROID[\\&quot;GRS 1980\\&quot;,6378137,298.2572221010042,AUTHORITY[&quot;| __truncated__ .. ..$ wkt : chr &quot;GEOGCS[\\&quot;NAD83\\&quot;,\\n DATUM[\\&quot;North_American_Datum_1983\\&quot;,\\n SPHEROID[\\&quot;GRS 1980\\&quot;,6378137,298.25722210&quot;| __truncated__ .. ..- attr(*, &quot;class&quot;)= chr &quot;crs&quot; ..$ point : logi FALSE ..$ values: NULL ..- attr(*, &quot;class&quot;)= chr &quot;dimension&quot; $ y :List of 7 ..$ from : num 1 ..$ to : num 20 ..$ offset: num 46.6 ..$ delta : num -0.0417 ..$ refsys:List of 2 .. ..$ input: chr &quot;GEOGCS[\\&quot;NAD83\\&quot;,DATUM[\\&quot;North_American_Datum_1983\\&quot;,SPHEROID[\\&quot;GRS 1980\\&quot;,6378137,298.2572221010042,AUTHORITY[&quot;| __truncated__ .. ..$ wkt : chr &quot;GEOGCS[\\&quot;NAD83\\&quot;,\\n DATUM[\\&quot;North_American_Datum_1983\\&quot;,\\n SPHEROID[\\&quot;GRS 1980\\&quot;,6378137,298.25722210&quot;| __truncated__ .. ..- attr(*, &quot;class&quot;)= chr &quot;crs&quot; ..$ point : logi FALSE ..$ values: NULL ..- attr(*, &quot;class&quot;)= chr &quot;dimension&quot; $ date:List of 7 ..$ from : num 1 ..$ to : int 10 ..$ offset: Date[1:1], format: &quot;2009-08-11&quot; ..$ delta : &#39;difftime&#39; num 1 .. ..- attr(*, &quot;units&quot;)= chr &quot;days&quot; ..$ refsys: chr &quot;Date&quot; ..$ point : logi NA ..$ values: NULL ..- attr(*, &quot;class&quot;)= chr &quot;dimension&quot; - attr(*, &quot;raster&quot;)=List of 3 ..$ affine : num [1:2] 0 0 ..$ dimensions : chr [1:2] &quot;x&quot; &quot;y&quot; ..$ curvilinear: logi FALSE ..- attr(*, &quot;class&quot;)= chr &quot;stars_raster&quot; - attr(*, &quot;class&quot;)= chr &quot;dimensions&quot; For example, you can get offset of x as follows: dim_prcp_tmin$x$offset [1] -121.7292 You can extract dimension values using st_get_dimension_values(). For example, to get values of date, #--- get date values ---# st_get_dimension_values(prcp_tmax_PRISM_m8_y09, &quot;date&quot;) [1] &quot;2009-08-11&quot; &quot;2009-08-12&quot; &quot;2009-08-13&quot; &quot;2009-08-14&quot; &quot;2009-08-15&quot; [6] &quot;2009-08-16&quot; &quot;2009-08-17&quot; &quot;2009-08-18&quot; &quot;2009-08-19&quot; &quot;2009-08-20&quot; This can be handy as you will see in Chapter 9.4.2. Later in Chapter 7.5, we will learn how to set dimensions using st_set_dimensions(). 7.2.5 Attributes to dimensions, and vice versa You can make attributes to dimensions using merge(). ( prcp_tmax_four &lt;- merge(prcp_tmax_PRISM_m8_y09) ) stars object with 4 dimensions and 1 attribute attribute(s): X Min. : 0.00 1st Qu.: 0.00 Median :11.80 Mean :11.66 3rd Qu.:21.54 Max. :39.71 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL X1 1 2 NA NA NA NA ppt , tmax As you can see, the new stars object has an additional dimension called X1, which represents attributes and has two dimension values: the first for ppt and second for tmax. Now, if you want to access ppt, you can do the following: prcp_tmax_four[,,,,&quot;ppt&quot;] stars object with 4 dimensions and 1 attribute attribute(s): X Min. : 0.00 1st Qu.: 0.00 Median :11.80 Mean :11.66 3rd Qu.:21.54 Max. :39.71 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL X1 1 2 NA NA NA NA ppt , tmax We can do this because the merge kept the attribute names as dimension values as you can see in values. You can revert it back to the original state using split(). Since we want the fourth dimension to dissolve, split(prcp_tmax_four, 4) stars object with 3 dimensions and 2 attributes attribute(s): ppt tmax Min. : 0.000 Min. : 1.833 1st Qu.: 0.000 1st Qu.:17.556 Median : 0.000 Median :21.483 Mean : 1.292 Mean :22.035 3rd Qu.: 0.011 3rd Qu.:26.543 Max. :30.851 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL "],
["quick-visualization-for-exploration.html", "7.3 Quick visualization for exploration", " 7.3 Quick visualization for exploration You can use tmap::qtm() to have a quick static map and mapview() or the tmap::tmap_leaflet() package for interactive views. 7.3.1 quick static map qtm(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,]) It gives you an error if you try to plot a stars object with multiple attributes: qtm(prcp_tmax_PRISM_m8_y09) Error in `[.data.frame`(as.data.frame(shp), shpnames): undefined columns selected , which is identical with this: qtm(prcp_tmax_PRISM_m8_y09[&quot;ppt&quot;,,,]) 7.3.2 interactive map mapview approach You can use mapview() from the mapview package to create an interactive map. library(mapview) #--- interactive map ---# mapView(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,]) Unfortunately, it does not map multiple layers on the same map even though you can select the band to plot with the band option. One thing you can do is to convert a stars to a Raster\\(^*\\) object first and then apply mapview() to it. as(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,], &quot;Raster&quot;) %&gt;% mapView() Note that the data values are lost and the layers are now named layer.1 through layer.10. This is because the value of the third dimension (date) is lost in the process of the conversion. tmap approach Alternatively, you can use the tmap package. You can apply tmap_leaflet() to a static tmap object to make it an interactive map. The tm_facets(as.layers = TRUE) option stacks all the layers in a single map. #--- make it interactive ---# tmap_leaflet( tm_shape(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,]) + tm_raster() + tm_facets(as.layers = TRUE) ) "],
["read-write-stars.html", "7.4 Reading and writing raster data", " 7.4 Reading and writing raster data There are so many formats in which raster data is stored. Some of the common ones include GeoTIFF, netCDF, GRIB. All the available GDAL drivers for reading and writing raster data can be found by the following code:88 sf::st_drivers(what = &quot;raster&quot;) The output of the above function is put into a table below. 7.4.1 Reading raster data You can use read_stars() to read a raster data file. It is very unlikely that the raster file you are trying to read is not in one of the supported formats. For example, you can read a GeoTIFF file as follows: ( ppt_m1_y09_stars &lt;- read_stars(&quot;./Data/PRISM_ppt_y2009_m1.tif&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] band 1 31 NA NA NA NA NULL This one imports a raw PRISM data stored as a BIL file. ( prism_tmax_20180701 &lt;- read_stars(&quot;./Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil&quot;) ) stars object with 2 dimensions and 1 attribute attribute(s): PRISM_tmax_stable_4kmD2_20180701_bil.bil Min. : 3.6 1st Qu.:25.3 Median :31.6 Mean :29.5 3rd Qu.:33.7 Max. :48.0 NA&#39;s :390874 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 NA NULL [x] y 1 621 49.9375 -0.0416667 NAD83 NA NULL [y] You can import multiple raster data files into one stars object by simply supplying a vector of file names: files &lt;- c(&quot;./Data/PRISM_tmax_stable_4kmD2_20180701_bil/PRISM_tmax_stable_4kmD2_20180701_bil.bil&quot;, &quot;./Data/PRISM_tmax_stable_4kmD2_20180702_bil/PRISM_tmax_stable_4kmD2_20180702_bil.bil&quot;) ( prism_tmax_201807_two &lt;- read_stars(files) ) stars object with 2 dimensions and 2 attributes attribute(s): PRISM_tmax_stable_4kmD2_20180701_bil.bil Min. : 3.6 1st Qu.:25.3 Median :31.6 Mean :29.5 3rd Qu.:33.7 Max. :48.0 NA&#39;s :390874 PRISM_tmax_stable_4kmD2_20180702_bil.bil Min. : 0.8 1st Qu.:26.5 Median :30.6 Mean :29.9 3rd Qu.:33.6 Max. :48.0 NA&#39;s :390874 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 NA NULL [x] y 1 621 49.9375 -0.0416667 NAD83 NA NULL [y] As you can see, each file becomes an attribute.89 It is more convenient to have them as layers stacked along the third dimension. To do that you can add along = 3 option as follows: ( prism_tmax_201807_two &lt;- read_stars(files, along = 3) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_tmax_stable_4kmD2_20180701_bil.bil Min. : 4.69 1st Qu.:19.59 Median :23.65 Mean :22.05 3rd Qu.:24.59 Max. :33.40 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 NA NULL [x] y 1 621 49.9375 -0.0416667 NAD83 NA NULL [y] new_dim 1 2 NA NA NA NA NULL Note that while the GeoTIFF format (and many other formats) can store multi-band (multi-layer) raster data allowing for an additional dimension beyond x and y, it does not store the values of the dimension like the date dimension we saw in prcp_tmax_PRISM_m8_y09. So, when you read a multi-layer raster data saved as a GeoTIFF, the third dimension of the resulting stars object will always be called band without any explicit time information. On the other hand, netCDF files are capable of storing the time dimension values. So, when you read a netCDF file with valid time dimension, you will have time dimension when it is read. ( read_ncdf(system.file(&quot;nc/bcsd_obs_1999.nc&quot;, package = &quot;stars&quot;)) ) stars object with 3 dimensions and 2 attributes attribute(s): pr [mm/m] tas [C] Min. : 0.59 Min. :-0.421 1st Qu.: 56.14 1st Qu.: 8.899 Median : 81.88 Median :15.658 Mean :101.26 Mean :15.489 3rd Qu.:121.07 3rd Qu.:21.780 Max. :848.55 Max. :29.386 NA&#39;s :7116 NA&#39;s :7116 dimension(s): from to offset delta refsys point values longitude 1 81 -85 0.125 WGS 84 NA NULL [x] latitude 1 33 33 0.125 WGS 84 NA NULL [y] time 1 12 NA NA POSIXct NA 1999-01-31,...,1999-12-31 The refsys of the time dimension is POSIXct, which is one of the date classes. 7.4.2 Writing a stars object to a file You can write a stars object to a file using write_stars() using one of the GDAL drivers. Let’s save prcp_tmax_PRISM_m8_y09[\"tmax\",,,], which has date dimension whose refsys is Date. write_stars(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,], &quot;./Data/tmax_m8_y09_from_stars.tif&quot;) Let’s read the file we just saved. read_stars(&quot;./Data/tmax_m8_y09_from_stars.tif&quot;) stars object with 3 dimensions and 1 attribute attribute(s): tmax_m8_y09_from_stars.tif Min. : 1.833 1st Qu.:17.556 Median :21.483 Mean :22.035 3rd Qu.:26.543 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] band 1 10 NA NA NA NA NULL Notice that the third dimension is now called band and all the date information is lost. The loss of information happened when we saved prcp_tmax_PRISM_m8_y09[\"tmax\",,,] as a GeoTIFF file. One easy way to avoid this problem is to just save a stars object as an R dataset. #--- save it as an rds file ---# saveRDS(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,], &quot;./Data/tmax_m8_y09_from_stars.rds&quot;) #--- read it back ---# readRDS(&quot;./Data/tmax_m8_y09_from_stars.rds&quot;) stars object with 3 dimensions and 1 attribute attribute(s): tmax Min. : 1.833 1st Qu.:17.556 Median :21.483 Mean :22.035 3rd Qu.:26.543 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL As you can see, date information is retained. So, if you are the only one who uses this data or all of your team members use R, then this is a nice solution to the problem.90 At the moment, it is not possible to use write_stars() to write to a netCDF file that supports the third dimension as time. However, this may not be the case for a long time (See the discussion here). What drivers are available varies depending on your particular installation of GDAL version. But, this is not something you need to worry about. It is very unlikely that you are reading or writing raster data in the format that is not available.↩︎ You can use ↩︎ But, you would be giving up the opportunity to use stars_proxy by doing so.↩︎ "],
["stars-set-time.html", "7.5 Setting the time dimension manually", " 7.5 Setting the time dimension manually For this section, we will use PRISM precipitation data for U.S. for January, 2009. ( ppt_m1_y09_stars &lt;- read_stars(&quot;./Data/PRISM/PRISM_ppt_y2009_m1.tif&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] band 1 31 NA NA NA NA NULL As you can see, when you read a GeoTIFF file, the third dimension will always be called band because GeoTIFF format does not support dimension values for the third dimension.91 You can use st_set_dimension() to set the third dimension (called band) as the time dimension using Date object. This can be convenient when you would like to filter the data by date using filter() as we will see later. For ppt_m1_y09_stars, precipitation is observed on a daily basis from January 1, 2009 to January 31, 2009, where the band value of x corresponds to January x, 2009. So, we can first create a vector of dates as follows (If you are not familiar with Dates and the lubridate pacakge, this is a good resource to learn them.): #--- starting date ---# start_date &lt;- ymd(&quot;2009-01-01&quot;) #--- ending date ---# end_date &lt;- ymd(&quot;2009-01-31&quot;) #--- sequence of dates ---# dates_ls_m1 &lt;- seq(start_date, end_date, &quot;days&quot;) We can then use st_set_dimensions() to change the third dimension to the dimension of date. #--- syntax (NOT RUN) ---# st_set_dimensions(stars object, dimension, values = dimension values, names = name of the dimension) ( ppt_m1_y09_stars &lt;- st_set_dimensions(ppt_m1_y09_stars, 3, values = dates_ls_m1, names = &quot;date&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 31 2009-01-01 1 days Date NA NULL As you can see, the third dimension has become date. The value of offset for the date dimension has become 2009-01-01 meaning the starting date value is now 2009-01-01. Further, the value of delta is now 1 days, so date dimension of x corresponds to 2009-01-01 + x - 1 = 2009-01-x. Note that the date dimension does not have to be regularly spaced. For example, you may have satellite images available for your area with a 5-day interval sometimes and a 6-day interval other times. This is perfectly fine. As an illustration, I will create a wrong sequence of dates for this data with a 2-day gap in the middle and assign them to the date dimension to see what happens. #--- 2009-01-23 removed and 2009-02-01 added ---# ( dates_ls_wrong &lt;- c(seq(start_date, end_date, &quot;days&quot;)[-23], ymd(&quot;2009-02-01&quot;)) ) [1] &quot;2009-01-01&quot; &quot;2009-01-02&quot; &quot;2009-01-03&quot; &quot;2009-01-04&quot; &quot;2009-01-05&quot; [6] &quot;2009-01-06&quot; &quot;2009-01-07&quot; &quot;2009-01-08&quot; &quot;2009-01-09&quot; &quot;2009-01-10&quot; [11] &quot;2009-01-11&quot; &quot;2009-01-12&quot; &quot;2009-01-13&quot; &quot;2009-01-14&quot; &quot;2009-01-15&quot; [16] &quot;2009-01-16&quot; &quot;2009-01-17&quot; &quot;2009-01-18&quot; &quot;2009-01-19&quot; &quot;2009-01-20&quot; [21] &quot;2009-01-21&quot; &quot;2009-01-22&quot; &quot;2009-01-24&quot; &quot;2009-01-25&quot; &quot;2009-01-26&quot; [26] &quot;2009-01-27&quot; &quot;2009-01-28&quot; &quot;2009-01-29&quot; &quot;2009-01-30&quot; &quot;2009-01-31&quot; [31] &quot;2009-02-01&quot; Now assign these date values to ppt_m1_y09_stars: #--- set date values ---# ( ppt_m1_y09_stars_wrong &lt;- st_set_dimensions(ppt_m1_y09_stars, 3, values = dates_ls_wrong, names = &quot;date&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 31 NA NA Date NA 2009-01-01,...,2009-02-01 Since the step between the date values is no longer \\(1\\) day for the entire sequence, the value of delta is now NA. However, notice that the value of date is no longer NULL. Since the date is not regular, you can not represent date using three values (from, to, and delta) any more, and date values for each observation have to be stored now. Finally, note that just applying st_set_dimensions() to a stars object does not change the dimension of the stars object (just like setNames() as we discussed above). ppt_m1_y09_stars stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 31 2009-01-01 1 days Date NA NULL As you can see, the date dimension has not been altered. You need to assign the results of st_set_dimensions() to a stars object to see the changes in the dimension reflected just like we did above with the right date values. In Chapter 7.1, we read an rds, which is a stars object with dimension name set manually by me.↩︎ "],
["dplyr-op.html", "7.6 dplyr-like operations", " 7.6 dplyr-like operations You can use the dplyr language to do basic data operations on stars objects. 7.6.1 filter() The filter() function allows you to subset data by dimension values: x, y, and band (here date). spatial filtering library(tidyverse) #--- longitude greater than -100 ---# filter(ppt_m1_y09_stars, x &gt; -100) %&gt;% plot() #--- latitude less than 40 ---# filter(ppt_m1_y09_stars, y &lt; 40) %&gt;% plot() temporal filtering Finally, since the date dimension is in Date, you can use Date math to filter the data.92 #--- dates after 2009-01-15 ---# filter(ppt_m1_y09_stars, date &gt; ymd(&quot;2009-01-21&quot;)) %&gt;% plot() filter by attribute? Just in case you are wondering. You cannot filter by attribute. filter(ppt_m1_y09_stars, ppt &gt; 20) Error: ``~``, `ppt &gt; 20` must refer to exactly one dimension, not `` 7.6.2 select() The select() function lets you pick certain attributes. dplyr::select(prcp_tmax_PRISM_m8_y09, ppt) stars object with 3 dimensions and 1 attribute attribute(s): ppt Min. : 0.000 1st Qu.: 0.000 Median : 0.000 Mean : 1.292 3rd Qu.: 0.011 Max. :30.851 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL 7.6.3 mutate() You can mutate attributes using the mutate() function. For example, this can be useful to calculate NDVI in a stars object that has Red and NIR (spectral reflectance measurements in the red and near-infrared regions) as attributes. Here, we just simply convert the unit of precipitation from mm to inches. #--- mm to inches ---# mutate(prcp_tmax_PRISM_m8_y09, ppt = ppt * 0.0393701) stars object with 3 dimensions and 2 attributes attribute(s): ppt tmax Min. :0.0000000 Min. : 1.833 1st Qu.:0.0000000 1st Qu.:17.556 Median :0.0000000 Median :21.483 Mean :0.0508793 Mean :22.035 3rd Qu.:0.0004331 3rd Qu.:26.543 Max. :1.2146069 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 FALSE NULL [x] y 1 20 46.6458 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-08-11 1 days Date NA NULL 7.6.4 pull() You can extract attribute values using pull(). #--- tmax values of the 1st date layer ---# pull(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,1], &quot;tmax&quot;) , , 1 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 25.495 27.150 22.281 19.553 21.502 19.616 21.664 24.458 21.712 19.326 [2,] 27.261 27.042 21.876 19.558 19.141 19.992 18.694 24.310 21.094 20.032 [3,] 27.568 21.452 23.220 19.428 18.974 19.366 20.293 24.418 20.190 21.127 [4,] 23.310 20.357 20.111 21.380 19.824 19.638 22.135 20.885 20.121 18.396 [5,] 19.571 20.025 18.407 19.142 19.314 22.759 22.652 19.566 18.817 16.065 [6,] 17.963 16.581 17.121 16.716 19.680 20.521 17.991 18.007 17.430 14.586 [7,] 20.911 19.202 16.309 14.496 17.429 19.083 18.509 18.723 17.645 16.415 [8,] 17.665 16.730 17.691 14.370 16.849 18.413 17.787 20.000 19.319 18.401 [9,] 16.795 18.091 20.460 16.405 18.331 19.005 19.142 21.226 21.184 19.520 [10,] 19.208 19.624 17.210 19.499 18.492 20.854 18.684 19.811 22.058 19.923 [11,] 23.148 18.339 19.676 20.674 18.545 21.126 19.013 19.722 21.843 21.271 [12,] 23.254 21.279 21.921 19.894 19.445 21.499 19.765 20.742 21.560 22.989 [13,] 23.450 21.956 19.813 18.970 20.173 20.567 21.152 20.932 19.836 20.347 [14,] 24.075 21.120 20.166 19.177 20.428 20.908 21.060 19.832 19.764 19.981 [15,] 24.318 20.943 20.024 20.022 19.040 19.773 20.452 20.152 20.321 20.304 [16,] 22.538 19.461 20.100 21.149 19.958 20.486 20.535 20.445 21.564 21.493 [17,] 20.827 20.192 21.165 22.369 21.488 22.031 21.552 21.089 21.687 23.375 [18,] 21.089 21.451 22.692 21.793 22.160 23.049 22.562 22.738 23.634 24.697 [19,] 22.285 22.992 23.738 23.497 24.255 25.177 25.411 24.324 24.588 26.032 [20,] 23.478 23.584 24.589 24.719 26.114 26.777 27.310 26.643 26.516 26.615 [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [1,] 24.845 23.211 21.621 23.180 21.618 21.322 22.956 23.749 23.162 25.746 [2,] 24.212 21.820 21.305 22.960 22.806 22.235 23.604 23.689 25.597 25.933 [3,] 20.879 20.951 22.280 23.536 23.455 22.537 24.760 23.587 26.170 24.764 [4,] 17.435 18.681 22.224 24.122 25.828 25.604 25.298 22.817 24.438 24.835 [5,] 14.186 17.789 20.624 23.416 26.059 27.571 25.158 24.201 26.001 26.235 [6,] 10.188 15.632 19.907 22.660 25.268 27.469 27.376 27.488 27.278 27.558 [7,] 14.797 15.933 19.204 21.641 23.107 25.626 26.990 25.838 26.906 27.247 [8,] 17.325 18.299 19.691 21.553 21.840 23.754 26.099 25.270 26.282 26.981 [9,] 19.322 19.855 20.489 22.597 23.614 25.873 26.906 26.368 26.332 25.844 [10,] 20.241 21.800 22.111 24.128 25.765 27.105 27.200 25.491 26.306 25.663 [11,] 23.398 24.090 24.884 25.596 26.545 27.014 26.464 25.708 25.742 25.336 [12,] 22.576 21.996 23.874 26.447 26.955 26.871 25.533 25.576 25.610 25.902 [13,] 22.023 22.358 24.996 26.185 27.249 25.617 25.623 25.600 25.433 26.681 [14,] 20.974 23.533 25.388 25.975 27.316 26.199 26.090 25.920 25.767 27.956 [15,] 20.982 23.632 24.703 25.539 26.515 27.133 27.407 27.518 27.149 28.506 [16,] 22.500 24.012 25.282 25.751 25.212 25.290 26.058 28.258 28.290 29.842 [17,] 23.024 24.381 25.157 25.259 24.829 24.183 25.632 26.947 28.601 29.589 [18,] 24.016 24.425 24.965 24.930 24.482 23.274 25.412 26.733 28.494 29.656 [19,] 24.065 24.105 24.145 24.318 23.912 22.782 25.039 26.554 28.184 29.012 [20,] 23.979 24.321 23.477 22.135 22.395 22.189 24.944 26.542 27.923 28.849 Of course, this is possible only because we have assigned date values to the band dimension above.↩︎ "],
["merging-stars-objects-using-c-and-st-mosaic.html", "7.7 Merging stars objects using c() and st_mosaic()", " 7.7 Merging stars objects using c() and st_mosaic() 7.7.1 Merging stars objects along the third dimension (band) Here we learn how to merge multiple stars objects that have the same attributes the same spatial extent and resolution different bands (dates here) For example, consider merging PRISM precipitation data in January and February. Both of them have exactly the same spatial extent and resolutions and represent the same attribute (precipitation). However, they differ in the third dimension (date). So, you are trying to stack data of the same attributes along the third dimension (date) while making sure that spatial correspondence is maintained. This merge is kind of like rbind() that stacks multiple data.frames vertically while making sure the variables are aligned correctly. Let’s import the PRISM precipitation data for February, 2009. #--- read the February ppt data ---# ( ppt_m2_y09_stars &lt;- read_stars(&quot;./Data/PRISM_ppt_y2009_m2.tif&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m2.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.00 Mean : 0.19 3rd Qu.: 0.00 Max. :11.63 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] band 1 28 NA NA NA NA NULL Note here that the third dimension of ppt_m2_y09_stars has not been changed to date. Now, let’s try to merge the two: #--- combine the two ---# ( ppt_m1_to_m2_y09_stars &lt;- c(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 59 2009-01-01 1 days Date NA NULL As you noticed, the second object (ppt_m2_y09_stars) was assumed to have the same date characteristics as the first one: the data in February is observed daily (delta is 1 day). This causes no problem in this instance as the February data is indeed observed daily starting from 2009-02-01. However, be careful if you are appending the data that does not start from 1 day after (or more generally delta for the time dimension) the first data or the data that does not follow the same observation interval. For this reason, it is advisable to first set the date values if it has not been set. Pretend that the February data actually starts from 2009-02-02 to 2009-03-01 to see what happens when the regular interval (delta) is not kept after merging. #--- starting date ---# start_date &lt;- ymd(&quot;2009-02-01&quot;) #--- ending date ---# end_date &lt;- ymd(&quot;2009-02-28&quot;) #--- sequence of dates ---# dates_ls &lt;- seq(start_date, end_date, &quot;days&quot;) #--- pretend the data actually starts from `2009-02-02` to `2009-03-01` ---# ( ppt_m2_y09_stars &lt;- st_set_dimensions(ppt_m2_y09_stars, 3, values = c(dates_ls[-1], ymd(&quot;2009-03-01&quot;)), name = &quot;date&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m2.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.00 Mean : 0.19 3rd Qu.: 0.00 Max. :11.63 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 28 2009-02-02 1 days Date NA NULL If you merge the two, c(ppt_m1_y09_stars, ppt_m2_y09_stars, along = 3) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif Min. : 0.00 1st Qu.: 0.00 Median : 0.44 Mean : 3.54 3rd Qu.: 3.49 Max. :56.21 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 59 NA NA Date NA 2009-01-01,...,2009-03-01 The date dimension does not have delta any more, and correctly so because there is a one-day gap between the end date of the first stars object (“2009-01-31”) and the start date of the second stars object (“2009-02-02”). So, all the date values are now stored in values. 7.7.2 Merging stars objects of different attributes Here we learn how to merge multiple stars objects that have different attributes the same spatial extent and resolution the same bands (dates here) For example, consider merging PRISM precipitation and tmax data in January. Both of them have exactly the same spatial extent and resolutions and the date characteristics (starting and ending on the same dates with the same time interval). However, they differ in what they represent: precipitation and tmax. This merge is kind of like cbind() that combines multiple data.frames of different variables while making sure the observation correspondence is correct. Let’s read the daily tmax data for January, 2009: ( tmax_m1_y09_stars &lt;- read_stars(&quot;./Data/PRISM_tmax_y2009_m1.tif&quot;) %&gt;% #--- change the attribute name ---# setNames(&quot;tmax&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: tmax Min. :-17.90 1st Qu.: -7.76 Median : -2.25 Mean : -3.06 3rd Qu.: 1.72 Max. : 7.99 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] band 1 31 NA NA NA NA NULL Now, let’s merge the PRISM ppt and tmax data in January, 2009. c(ppt_m1_y09_stars, tmax_m1_y09_stars) Error in c.stars(ppt_m1_y09_stars, tmax_m1_y09_stars): don&#39;t know how to merge arrays: please specify parameter along Oops. Well, the problem is that the third dimension of the two objects is not the same. Even though we know that the xth element of their third dimension represent the same thing, they look different to R’s eyes. So, we first need to change the third dimension of tmax_m1_y09_stars to be consistent with the third dimension of ppt_m1_y09_stars (dates_ls_m1 was defined in Chapter 7.5). ( tmax_m1_y09_stars &lt;- st_set_dimensions(tmax_m1_y09_stars, 3, values = dates_ls_m1, names = &quot;date&quot;) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: tmax Min. :-17.90 1st Qu.: -7.76 Median : -2.25 Mean : -3.06 3rd Qu.: 1.72 Max. : 7.99 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 31 2009-01-01 1 days Date NA NULL Now, we can merge the two. ( ppt_tmax_m1_y09_stars &lt;- c(ppt_m1_y09_stars, tmax_m1_y09_stars) ) stars object with 3 dimensions and 2 attributes attribute(s), summary of first 1e+05 cells: PRISM_ppt_y2009_m1.tif tmax Min. : 0.00 Min. :-17.90 1st Qu.: 0.00 1st Qu.: -7.76 Median : 0.44 Median : -2.25 Mean : 3.54 Mean : -3.06 3rd Qu.: 3.49 3rd Qu.: 1.72 Max. :56.21 Max. : 7.99 NA&#39;s :60401 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 31 2009-01-01 1 days Date NA NULL As you can see, now we have another attribute called tmax. 7.7.3 Merging stars objects of different spatial extents Here we learn how to merge multiple stars objects that have the same attributes different spatial extent but same resolution93 the same bands (dates here) Some times you have multiple separate raster datasets that have different spatial coverages and would like to combine them into one. You can do that using st_mosaic(). Let’s split tmax_m1_y09_stars into two parts: tmax_1 &lt;- filter(tmax_m1_y09_stars, x &lt;= -100) tmax_2 &lt;- filter(tmax_m1_y09_stars, x &gt; -100) Here (Figure 7.3) is what they look like (only Jan 1, 1980): g_1 &lt;- ggplot() + geom_stars(data = tmax_1[,,,1]) + theme_for_map + theme( legend.position = &quot;bottom&quot; ) g_2 &lt;- ggplot() + geom_stars(data = tmax_2[,,,1]) + theme_for_map + theme( legend.position = &quot;bottom&quot; ) library(patchwork) g_1 + g_2 Figure 7.3: Two spatially non-overlapping stars objects Let’s combine the two using st_mosaic(): tmax_combined &lt;- st_mosaic(tmax_1, tmax_2) Here is what the combined object looks like (Figure 7.4) ggplot() + geom_stars(data = tmax_combined[,,,1]) + theme_for_map Figure 7.4: Map of the stars objects combined into one It is okay to have the two stars objects to be combined have a spatial overlap. The following split creates two stars objects with a spatial overlap. tmax_1 &lt;- filter(tmax_m1_y09_stars, x &lt;= -100) tmax_2 &lt;- filter(tmax_m1_y09_stars, x &gt; -110) Here (Figure 7.5) is what they look like (only Jan 1, 1980): g_1 &lt;- ggplot() + geom_stars(data = tmax_m1_y09_stars[,,,1], fill = NA) + geom_stars(data = tmax_1[,,,1]) + theme_for_map g_2 &lt;- ggplot() + geom_stars(data = tmax_m1_y09_stars[,,,1], fill = NA) + geom_stars(data = tmax_2[,,,1]) + theme_for_map library(patchwork) g_1 / g_2 Figure 7.5: Two spatially overlapping stars objects As you can see below, st_mosaic() reconciles the spatial overlap between the two stars objects. ( tmax_combined &lt;- st_mosaic(tmax_1, tmax_2) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: tmax Min. :-17.90 1st Qu.: -7.76 Median : -2.25 Mean : -3.06 3rd Qu.: 1.72 Max. : 7.99 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 NA NULL [x] y 1 621 49.9375 -0.0416667 NAD83 NA NULL [y] band 1 31 NA NA NA NA NULL Here is the plot (Figure 7.6): ggplot() + geom_stars(data = tmax_combined[,,,1]) + theme_for_map Figure 7.6: Map of the spatially-overlapping stars objects combined into one Technically, you can actually merge stars objects of different spatial resolutions. But, you probably should not.↩︎ "],
["convert-to-rb.html", "7.8 Convert from and to Raster\\(^*\\) objects", " 7.8 Convert from and to Raster\\(^*\\) objects When you need to convert a stars object to a Raster\\(^*\\) object, you can use the as() function as follows: ( prcp_tmax_PRISM_m8_y09_rb &lt;- as(prcp_tmax_PRISM_m8_y09, &quot;Raster&quot;) ) class : RasterBrick dimensions : 20, 20, 400, 10 (nrow, ncol, ncell, nlayers) resolution : 0.04166667, 0.04166667 (x, y) extent : -121.7292, -120.8958, 45.8125, 46.64583 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0 source : memory names : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 min values : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 max values : 0.019, 18.888, 30.851, 4.287, 0.797, 0.003, 3.698, 1.801, 0.000, 0.000 As you can see, date values in prcp_tmax_PRISM_m8_y09 are lost, and the band names are not informative. Direct conversion to a terra object is not supported at the moment. Note also that the conversion was done for only the ppt attribute. This is simply because the raster package cannot accommodate multiple attributes of 3-dimensional array. So, if you want a RasterBrick of the tmax data, then you need to do the following: ( tmax_PRISM_m8_y09_rb &lt;- as(prcp_tmax_PRISM_m8_y09[&quot;tmax&quot;,,,], &quot;Raster&quot;) ) class : RasterBrick dimensions : 20, 20, 400, 10 (nrow, ncol, ncell, nlayers) resolution : 0.04166667, 0.04166667 (x, y) extent : -121.7292, -120.8958, 45.8125, 46.64583 (xmin, xmax, ymin, ymax) crs : +proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0 source : memory names : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6, layer.7, layer.8, layer.9, layer.10 min values : 10.188, 8.811, 6.811, 3.640, 1.833, 6.780, 10.318, 14.270, 18.281, 19.818 max values : 29.842, 28.892, 27.123, 23.171, 21.858, 24.690, 27.740, 32.906, 35.568, 39.707 You can convert a Raster\\(^*\\) object to a stars object using st_as_stars() (you will see a use case in Chapter 9.4.2). ( tmax_PRISM_m8_y09_back_to_stars &lt;- st_as_stars(tmax_PRISM_m8_y09_rb) ) stars object with 3 dimensions and 1 attribute attribute(s): layer.1 Min. : 1.833 1st Qu.:17.556 Median :21.483 Mean :22.035 3rd Qu.:26.543 Max. :39.707 dimension(s): from to offset delta refsys point values x 1 20 -121.729 0.0416667 NAD83 NA NULL [x] y 1 20 46.6458 -0.0416667 NAD83 NA NULL [y] band 1 10 NA NA NA NA layer.1,...,layer.10 Of course, we have lost date values when we turned prcp_tmax_PRISM_m8_y09[\"tmax\",,,] into tmax_PRISM_m8_y09_rb. So, obviously, we are not getting that back when we turned tmax_PRISM_m8_y09_rb back into a stars object. "],
["spatial-cropping.html", "7.9 Spatial cropping", " 7.9 Spatial cropping If the region of interest is smaller than the spatial extent of the stars raster data, then there is no need to carry around the irrelevant part of the stars. In such a case, you can crop the stars to the region of interest using st_crop(). The general syntax of st_crop() is #--- NOT RUN ---# st_crop(stars object, sf object) You can use an sfc or bbox objects in place of an sf object. For demonstration, we use PRISM tmax data for the U.S. for January 2019 as a stars object. ( tmax_m8_y09_stars &lt;- read_stars(&quot;./Data/PRISM_tmax_y2009_m8.tif&quot;) %&gt;% setNames(&quot;tmax&quot;) %&gt;% filter(band &lt;= 10) %&gt;% st_set_dimensions( &quot;band&quot;, values = seq(ymd(&quot;2009-01-01&quot;), ymd(&quot;2009-01-10&quot;), by = &quot;days&quot;), name = &quot;date&quot; ) ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: tmax Min. :15.08 1st Qu.:20.18 Median :22.58 Mean :23.87 3rd Qu.:26.33 Max. :41.25 NA&#39;s :60401 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 FALSE NULL [x] y 1 621 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-01-01 1 days Date NA NULL The region of interest is Michigan. MI_county_sf &lt;- st_as_sf(maps::map(&quot;county&quot;, &quot;michigan&quot;, plot = FALSE, fill = TRUE)) %&gt;% #--- transform using the CRS of the PRISM stars data ---# st_transform(st_crs(tmax_m8_y09_stars)) We can crop the tmax data to the Michigan state border using st_crop() as follows: ( tmax_MI &lt;- st_crop(tmax_m8_y09_stars, MI_county_sf) ) stars object with 3 dimensions and 1 attribute attribute(s): tmax Min. :16.02 1st Qu.:22.54 Median :24.38 Mean :24.67 3rd Qu.:26.23 Max. :35.37 NA&#39;s :172350 dimension(s): from to offset delta refsys point values x 831 1022 -125.021 0.0416667 NAD83 FALSE NULL [x] y 59 198 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-01-01 1 days Date NA NULL Notice that from and to for x and y have changed to cover only the boundary box of the Michigan state border. Note that the values for the cells outside of the Michigan state border were set to NA. The following plot clearly shows the cropping was successful. plot(tmax_MI[,,,1]) Figure 7.7: PRISM tmax data cropped to the Michigan state border Alternatively, you could use [] like as follows to crop a stars object. tmax_m8_y09_stars[MI_county_sf] stars object with 3 dimensions and 1 attribute attribute(s): tmax Min. :16.02 1st Qu.:22.54 Median :24.38 Mean :24.67 3rd Qu.:26.23 Max. :35.37 NA&#39;s :172350 dimension(s): from to offset delta refsys point values x 831 1022 -125.021 0.0416667 NAD83 FALSE NULL [x] y 59 198 49.9375 -0.0416667 NAD83 FALSE NULL [y] date 1 10 2009-01-01 1 days Date NA NULL "],
["extraction-stars.html", "7.10 Extracting values from stars for vector data", " 7.10 Extracting values from stars for vector data 7.10.1 Points We can extract the value of raster cells in which points are located (exactly like Chapter 5) using st_extract(). #--- NOT RUN ---# st_extract(stars object, sf of points) Let’s first create points data that are randomly located inside Michigan. MI_points &lt;- st_sample(MI_county_sf, size = 1000) %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) Here is what the generated points look like: ggplot() + geom_stars(data = tmax_MI[,,,1]) + geom_sf(data = MI_points) + theme_for_map Figure 7.8: Map of randomly located points inside Michigan We now extract the value of rasters in which points are located: ( extracted_tmax &lt;- st_extract(tmax_MI, MI_points) %&gt;% st_as_sf() ) Simple feature collection with 1000 features and 10 fields geometry type: POINT dimension: XY bbox: xmin: -90.26031 ymin: 41.71712 xmax: -82.47312 ymax: 47.43735 geographic CRS: NAD83 First 10 features: 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 1 25.450 25.727 24.810 27.087 29.131 22.974 25.247 2 21.029 18.575 20.100 21.445 18.854 19.699 20.109 3 21.757 22.835 21.778 22.979 21.894 21.873 22.121 4 23.637 26.514 24.561 25.808 29.675 22.183 23.904 5 22.868 19.672 20.685 23.881 20.625 21.422 21.553 6 24.280 22.298 21.853 23.678 22.827 22.326 23.254 7 22.751 19.308 20.950 22.155 20.325 20.560 21.333 8 24.793 25.775 24.474 26.520 28.726 23.518 24.486 9 25.878 26.348 23.853 27.044 28.963 24.661 25.572 10 24.918 21.120 22.429 25.840 22.183 22.865 23.427 2009-01-08 2009-01-09 2009-01-10 sfc 1 23.726 23.338 32.184 POINT (-83.34935 43.49406) 2 20.965 22.658 22.536 POINT (-88.20274 46.66783) 3 20.369 19.555 29.512 POINT (-83.50303 45.32732) 4 21.052 22.764 32.978 POINT (-82.47312 43.03622) 5 20.193 26.205 25.094 POINT (-89.91415 46.33971) 6 21.973 22.260 28.648 POINT (-84.57265 44.56324) 7 23.214 22.396 23.083 POINT (-88.43553 47.33342) 8 23.078 23.250 32.018 POINT (-83.014 43.4846) 9 25.717 25.521 32.768 POINT (-84.04252 42.82157) 10 21.740 24.730 27.868 POINT (-87.85009 46.39967) As you can see each date forms a column of extracted values for the points because the third dimension of tmax_MI is Dates object. So, you can easily turn the outcome to an sf object with date as Date object as follows. pivot_longer(extracted_tmax, - sfc, names_to = &quot;date&quot;, values_to = &quot;tmax&quot;) %&gt;% st_as_sf() %&gt;% mutate(date = ymd(date)) Simple feature collection with 10000 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: -90.26031 ymin: 41.71712 xmax: -82.47312 ymax: 47.43735 geographic CRS: NAD83 # A tibble: 10,000 x 3 sfc date tmax * &lt;POINT [°]&gt; &lt;date&gt; &lt;dbl&gt; 1 (-83.34935 43.49406) 2009-01-01 25.5 2 (-83.34935 43.49406) 2009-01-02 25.7 3 (-83.34935 43.49406) 2009-01-03 24.8 4 (-83.34935 43.49406) 2009-01-04 27.1 5 (-83.34935 43.49406) 2009-01-05 29.1 6 (-83.34935 43.49406) 2009-01-06 23.0 7 (-83.34935 43.49406) 2009-01-07 25.2 8 (-83.34935 43.49406) 2009-01-08 23.7 9 (-83.34935 43.49406) 2009-01-09 23.3 10 (-83.34935 43.49406) 2009-01-10 32.2 # … with 9,990 more rows You cannot extract values from multiple attributes at the same time. tmax_MI %&gt;% mutate(second_atr = 1) %&gt;% st_extract(MI_points) %&gt;% st_as_sf() Error in sf::gdal_write(obj, ..., file = dsn, driver = driver, options = options, : dimensions don&#39;t match aggregate() is much slower compared to terra::extract(), which is discussed in section 5. So, if you are finding the speed of extraction an issue, you can do the following: convert the stars object to a SpatRaster object use terra::extract() to extract values for the vector data assign the date values to the data.frame of extracted values #--- Step 1: stars to RasterBrick to SpatRaster ---# tmax_MI_sr &lt;- tmax_MI %&gt;% #--- to RasterBrick ---# as(&quot;Raster&quot;) %&gt;% #--- to SpatRaster ---# rast() #--- Step 2: extraction ---# extracted_values &lt;- terra::extract(tmax_MI_sr, st_coordinates(MI_points)) %&gt;% data.frame() %&gt;% #--- assign id ---# mutate(id = MI_points$id) #--- Step 3: assign dates as the variable names ---# date_values &lt;- as.character(st_get_dimension_values(tmax_MI, &quot;date&quot;)) names(extracted_values) &lt;- c(date_values, &quot;id&quot;) #--- wide to long ---# pivot_longer(extracted_values, -id, , names_to = &quot;date&quot;, values_to = &quot;tmax&quot;) I must say, this is quite a hassle in terms of coding. But, if you need to do extraction jobs fast many many times, this approach may be beneficial. 7.10.2 Polygons In order to extract cell values from stars objects (just like Chapter 5) and summarize them for polygons, you can aggregate(). #--- NOT RUN ---# aggregate(stars object, sf object, FUN = function to apply) Let’s create polygons such that they cover Michigan: MI_polygons &lt;- st_make_grid(MI_county_sf, n = c(50, 50)) %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) Here is what it looks like (Figure 7.9): ggplot() + geom_stars(data = tmax_MI[,,,1]) + geom_sf(data = MI_polygons, fill = NA) + theme_for_map Figure 7.9: Map of regularly-sized polygons over Michigan For example this will find the mean of the tmax values for each polygon: ( mean_tmax &lt;- aggregate(tmax_MI, MI_polygons, FUN = mean) %&gt;% st_as_sf() ) Simple feature collection with 1064 features and 10 fields geometry type: POLYGON dimension: XY bbox: xmin: -90.41273 ymin: 41.71133 xmax: -82.44289 ymax: 47.48101 geographic CRS: NAD83 First 10 features: 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 1 NA NA NA NA NA NA NA 2 NA NA NA NA NA NA NA 3 NA NA NA NA NA NA NA 4 NA NA NA NA NA NA NA 5 NA NA NA NA NA NA NA 6 NA NA NA NA NA NA NA 7 NA NA NA NA NA NA NA 8 NA NA NA NA NA NA NA 9 NA NA NA NA NA NA NA 10 NA NA NA NA NA NA NA 2009-01-08 2009-01-09 2009-01-10 x 1 NA NA NA POLYGON ((-86.906 41.71133,... 2 NA NA NA POLYGON ((-86.74661 41.7113... 3 NA NA NA POLYGON ((-86.58721 41.7113... 4 NA NA NA POLYGON ((-86.42781 41.7113... 5 NA NA NA POLYGON ((-86.26842 41.7113... 6 NA NA NA POLYGON ((-86.10902 41.7113... 7 NA NA NA POLYGON ((-85.94962 41.7113... 8 NA NA NA POLYGON ((-85.79023 41.7113... 9 NA NA NA POLYGON ((-85.63083 41.7113... 10 NA NA NA POLYGON ((-85.47143 41.7113... Oops, so many NAs! This is clearly because many polygons intersect with cells that have NA values. To avoid this, we can add na.rm=TRUE option like this: ( mean_tmax &lt;- aggregate(tmax_MI, MI_polygons, FUN = mean, na.rm = TRUE) %&gt;% st_as_sf() ) Simple feature collection with 1064 features and 10 fields geometry type: POLYGON dimension: XY bbox: xmin: -90.41273 ymin: 41.71133 xmax: -82.44289 ymax: 47.48101 geographic CRS: NAD83 First 10 features: 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 1 26.65600 23.98400 23.89500 28.33900 26.07500 23.85000 24.39900 2 26.04367 24.34467 23.89733 28.45467 26.54600 24.28400 24.56967 3 25.73825 24.87400 23.97600 28.76300 27.26975 25.34700 25.27225 4 25.87100 25.17925 24.11325 29.06600 27.85700 25.98200 25.78500 5 25.70775 25.48625 24.11050 28.94025 27.95825 26.21000 25.90000 6 25.57850 25.64950 24.14375 28.74075 28.05550 26.18900 25.85250 7 25.78925 25.75600 24.19150 28.65325 28.15450 26.45800 26.12075 8 26.04167 25.81667 24.33033 28.58833 28.35567 26.91667 26.40033 9 26.04475 26.06450 24.21250 28.53550 28.27700 26.67875 26.60900 10 26.14425 26.46525 24.13775 28.64750 28.56825 26.36500 26.83575 2009-01-08 2009-01-09 2009-01-10 x 1 22.35300 30.46500 31.82900 POLYGON ((-86.906 41.71133,... 2 22.76133 30.39567 31.98000 POLYGON ((-86.74661 41.7113... 3 23.05250 30.66375 32.50500 POLYGON ((-86.58721 41.7113... 4 23.27725 30.97350 33.02000 POLYGON ((-86.42781 41.7113... 5 23.70050 30.78825 33.31025 POLYGON ((-86.26842 41.7113... 6 24.21475 30.35250 33.41150 POLYGON ((-86.10902 41.7113... 7 24.51575 30.16125 33.44150 POLYGON ((-85.94962 41.7113... 8 24.67367 30.01033 33.43500 POLYGON ((-85.79023 41.7113... 9 25.00725 30.10925 33.43200 POLYGON ((-85.63083 41.7113... 10 25.59000 30.30200 33.72700 POLYGON ((-85.47143 41.7113... Note that this would not work: mean_tmax &lt;- aggregate(tmax_MI, MI_polygons, FUN = mean(na.rm = TRUE)) %&gt;% st_as_sf() Error in mean.default(na.rm = TRUE): argument &quot;x&quot; is missing, with no default which is the first thing I tried before I googled the problem. Right now, the aggregate() function is slower for large raster data than exact_extract(), which we saw in Chapter 5. If the extraction speed of aggregate() is not satisfactory for your applications, you could just convert the stars object to RasterBrick using the as() method (section 7.8) and then use exact_extract() as follows: extracted_tmax_ee &lt;- tmax_MI %&gt;% as(&quot;Raster&quot;) %&gt;% exact_extract(., MI_polygons, progress = F) The good news is, exact_extract() is coming soon to the aggregate() methods for stars (see here), which means that you do not have to do these tedious conversions. Actually, if you are willing to install the version in development, you can add exact = TRUE option to take advantage of the speed of exact_extract(). "],
["create-maps.html", "Chapter 8 Creating Maps using ggplot2 ", " Chapter 8 Creating Maps using ggplot2 "],
["before-you-start-7.html", "Before you start", " Before you start In previous chapters, we have shown how to create very simple maps very quickly from vector and raster data. This section focuses on using the ggplot2 package to create high-quality maps that are publishable in journal articles, conference presentations, and any kind of professional reports. This requires fine-tuning the aesthetics of maps beyond default maps, such as using the appropriate color scheme, removing unnecessary information, formatting legends, etc. This chapter focuses on creating static maps, and does not cover interactive maps that you often see on the web. Creating maps differs from creating non-spatial figures in some ways. However, the underlying principle and syntax under ggplot2 to create maps and non-spatial figures are very similar. Indeed, you will find map making very intuitive and rather easy if you already have some knowledge of how ggplot2 works even if you have not creates maps using ggplot2. Indeed, the only major difference between them is the choice of geom_*() types. We have several geom_*() types at our disposal that are dedicated for spatial data visualization. geom_sf() for sf (vector) objects geom_raster() for raster data geom_stars() for stars (both vector and raster) object These geom_*()s allow for visualizing both vector and raster data through consistent and simple ggplot2 syntax. It provides direct supports to sf and stars objects, meaning that no transformation of those objects is necessary prior to creating maps. On the other hand, (a very simple) data transformation is necessary for Raster\\(^*\\) objects and SpatRaster or SpatVector by the terra package. We will look at each of the geoms individually to understand their basic usage below in sections 8.1 and 8.2. You will notice that there is nothing spatial about the sections following these sections. They are general and applicable to any kind of figures. Note : While this chapter does not assume much knowledge of ggplot2, the basic knowledge of ggplot2 is extremely helpful. If you do not know anything about ggplot2 or you are afraid that your knowledge of ggplot2 is insufficient, Appendix B provides minimal knowledge of data visualization using the ggplot2 package so you can at least understand what is happening in this Chapter. Useful resources As mentioned earlier, general knowledge of how ggplot2 works is very useful. So, any resources for learning ggplot2 are useful. Some of them are: ggplot2: Elegant Graphics for Data Analysis The following book provides numerous map making examples using ggplot2. It is a good place to further improve your map making skills after completing this chapter. Bob Rudis’ Code Examples Direction for replication Datasets All the datasets that you need to import are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps: set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory (if you have created a “Data” folder previously, skip this step) download the pertinent datasets from here place all the files in the downloaded folder in the “Data” folder Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( stars, # spatiotemporal data handling raster, # raster data handling terra, # raster data handling sf, # vector data handling dplyr, # data wrangling stringr, # string manipulation lubridate, # dates handling data.table, # data wrangling patchwork, # arranging figures tigris, # county border colorspace, # color scale viridis, # arranging figures tidyr, # reshape ggspatial, # north arrow and scale bar ggplot2 # make maps ) "],
["geom-sf.html", "8.1 Creating maps from sf objects", " 8.1 Creating maps from sf objects This section explains how to create maps from vector data stored as an sf object via geom_sf(). 8.1.1 Datasets The following datasets will be used for illustrations. Points af_used: total annual groundwater pumping at individual irrigatiton wells #--- read in the KS wells data ---# ( gw_KS_sf &lt;- readRDS( &quot;./Data/gw_KS_sf.rds&quot;) ) Simple feature collection with 56225 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191 geographic CRS: NAD83 First 10 features: well_id year af_used geometry 1 1 2010 67.00000 POINT (-100.4423 37.52046) 2 1 2011 171.00000 POINT (-100.4423 37.52046) 3 3 2010 30.93438 POINT (-100.7118 39.91526) 4 3 2011 12.00000 POINT (-100.7118 39.91526) 5 7 2010 0.00000 POINT (-101.8995 38.78077) 6 7 2011 0.00000 POINT (-101.8995 38.78077) 7 11 2010 154.00000 POINT (-101.7114 39.55035) 8 11 2011 160.00000 POINT (-101.7114 39.55035) 9 12 2010 28.17239 POINT (-95.97031 39.16121) 10 12 2011 89.53479 POINT (-95.97031 39.16121) Polygons ( KS_county &lt;- counties(state = &quot;Kansas&quot;, cb = TRUE) %&gt;% st_as_sf() %&gt;% st_transform(st_crs(gw_KS_sf)) ) Simple feature collection with 105 features and 9 fields geometry type: POLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316 geographic CRS: NAD83 First 10 features: STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 98 20 053 00484996 0500000US20053 20053 Ellsworth 06 1853331063 99 20 095 00485012 0500000US20095 20095 Kingman 06 2236100137 100 20 135 00485031 0500000US20135 20135 Ness 06 2783562240 101 20 199 00485060 0500000US20199 20199 Wallace 06 2366348284 675 20 069 00485001 0500000US20069 20069 Gray 06 2250356170 676 20 143 00485035 0500000US20143 20143 Ottawa 06 1866683176 677 20 181 00485053 0500000US20181 20181 Sherman 06 2735193251 895 20 033 00484986 0500000US20033 20033 Comanche 06 2041681114 896 20 149 00485038 0500000US20149 20149 Pottawatomie 06 2177623918 937 20 083 00485006 0500000US20083 20083 Hodgeman 06 2227369150 AWATER geometry 98 19969761 POLYGON ((-98.48554 38.7839... 99 8536908 POLYGON ((-98.46489 37.6453... 100 667491 POLYGON ((-100.2477 38.6544... 101 140936 POLYGON ((-102.0472 39.1331... 675 1113338 POLYGON ((-100.665 37.86809... 676 1567629 POLYGON ((-97.93161 39.2770... 677 548248 POLYGON ((-102.0498 39.5681... 895 3604155 POLYGON ((-99.54467 37.2682... 896 54032546 POLYGON ((-96.72833 39.4271... 937 826344 POLYGON ((-100.2271 38.2622... Lines ( KS_railroads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_crop(KS_county) ) Simple feature collection with 5804 features and 3 fields geometry type: GEOMETRY dimension: XY bbox: xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316 geographic CRS: NAD83 First 10 features: LINEARID FULLNAME MTFCC 2124 11051038759 Bnsf RR R1011 2136 11051038771 Bnsf RR R1011 2141 11051038776 Bnsf RR R1011 2186 11051047374 Rock Island RR R1011 2240 11051048071 Burlington Northern Santa Fe RR R1011 2252 11051048083 Burlington Northern Santa Fe RR R1011 2256 11051048088 Burlington Northern Santa Fe RR R1011 2271 11051048170 Chicago Burlington and Quincy RR R1011 2272 11051048171 Chicago Burlington and Quincy RR R1011 2293 11051048193 Chicago Burlington and Quincy RR R1011 geometry 2124 LINESTRING (-94.58841 39.15... 2136 LINESTRING (-94.59017 39.11... 2141 LINESTRING (-94.58841 39.15... 2186 LINESTRING (-94.58893 39.11... 2240 LINESTRING (-94.58841 39.15... 2252 LINESTRING (-94.59017 39.11... 2256 LINESTRING (-94.58841 39.15... 2271 LINESTRING (-94.58862 39.15... 2272 LINESTRING (-94.58883 39.11... 2293 LINESTRING (-94.58871 39.11... 8.1.2 Basic usage of geom_sf() geom_sf() allows for visualizing sf objects. Conveniently, geom_sf() automatically detects the geometry type of spatial objects stored in sf and draw maps accordingly. For example, the following codes create maps of Kansas wells (points), Kansas counties (polygons), and railroads in Kansas (lines): ( g_wells &lt;- ggplot(data = gw_KS_sf) + geom_sf() ) ( g_county &lt;- ggplot(data = KS_county) + geom_sf() ) ( g_rail &lt;- ggplot(data = KS_railroads) + geom_sf() ) As you can see, the different geometry types are handled by a single geom type, geom_sf(). Notice also that neither of the x-axis (longitude) and y-axis (latitude) is provided to geom_sf(). When you create a map, longitude and latitude are always used for x- and y-axis. geom_sf() is smart enough to know the geometry types and draw spatial objects accordingly. 8.1.3 Specifying the aesthetics There are various aesthetics options you can use. Available aesthetics vary by the type of geometry. This section shows the basics of how to specify the aesthetics of maps. Finer control of aesthetics will be discussed later. 8.1.3.1 Points color: color of the points fill: available for some shapes (but likely useless) shape: shape of the points size: size of the points (rarely useful) For illustration here, let’s focus on the wells in one county so it is easy to detect the differences across various aesthetics configurations. #--- wells in Stevens County ---# gw_Stevens &lt;- KS_county %&gt;% filter(NAME == &quot;Stevens&quot;) %&gt;% st_crop(gw_KS_sf, .) example 1 color: dependent on af_used (the amount of groundwater extraction) size: constant across the points (bigger than default) ( ggplot(data = gw_Stevens) + geom_sf(aes(color = af_used), size = 2) ) example 2 color: constant across the points (blue) size: dependent on af_used shape: constant across the points (square) ( ggplot(data = gw_Stevens) + geom_sf(aes(size = af_used), color = &quot;blue&quot;, shape = 15) ) example 3 color: dependent on whether located east of west of -101.3 in longitude shape: dependent on whether located east of west of -101.3 in longitude ( gw_Stevens %&gt;% cbind(., st_coordinates(.)) %&gt;% mutate(east_west = ifelse(X &lt; -101.3, &quot;west&quot;, &quot;east&quot;)) %&gt;% ggplot(data = .) + geom_sf(aes(shape = east_west, color = east_west)) ) 8.1.3.2 Polygons color: color of the borders of the polygons fill: color of the inside of the polygons shape: not available size: not available example 1 color: constant (red) fill: constant (dark green) ( ggplot(data = KS_county) + geom_sf(color = &quot;red&quot;, fill = &quot;darkgreen&quot;) ) example 2 color: default (black) fill: dependent on the total amount of pumping in 2010 KS_county_with_pumping &lt;- gw_KS_sf %&gt;% #--- only year == 2010 ---# filter(., year == 2010) %&gt;% #--- get total pumping by county ---# aggregate(., KS_county, sum, na.rm = TRUE) ( ggplot(data = KS_county_with_pumping) + geom_sf(aes(fill = af_used)) ) 8.1.4 Plotting multiple spatial objects in one figure You can combine all the layers created by geom_sf() additively so they appear in a single map: ggplot() + #--- this one uses KS_wells ---# geom_sf(data = gw_KS_sf, size = 0.4) + #--- this one uses KS_county ---# geom_sf(data = KS_county) + #--- this one uses KS_railroads ---# geom_sf(data = KS_railroads, color = &quot;red&quot;) Oops, you cannot see wells (points) in the figure. The order of geom_sf() matters. The layer added later will come on top of the preceding layers. That’s why wells are hidden beneath Kansas counties. So, let’s do this: ggplot(data = KS_county) + #--- this one uses KS_county ---# geom_sf() + #--- this one uses KS_county ---# geom_sf(data = gw_KS_sf, size = 0.4) + #--- this one uses KS_railroads ---# geom_sf(data = KS_railroads, color = &quot;red&quot;) Better. Note that since you are using different datasets for each layer, you need to specify the dataset to use in each layer except for the first geom_sf() which inherits data = KS_wells from ggplot(data = KS_wells). Of course, this will create exactly the same map: ( g_all &lt;- ggplot() + #--- this one uses KS_county ---# geom_sf(data = KS_county) + #--- this one uses KS_wells ---# geom_sf(data = gw_KS_sf, size = 0.4) + #--- this one uses KS_railroads ---# geom_sf(data = KS_railroads, color = &quot;red&quot;) ) There is no rule that you need to supply data to ggplot().94 Alternatively, you could add fill = NA to geom_sf(data = KS_county) instead of switching the order. ggplot() + #--- this one uses KS_wells ---# geom_sf(data = gw_KS_sf, size = 0.4) + #--- this one uses KS_county ---# geom_sf(data = KS_county, fill = NA) + #--- this one uses KS_railroads ---# geom_sf(data = KS_railroads, color = &quot;red&quot;) This is fine as long as you do not intend to color-code counties. 8.1.5 CRS ggplot() uses the CRS of the sf to draw a map. For example, right now the CRS of KS_county is this: st_crs(KS_county) Coordinate Reference System: User input: EPSG:4269 wkt: GEOGCS[&quot;NAD83&quot;, DATUM[&quot;North_American_Datum_1983&quot;, SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101, AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]] Let’s convert the CRS to WGS 84/ UTM zone 14N (EPSF code: 32614), make a map, and compare the ones with different CRS side by side. g_32614 &lt;- st_transform(KS_county, 32614) %&gt;% ggplot(data = .) + geom_sf() g_county/g_32614 Alternatively, you could use coord_sf() to alter the CRS on the map, but not the CRS of the sf object itself. ggplot() + #--- epsg: 4269 ---# geom_sf(data = KS_county) + coord_sf(crs = 32614) When multiple layers are used for map creation, the CRS of the first layer is applied for all the layers. ggplot() + #--- epsg: 32614 ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- epsg: 4269 ---# geom_sf(data = KS_railroads) coord_sf() applies to all the layers. ggplot() + #--- epsg: 32614 ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- epsg: 4269 ---# geom_sf(data = KS_railroads) + #--- using 4269 ---# coord_sf(crs = 4269) Finally, you could limit the geographic scope of the map to be created by adding xlim() and ylim(). ggplot() + #--- epsg: 32614 ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- epsg: 4269 ---# geom_sf(data = KS_railroads) + #--- using 4269 ---# coord_sf(crs = 4269) + #--- limit the geographic scope of the map ---# xlim(-99, -97) + ylim(37, 39) 8.1.6 Faceting Faceting splits the data into groups and generates a figure for each group, where the aesthetics of the figures are consistent across the groups. Faceting can be done using facet_wrap() or facet_grid(). Let’s try to create a map of groundwater use at wells by year where the points are color differentiated by the amount of groundwater use (af_used). ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap(. ~ year) Note that the above code creates a single legend that applies to both panels, which allows you to compare values across panels (years here). Further, also note that the values of the faceting variable (year) are displayed in the gray strips above the maps. You can have panels stacked vertically by using the ncol option (or nrow also works) in facet_wrap(. ~ year) as follows: ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap(. ~ year, ncol = 1) Two-way faceting is possible by supplying a variable name (or expression) in place of . in facet_wrap(. ~ year). The code below uses an expression (af_used &gt; 200) in place of .. This divides the dataset by whether water use is greater than 200 or not and by year. ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap((af_used &gt; 200) ~ year) The values of the expression (TRUE or FALSE) appear in the gray strips, which is not informative. We will discuss in detail how to control texts in the strips section 8.5. If you feel like the panels are too close to each other, you could provide more space between them using panel.spacing (both vertically and horizontally), panel.spacing.x (horizontally), and panel.spacing.y (vertically) options in theme(). Suppose you would like to place more space between the upper and lower panels, then you use panel.spacing.y like this: ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap((af_used &gt; 200) ~ year) + #--- add more space between panels ---# theme(panel.spacing.y = unit(2, &quot;lines&quot;)) 8.1.7 Adding texts (labels) on a map You can add labels to a map using geom_sf_text() or geom_sf_label() and providing aes(label = x) inside it where x is the variable that contains labels to print on a map. ggplot() + #--- KS county boundary ---# geom_sf(data = KS_county) + geom_sf_text( data = KS_county, aes(label = NAME), size = 3, color = &quot;blue&quot; ) If you would like to have overlapping labels not printed, you can add check_overlap = TRUE. ggplot() + #--- KS county boundary ---# geom_sf(data = KS_county) + geom_sf_text( data = KS_county, aes(label = NAME), check_overlap = TRUE, size = 3, color = &quot;blue&quot; ) The nudge_x and nudge_y options let you shift the labels. ggplot() + #--- KS county boundary ---# geom_sf(data = KS_county) + geom_sf_text( data = KS_county, aes(label = NAME), check_overlap = TRUE, size = 3, color = &quot;blue&quot;, nudge_x = -0.1, nudge_y = 0.1 ) If you would like a fine control on a few objects, you can always work on them separately. Cheyenne &lt;- filter(KS_county, NAME == &quot;Cheyenne&quot;) KS_less_Cheyenne &lt;- filter(KS_county, NAME != &quot;Cheyenne&quot;) ggplot() + #--- KS county boundary ---# geom_sf(data = KS_county) + geom_sf_text( data = KS_less_Cheyenne, aes(label = NAME), check_overlap = TRUE, size = 3, color = &quot;blue&quot;, nudge_x = -0.1, nudge_y = 0.1 ) + geom_sf_text( data = Cheyenne, aes(label = NAME), size = 2.5, color = &quot;red&quot;, nudge_y = 0.2 ) You could also use annotate() to place texts on a map, which can be useful if you would like to place arbitrary texts that are not part of sf object. ggplot() + #--- KS county boundary ---# geom_sf(data = KS_county) + geom_sf_text( data = KS_less_Cheyenne, aes(label = NAME), check_overlap = TRUE, size = 3, color = &quot;blue&quot;, nudge_x = -0.1, nudge_y = 0.1 ) + #--- use annotate to add texts on the map ---# annotate( geom = &quot;text&quot;, x = -102, y = 39.8, size = 3, label = &quot;Cheyennes&quot;, color = &quot;red&quot; ) As you can see, you need to tell where the texts should be placed with x and y, provide the texts you want on the map to label. Supplying data in ggplot() can be convenient if you are creating multiple geom from the data because you do not need to tell what data to use in each of the geoms.↩︎ "],
["geom-raster.html", "8.2 Raster data visualization: geom_raster() and geom_stars()", " 8.2 Raster data visualization: geom_raster() and geom_stars() This section shows how to use geom_raster() and geom_stars() to create maps from raster datasets of two object classes: Raster\\(^*\\) objects from the raster package and stars objects from the stars package. geom_raster does not accept either Raster\\(^*\\) or stars as the input. Instead, geom_raster() accepts a data.frame with coordinates to create maps. So, it is a two-step procedure convert raster dataset into a data.frame with coordinates use geom_raster() to make a map geom_stars() from the stars package accepts a stars object, and no data transformation is necessary. We use the following objects that have the same information but come in different object classes for illustration in this section. Raster as stars tmax_Jan_09 &lt;- readRDS(&quot;./Data/tmax_Jan_09_stars.rds&quot;) Raster as RasterStack tmax_Jan_09_rs &lt;- stack(&quot;./Data/tmax_Jan_09.tif&quot;) 8.2.1 Visualize Raster\\(^*\\) with geom_raster() In order to create maps from the information stored in Raster\\(^*\\) objects, you first convert them to a regular data.frame using as.data.frame() as follows: #--- convert to data.frame ---# tmax_Jan_09_df &lt;- as.data.frame(as(tmax_Jan_09_rs, &quot;Raster&quot;), xy = TRUE) %&gt;% #--- remove cells with NA for any of the layers ---# na.omit() %&gt;% #--- change the variable names ---# setnames( paste0(&quot;tmax_Jan_09.&quot;, 1:5), seq(ymd(&quot;2009-01-01&quot;), ymd(&quot;2009-01-05&quot;), by = &quot;days&quot;) %&gt;% as.character() ) #--- take a look ---# head(tmax_Jan_09_df) x y 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 17578 -95.12500 49.41667 -12.863 -11.455 -17.516 -12.755 -26.446 18982 -95.16667 49.37500 -12.698 -11.441 -17.436 -12.672 -25.889 18983 -95.12500 49.37500 -12.921 -11.611 -17.589 -12.830 -26.544 18984 -95.08333 49.37500 -13.074 -11.827 -17.684 -12.913 -26.535 18985 -95.04167 49.37500 -13.591 -12.220 -17.920 -12.684 -25.780 18986 -95.00000 49.37500 -13.688 -12.177 -17.911 -12.548 -25.424 The xy = TRUE option adds the coordinates of the centroid of the raster cells, and na.omit() removes cells that are outside of the Kansas border and have NA values. Notice that each band comprises a column in the data.frame. Once the conversion is done, you can use geom_raster() to create a map. Unlike geom_sf(), you need to supply the variables names for the geographical coordinates (here x for longitude and y for latitude). Further, you also need to specify which variable to use for fill color differentiation. ( g_tmax_map &lt;- ggplot(data = tmax_Jan_09_df) + geom_raster(aes(x = x, y = y, fill = `2009-01-01`)) + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) ) You can add coord_equal() so that one degree in latitude and longitude are the same length on the map. Of course, one degree in longitude and one degree in latitude are not the same length for US. However, distortion is smaller compared to the default at least. g_tmax_map + coord_equal() We only visualized tmax data for one day (layer.1) out of five days of tmax records. To present all of them at the same time we can facet using facet_wrap(). However, before we do that we need to have the data in a long format like this: tmax_long_df &lt;- pivot_longer(tmax_Jan_09_df, c(-x, -y), names_to = &quot;date&quot;, values_to = &quot;tmax&quot;) #--- take a look ---# head(tmax_long_df) # A tibble: 6 x 4 x y date tmax &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; 1 -95.1 49.4 2009-01-01 -12.9 2 -95.1 49.4 2009-01-02 -11.5 3 -95.1 49.4 2009-01-03 -17.5 4 -95.1 49.4 2009-01-04 -12.8 5 -95.1 49.4 2009-01-05 -26.4 6 -95.2 49.4 2009-01-01 -12.7 Let’s now use facet_wrap() to create a series of tmax maps. ggplot() + geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) + facet_wrap(date ~ .) + coord_equal() + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) 8.2.2 Visualize stars with geom_raster() Similarly with Raster\\(^*\\) objects, we first need to convert a stars to a regular data.frame using as.data.frame() as follows: #--- converting to a data.frame ---# tmax_long_df &lt;- as.data.frame(tmax_Jan_09, xy = TRUE) %&gt;% na.omit() #--- take a look ---# head(tmax_Jan_09_df) x y 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 17578 -95.12500 49.41667 -12.863 -11.455 -17.516 -12.755 -26.446 18982 -95.16667 49.37500 -12.698 -11.441 -17.436 -12.672 -25.889 18983 -95.12500 49.37500 -12.921 -11.611 -17.589 -12.830 -26.544 18984 -95.08333 49.37500 -13.074 -11.827 -17.684 -12.913 -26.535 18985 -95.04167 49.37500 -13.591 -12.220 -17.920 -12.684 -25.780 18986 -95.00000 49.37500 -13.688 -12.177 -17.911 -12.548 -25.424 One key difference from the conversion of Raster\\(^*\\) objects is that the data.frame is already in the long format, which means that you can immediately make faceted figures like this: ggplot() + geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) + facet_wrap(date ~ .) + coord_equal() + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) 8.2.3 Visualize stars with geom_stars() We saw above that geom_raster() requires converting a stars object to a data.frame first before creating a map. geom_stars() from the stars package lets you use a stars object directly to easily create a map under the ggplot2 framework. geom_stars() works just like geom_sf(). All you need to do is supply a stars object to geom_stars() as data. ggplot() + geom_stars(data = tmax_Jan_09) + theme_void() The fill color of the raster cells are automatically set to the attribute (here tmax) as if aes(fill = tmax). It is a good idea to add coord_equal() because of the same issue we saw with geom_raster(). ggplot() + geom_stars(data = tmax_Jan_09) + theme_void() + coord_equal() By default, geom_stars() plots only the first band. In order to present all the layers at the same time, you can add facet_wrap( ~ x) where x is the name of the third dimension of the stars object (date here). ggplot() + geom_stars(data = tmax_Jan_09) + facet_wrap( ~ date) + coord_equal() + theme_void() 8.2.4 adding geom_sf() layers You can easily add geom_sf() layers to a map created with geom_raster() or geom_stars(). Let’s crop the tmax data to Kansas and create a map of tmax values displayed on top of the Kansas county borders. #--- crop to KS ---# KS_tmax_Jan_09_stars &lt;- tmax_Jan_09 %&gt;% st_crop(., KS_county) #--- convert to a df ---# KS_tmax_Jan_09_df &lt;- as.data.frame(KS_tmax_Jan_09_stars, xy = TRUE) %&gt;% na.omit() 8.2.4.1 adding geom_sf() to a map with geom_raster() ggplot() + geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) + geom_sf(data = KS_county, fill = NA) + facet_wrap(date ~ .) + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) Notice that coord_equal() is not necessary in the above code. Indeed, if you try to add coord_equal(), you will have an error. ggplot() + geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) + geom_sf(data = KS_county, fill = NA) + facet_wrap(date ~ .) + coord_equal() + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) Error: geom_sf() must be used with coord_sf() Further, the original stars or Raster\\(^*\\) must have the same CRS as the sf objects. The following code transforms the CRS of KS_county to 32614 and try to plot them together. ggplot() + geom_raster(data = KS_tmax_Jan_09_df, aes(x = x, y = y, fill = tmax)) + geom_sf(data = st_transform(KS_county, 32614), fill = NA) + facet_wrap(date ~ .) + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) When plotting multiple sf objects, the CRS for the map was set to the CRS of the sf objects used for the first geom_sf() layer, and the rest of the sf objects followed suit. That is not the case here. 8.2.4.2 adding geom_sf() to a map with geom_stars() This is basically the same as the case with geom_stars() ggplot() + geom_stars(data = KS_tmax_Jan_09_stars) + geom_sf(data = KS_county, fill = NA) + facet_wrap( ~ date) + theme_void() Just like the case with geom_raster(), coord_equal() is not necessary and the stars and sf objects must have the same CRS. ggplot() + geom_stars(data = KS_tmax_Jan_09_stars) + geom_sf(data = st_transform(KS_county, 32614), fill = NA) + facet_wrap( ~ date) + theme_void() "],
["color-scale.html", "8.3 Color scale", " 8.3 Color scale Color scale refers to the way variable values are mapped to colors on a figure. For example, in the example below, the color scale for fill maps the value of af_used to a gradient of colors: dark blue for low values to light blue for high values of af_used. tmax_long_df &lt;- readRDS(&quot;./Data/tmax_Jan_09_stars.rds&quot;) %&gt;% as.data.frame(xy = TRUE) %&gt;% na.omit() g_col_scale &lt;- ggplot() + geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) + facet_wrap(date ~ .) + coord_equal() + theme_void() + theme( legend.position = &quot;bottom&quot; ) Often times, it is aesthetically desirable to change the default color scale of ggplot2. For example, if you would like to color-differentiate temperature values, you might want to start from blue for low values to red for high values. You can control the color scale using scale_*() functions. Which scale_*() function to use depends on the type of aesthetics (fill or color) and whether the aesthetic variable is continuous or discrete. Providing a wrong kind of scale_*() function results in an error. 8.3.1 Viridis color maps The ggplot2 packages offers scale_A_viridis_B() functions for viridis color map, where A is the type of aesthetics attribute (fill, color), and B is the type of variable. For example, scale_fill_viridis_c() can be used for fill aesthetics applied to a continuous variable. There are five types of palettes available under the viridis color map and can be selected using the option = option. Here is a visualization of all the five palettes. data(&quot;geyser&quot;, package=&quot;MASS&quot;) ggplot(geyser, aes(x = duration, y = waiting)) + xlim(0.5, 6) + ylim(40, 110) + stat_density2d(aes(fill = ..level..), geom=&quot;polygon&quot;) + theme_bw() + theme(panel.grid=element_blank()) -&gt; gg (gg + scale_fill_viridis_c(option=&quot;A&quot;) + labs(x=&quot;magma&quot;, y=NULL))/ (gg + scale_fill_viridis_c(option=&quot;B&quot;) + labs(x=&quot;inferno&quot;, y=NULL))/ (gg + scale_fill_viridis_c(option=&quot;C&quot;) + labs(x=&quot;plasma&quot;, y=NULL))/ (gg + scale_fill_viridis_c(option=&quot;D&quot;) + labs(x=&quot;viridis&quot;, y=NULL))/ (gg + scale_fill_viridis_c(option=&quot;E&quot;) + labs(x=&quot;cividis&quot;, y=NULL)) Let’s see what the PRISM tmax maps look like using Option A and D (default). Since the aesthetics type is fill and tmax is continuous, scale_fill_viridis_c() is the appropriate one here. g_col_scale + scale_fill_viridis_c(option = &quot;A&quot;) g_col_scale + scale_fill_viridis_c(option = &quot;D&quot;) You can reverse the order of the color by adding direction = -1. g_col_scale + scale_fill_viridis_c(option = &quot;D&quot;, direction = -1) Let’s now work on aesthetics mapping based on a discrete variable. The code below groups af_used into five groups of ranges. #--- convert af_used to a discrete variable ---# gw_Stevens &lt;- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5)) #--- take a look ---# head(gw_Stevens) Simple feature collection with 6 features and 4 fields geometry type: POINT dimension: XY bbox: xmin: -101.4232 ymin: 37.04683 xmax: -101.1111 ymax: 37.29295 geographic CRS: NAD83 well_id year af_used geometry af_used_cat 1 234 2010 461.9565 POINT (-101.4232 37.1165) (377,508] 2 234 2011 486.0534 POINT (-101.4232 37.1165) (377,508] 3 290 2010 457.4391 POINT (-101.2301 37.29295) (377,508] 4 290 2011 580.4156 POINT (-101.2301 37.29295) (508,1.19e+03] 5 317 2010 258.0000 POINT (-101.1111 37.04683) (157,264] 6 317 2011 255.0000 POINT (-101.1111 37.04683) (157,264] Since we would like to control color aesthetics based on a discrete variable, we should be using scale_color_viridis_d(). ggplot(data = gw_Stevens) + geom_sf(aes(color = af_used_cat), size = 2) + scale_color_viridis_d(option = &quot;C&quot;) 8.3.2 RColorBrewer: scale_*_distiller() and scale_*_brewer() The RColorBrewer package provides a set of color scales that are useful. Here is the list of color scales you can use. #--- load RColorBrewer ---# library(RColorBrewer) #--- disply all the color schemes from the package ---# display.brewer.all() The first set of color palettes are sequential palettes and are suitable for a variable that has ordinal meaning: temperature, precipitation, etc. The second set of palettes are qualitative palettes and suitable for qualitative or categorical data. Finally, the third set of palettes are diverging palettes and can be suitable for variables that take both negative and positive values like changes in groundwater level. Two types of scale functions are offered by the packages: scale_*_distiller() for a continuous variable scale_*_brewer() for a discrete variable To use a specific color palette, you can simply add palette = \"palette name\" inside scale_fill_distiller(). The codes below applies “Spectral” as an example. g_col_scale + theme_void() + scale_fill_distiller(palette = &quot;Spectral&quot;) You can reverse the color order by adding trans = \"reverse\" option. g_col_scale + theme_void() + scale_fill_distiller(palette = &quot;Spectral&quot;, trans = &quot;reverse&quot;) If you are specifying the color aesthetics based on a continuous variable, then you use scale_color_distiller(). ggplot(data = gw_Stevens) + geom_sf(aes(color = af_used), size = 2) + scale_color_distiller(palette = &quot;Spectral&quot;) Now, suppose the variable of interest comes with categories of ranges of values. The code below groups af_used into five ranges using ggplo2::cut_number(). gw_Stevens &lt;- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5)) Since af_used_cat is a discrete variable, you can use scale_color_brewer() instead. ggplot(data = gw_Stevens) + geom_sf(aes(color = af_used_cat), size = 2) + scale_color_brewer(palette = &quot;Spectral&quot;) 8.3.3 colorspace package If you are not satisfied with the viridis color map or the ColorBrewer palette options, you might want to try the colorspace package. Here is the palettes the colorspace package offers. #--- plot the palettes ---# hcl_palettes(plot = TRUE) The packages offers its own scale_*() functions that follows the following naming convention: scale_aesthetic_datatype_colorscale where aesthetic: fill or color datatype: continuous or discrete colorscale: qualitative, sequential, diverging, divergingx For example, to add a sequential color scale to the following map, we would use scale_fill_continuous_sequential() and then pick a palette from the set of sequential palettes shown above. The code below uses the Viridis palette with the reverse option: ggplot() + geom_sf(data = gw_by_county, aes(fill = af_used)) + facet_wrap(. ~ year) + scale_fill_continuous_sequential(palette = &quot;Viridis&quot;, trans = &quot;reverse&quot;) If you still cannot find a palette that satisfies your need (or obsession at this point), then you can easily make your own. The package offers hclwizard(), which starts shiny-based web application to let you design your own color palette. After running this, hclwizard() you should see a web application pop up that looks like this. After you find a color scale you would like to use, you can go to the Exporttab, select the R tab, and then copy the code that appear in the highlighted area. You could register the color palette by completing the register = option in the copied code if you think you will use it other times. Otherwise, you can delete the option. col_palette &lt;- sequential_hcl(n = 7, h = c(36, 200), c = c(60, NA, 0), l = c(25, 95), power = c(0.7, 1.3)) We then use the code as follows: g_col_scale + theme_void() + scale_fill_gradientn(colors = col_palette) Note that you are now using scale_*_gradientn() with this approach. For a discrete variable, you can use scale_*_manual(): col_discrete &lt;- sequential_hcl(n = 5, h = c(240, 130), c = c(30, NA, 33), l = c(25, 95), power = c(1, NA), rev = TRUE) ggplot() + geom_sf(data = gw_Stevens, aes(color = af_used_cat), size = 2) + scale_color_manual(values = col_discrete) "],
["arranging-maps.html", "8.4 Arranging maps", " 8.4 Arranging maps 8.4.1 Multiple panels of figures as a single figure Faceting using facet_wrap() or facet_grid() allows for dividing the data into groups and creating a map for each group. It is particularly suitable for visualizing one variable at different facets. A good example is a collection of maps of tmax observed at different dates (see figure below). Faceting provides a single consistent color scale shared across the facets. ggplot() + geom_raster(data = tmax_long_df, aes(x = x, y = y, fill = tmax)) + facet_wrap(date ~ .) + coord_equal() + scale_fill_viridis_c() + theme_void() + theme( legend.position = &quot;bottom&quot; ) However, faceting is not suitable for creating maps of different variables. To see this let’s plot tmax and precipitation on Jan 1, 2009 together. #--- import precip data ---# ppt_long_df &lt;- read_stars(&quot;./Data/PRISM/PRISM_ppt_y2009_m1.tif&quot;) %&gt;% #--- change the variable name ---# setNames(&quot;ppt&quot;) %&gt;% #--- first five days of January ---# filter(band &lt;= 5) %&gt;% #--- set the date values ---# st_set_dimensions(&quot;band&quot;, values = seq(ymd(&quot;2009-01-01&quot;), ymd(&quot;2009-01-05&quot;), by = &quot;days&quot;), names = &quot;date&quot; ) %&gt;% as.data.frame(xy = TRUE) %&gt;% na.omit() Let’s extract tmax and precipitation on Jan 1, 2009 from their respective datasets, and combine them. #--- get tmax on Jan 1, 2009 ---# tmax_df &lt;- filter(tmax_long_df, date == ymd(&quot;2009-01-01&quot;)) %&gt;% setnames(&quot;tmax&quot;, &quot;value&quot;) %&gt;% mutate(type = &quot;tmax&quot;) #--- get precipitation on Jan 1, 2009 ---# ppt_df &lt;- filter(ppt_long_df, date == ymd(&quot;2009-01-01&quot;)) %&gt;% setnames(&quot;ppt&quot;, &quot;value&quot;) %&gt;% mutate(type = &quot;ppt&quot;) #--- combine them ---# combined_df &lt;- rbind(tmax_df, ppt_df) Here is the map faceted for tmax and precipitation: ggplot() + geom_raster(data = combined_df, aes(x = x, y = y, fill = value)) + facet_grid(type ~ .) + scale_fill_viridis() + theme_void() As you can see, a single color scale is created for precipitation recorded in mm and temperature observed in Celsius. On this particular day, precipitation of more than 150 mm was observed on a part of the west coast. Consequently, you see almost no color differentiation on the tmax map which ranges roughly from -20 to 30. It is simply not a good idea to facet for two variables observed at different scales. Instead, you should have an independent color scale for each of the variables and then just combine them. Now, you might ask if you really need to combine the two. Can’t you just have two figures and arrange them in the manner you would like on a pdf or WORD document? If you are still convinced that you need to have two panels of figures as one figure, then you can use the patchwork package. library(patchwork) patchwork combines ggplot objects (maps) using simple operators: +, /, and |. Let’s first create maps of tmax and precipitation separately. #--- tmax ---# ( g_tmax &lt;- ggplot() + geom_raster(data = tmax_df, aes(x = x, y = y, fill = value)) + scale_fill_viridis() + theme_void() + coord_equal() + theme(legend.position = &quot;bottom&quot;) ) #--- ppt ---# ( g_ppt &lt;- ggplot() + geom_raster(data = ppt_df, aes(x = x, y = y, fill = value)) + scale_fill_viridis() + theme_void() + coord_equal() + theme(legend.position = &quot;bottom&quot;) ) It is best to just look at examples to get the sense of how patchwork works. A fuller treatment of patchwork is found at its packagedown website (https://patchwork.data-imaginist.com/index.html). Example 1 g_tmax + g_ppt Example 2 g_tmax / g_ppt / g_tmax Example 3 g_tmax + g_ppt + plot_layout(nrow = 2) Example 4 g_tmax + g_ppt + g_tmax + g_ppt + plot_layout(nrow = 3, byrow = FALSE) Example 5 g_tmax | (g_ppt/g_tmax) Sometimes figures are placed too close to each other. In such a case, you can pad a figure at the time of generating individual figures by adding the plot.margin option to theme(). For example, the following code creates space at the bottom of g_tmax (5 cm), and vertically stack g_tmax and g_ppt. #--- space at the bottom ---# g_tmax &lt;- g_tmax + theme(plot.margin = unit(c(0, 0, 5, 0), &quot;cm&quot;)) #--- vertically stack ---# g_tmax/g_ppt In plot.margin = unit(c(a, b, c, d), \"cm\"), here is which margin a, b, c, and d refers to. a: top b: right c: bottom d: left 8.4.2 A map in a map: inset Sometimes, it is useful to present a map that covers a larger geographical range than the area of interest in the same map. This provides a better sense of geographic extent and the location of the area of interest relative to the larger geographic extent that the readers are more familiar with. For example, suppose your work is restricted to three counties in Kansas: Cheyenne, Sherman, and Wallace. Here is the map of three counties: three_counties &lt;- filter(KS_county, NAME %in% c(&quot;Cheyenne&quot;, &quot;Sherman&quot;, &quot;Wallace&quot;)) ( g_three_counties &lt;- ggplot() + geom_sf(data = three_counties) + geom_sf_text(data = three_counties, aes(label = NAME)) + theme_void() ) Well, for those who are not familiar with Kansas, it might be useful to show where in Kansas they are located on the same map (or even where Kansas is in the U.S.). This can be achieved using ggplotGrob() and annotation_custom(). The steps are the following: create a map of the area of interest, turn it into a grob using ggplotGrob() create a map of the region that includes the area of interest combine the two using annotation_custom() #--- convert the ggplot into a grob ---# grob_aoi &lt;- ggplotGrob(g_three_counties) #--- check the class ---# class(grob_aoi) [1] &quot;gtable&quot; &quot;gTree&quot; &quot;grob&quot; &quot;gDesc&quot; #--- create a map of Kansas ---# g_region &lt;- ggplot() + geom_sf(data = KS_county) + geom_sf(data = three_counties, fill = &quot;blue&quot;, color = &quot;red&quot;, alpha = 0.5) + theme_void() #--- convert to a grob ---# grob_region &lt;- ggplotGrob(g_region) Now that we have two maps, we can now put them on the same map using annotation_custom(). The first task is to initiate a ggplot with coord_equal() as follows: ( g_inset &lt;- ggplot() + coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE) ) You now have a blank canvas to put the images on. Let’s add a layer with annotation_custom() in which you provide the grob object (a map) and specify the range of the canvas the map occupies. Since the extent of x and y are set to [0, 1] above with coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = FALSE), the following code put the grob_aoi to cover the entire y range and up to 0.8 of x from 0. g_inset + annotation_custom(grob_aoi, xmin = 0, xmax = 0.8, ymin = 0, ymax = 1) Similarly, we can add grob_region using annotation_custom(). Let’s put it at the right lower corner of the map. g_inset + annotation_custom(grob_aoi, xmin = 0, xmax = 0.8, ymin = 0, ymax = 1) + annotation_custom(grob_region, xmin = 0.6, xmax = 1, ymin = 0, ymax = 0.3) Note that the resulting map still has the default theme because it does not inherit the theme of maps added by annotation_custom(). So, you can add theme_void() to the map to make the border disappear. g_inset + annotation_custom(grob_aoi, xmin = 0, xmax = 0.8, ymin = 0, ymax = 1) + annotation_custom(grob_region, xmin = 0.6, xmax = 1, ymin = 0, ymax = 0.3) + theme_void() "],
["fine-tune.html", "8.5 Fine-tuning maps for publication", " 8.5 Fine-tuning maps for publication This section presents a number of small tips to beautify your maps so that they can be publishable in professional reports. Often times, academic journals have their own particular sets of rules about figures, and reviewers or your boss might have their own views of what maps should look like. Whatever the requirements or requests, you need to accommodate their requests and modify the maps accordingly. It is worth mentioning that there is really nothing specific to creating maps here. Techniques presented here are applicable to any kind of figure. It is just that we limit ourselves to specific components of a figure that you are likely to want to modify from the default when creating maps. Specifically, pre-made ggplot2 themes and how to modify legends and facet strips are discussed. ( gw_by_county &lt;- st_join(KS_county, gw_KS_sf) %&gt;% data.table() %&gt;% .[, .(af_used = sum(af_used, na.rm = TRUE)), by = .(COUNTYFP, year)] %&gt;% left_join(KS_county, ., by = c(&quot;COUNTYFP&quot;)) %&gt;% filter(!is.na(year)) ) Simple feature collection with 184 features and 11 fields geometry type: POLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99302 xmax: -94.58841 ymax: 40.00316 geographic CRS: NAD83 First 10 features: STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 20 053 00484996 0500000US20053 20053 Ellsworth 06 1853331063 2 20 053 00484996 0500000US20053 20053 Ellsworth 06 1853331063 3 20 095 00485012 0500000US20095 20095 Kingman 06 2236100137 4 20 095 00485012 0500000US20095 20095 Kingman 06 2236100137 5 20 135 00485031 0500000US20135 20135 Ness 06 2783562240 6 20 135 00485031 0500000US20135 20135 Ness 06 2783562240 7 20 199 00485060 0500000US20199 20199 Wallace 06 2366348284 8 20 199 00485060 0500000US20199 20199 Wallace 06 2366348284 9 20 069 00485001 0500000US20069 20069 Gray 06 2250356170 10 20 069 00485001 0500000US20069 20069 Gray 06 2250356170 AWATER year af_used geometry 1 19969761 2010 93.0000 POLYGON ((-98.48554 38.7839... 2 19969761 2011 216.2429 POLYGON ((-98.48554 38.7839... 3 8536908 2010 17276.8266 POLYGON ((-98.46489 37.6453... 4 8536908 2011 23494.8605 POLYGON ((-98.46489 37.6453... 5 667491 2010 3435.3507 POLYGON ((-100.2477 38.6544... 6 667491 2011 5922.7644 POLYGON ((-100.2477 38.6544... 7 140936 2010 49764.0469 POLYGON ((-102.0472 39.1331... 8 140936 2011 59961.6326 POLYGON ((-102.0472 39.1331... 9 1113338 2010 206227.2389 POLYGON ((-100.665 37.86809... 10 1113338 2011 255934.0108 POLYGON ((-100.665 37.86809... ( g_base &lt;- ggplot() + geom_sf(data = gw_by_county, aes(fill = af_used)) + facet_wrap(. ~ year) ) 8.5.1 Setting the theme Right now, the map shows geographic coordinates, gray background, and grid lines. They are not very aesthetically appealing. Adding the pre-defined theme by theme_*() can alter the theme of a map very quickly. One of the themes suitable for maps is theme_void(). g_base + theme_void() As you can see, all the axes information (axis.title, axis.ticks, axis.text,) and panel information (panel.background, panel.border, panel.grid) are gone among other parts of the figure. You can confirm this by evaluating theme_void(). theme_void() List of 92 $ line : list() ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; $ rect : list() ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; $ text :List of 11 ..$ family : chr &quot;&quot; ..$ face : chr &quot;plain&quot; ..$ colour : chr &quot;black&quot; ..$ size : num 11 ..$ hjust : num 0.5 ..$ vjust : num 0.5 ..$ angle : num 0 ..$ lineheight : num 0.9 ..$ margin : &#39;margin&#39; num [1:4] 0points 0points 0points 0points .. ..- attr(*, &quot;unit&quot;)= int 8 ..$ debug : logi FALSE ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ title : NULL $ aspect.ratio : NULL $ axis.title : list() ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; $ axis.title.x : NULL $ axis.title.x.top : NULL $ axis.title.x.bottom : NULL $ axis.title.y : NULL $ axis.title.y.left : NULL $ axis.title.y.right : NULL $ axis.text : list() ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_blank&quot; &quot;element&quot; $ axis.text.x : NULL $ axis.text.x.top : NULL $ axis.text.x.bottom : NULL $ axis.text.y : NULL $ axis.text.y.left : NULL $ axis.text.y.right : NULL $ axis.ticks : NULL $ axis.ticks.x : NULL $ axis.ticks.x.top : NULL $ axis.ticks.x.bottom : NULL $ axis.ticks.y : NULL $ axis.ticks.y.left : NULL $ axis.ticks.y.right : NULL $ axis.ticks.length : &#39;simpleUnit&#39; num 0points ..- attr(*, &quot;unit&quot;)= int 8 $ axis.ticks.length.x : NULL $ axis.ticks.length.x.top : NULL $ axis.ticks.length.x.bottom: NULL $ axis.ticks.length.y : NULL $ axis.ticks.length.y.left : NULL $ axis.ticks.length.y.right : NULL $ axis.line : NULL $ axis.line.x : NULL $ axis.line.x.top : NULL $ axis.line.x.bottom : NULL $ axis.line.y : NULL $ axis.line.y.left : NULL $ axis.line.y.right : NULL $ legend.background : NULL $ legend.margin : NULL $ legend.spacing : NULL $ legend.spacing.x : NULL $ legend.spacing.y : NULL $ legend.key : NULL $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ..- attr(*, &quot;unit&quot;)= int 3 $ legend.key.height : NULL $ legend.key.width : NULL $ legend.text :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : &#39;rel&#39; num 0.8 ..$ hjust : NULL ..$ vjust : NULL ..$ angle : NULL ..$ lineheight : NULL ..$ margin : NULL ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ legend.text.align : NULL $ legend.title :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : NULL ..$ hjust : num 0 ..$ vjust : NULL ..$ angle : NULL ..$ lineheight : NULL ..$ margin : NULL ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ legend.title.align : NULL $ legend.position : chr &quot;right&quot; $ legend.direction : NULL $ legend.justification : NULL $ legend.box : NULL $ legend.box.just : NULL $ legend.box.margin : NULL $ legend.box.background : NULL $ legend.box.spacing : NULL $ panel.background : NULL $ panel.border : NULL $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ..- attr(*, &quot;unit&quot;)= int 8 $ panel.spacing.x : NULL $ panel.spacing.y : NULL $ panel.grid : NULL $ panel.grid.major : NULL $ panel.grid.minor : NULL $ panel.grid.major.x : NULL $ panel.grid.major.y : NULL $ panel.grid.minor.x : NULL $ panel.grid.minor.y : NULL $ panel.ontop : logi FALSE $ plot.background : NULL $ plot.title :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : &#39;rel&#39; num 1.2 ..$ hjust : num 0 ..$ vjust : num 1 ..$ angle : NULL ..$ lineheight : NULL ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points .. ..- attr(*, &quot;unit&quot;)= int 8 ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ plot.title.position : chr &quot;panel&quot; $ plot.subtitle :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : NULL ..$ hjust : num 0 ..$ vjust : num 1 ..$ angle : NULL ..$ lineheight : NULL ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points .. ..- attr(*, &quot;unit&quot;)= int 8 ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ plot.caption :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : &#39;rel&#39; num 0.8 ..$ hjust : num 1 ..$ vjust : num 1 ..$ angle : NULL ..$ lineheight : NULL ..$ margin : &#39;margin&#39; num [1:4] 5.5points 0points 0points 0points .. ..- attr(*, &quot;unit&quot;)= int 8 ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ plot.caption.position : chr &quot;panel&quot; $ plot.tag :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : &#39;rel&#39; num 1.2 ..$ hjust : num 0.5 ..$ vjust : num 0.5 ..$ angle : NULL ..$ lineheight : NULL ..$ margin : NULL ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ plot.tag.position : chr &quot;topleft&quot; $ plot.margin : &#39;simpleUnit&#39; num [1:4] 0lines 0lines 0lines 0lines ..- attr(*, &quot;unit&quot;)= int 3 $ strip.background : NULL $ strip.background.x : NULL $ strip.background.y : NULL $ strip.placement : NULL $ strip.text :List of 11 ..$ family : NULL ..$ face : NULL ..$ colour : NULL ..$ size : &#39;rel&#39; num 0.8 ..$ hjust : NULL ..$ vjust : NULL ..$ angle : NULL ..$ lineheight : NULL ..$ margin : NULL ..$ debug : NULL ..$ inherit.blank: logi TRUE ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; $ strip.text.x : NULL $ strip.text.y : NULL $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ..- attr(*, &quot;unit&quot;)= int 8 $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ..- attr(*, &quot;unit&quot;)= int 8 - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; - attr(*, &quot;complete&quot;)= logi TRUE - attr(*, &quot;validate&quot;)= logi TRUE Applying the theme to a map obviates the need to suppress parts of a figure individually. You can suppress parts of the figure individually using theme(). For example, the following code gets rid of axis.text. g_base + theme(axis.text = element_blank()) So, if theme_void() is overdoing things, you can build your own theme specifically for maps. For example, this is the theme I used for maps in Chapter 1. theme_for_map &lt;- theme( axis.ticks = element_blank(), axis.text= element_blank(), axis.line = element_blank(), panel.border = element_blank(), panel.grid = element_line(color=&#39;transparent&#39;), panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,color=&#39;transparent&#39;) ) Applying the theme to the map: g_base + theme_for_map This is very similar to theme_void() except that strip.background is not lost. You can use theme_void() as a starting point and override components of it like this. #--- bring back a color to strip.background ---# theme_for_map_2 &lt;- theme_void() + theme(strip.background = element_rect(fill = &quot;gray&quot;)) #--- apply the new theme ---# g_base + theme_for_map_2 theme_bw() is also a good theme for maps. ggplot() + geom_sf(data = gw_by_county, aes(fill = af_used)) + facet_grid(year ~ .) + theme_bw() If you do not like the gray grid lines, you can remove them like this. ggplot() + geom_sf(data = gw_by_county, aes(fill = af_used)) + facet_grid(year ~ .) + theme_bw() + theme( panel.grid = element_line(color=&#39;transparent&#39;) ) Not all themes are suitable for maps. For example, theme_classic() is not a very good option as you can see below: g_base + theme_classic() If you are not satisfied with theme_void() and not willing to make up your own theme, then you may want to take a look at other pre-made themes that are available from ggplot2 (see here) and ggthemes (see here). Note that some themes are more invasive than theme_void(), altering the default color scale. 8.5.2 Legend Legends can be modified using legend.*() options for theme() and guide_*(). It is impossible to discuss every single one of all the options for these functions. So, this section focuses on the most common and useful (that I consider) modifications you can make to legends. A legend consists of three elements: legend title, legend key (e.g., color bar), and legend label (or legend text). For example, in the figure below, af_used is the legend title, the color bar is the legend key, and the numbers below the color bar are legend labels. Knowing the name of these elements helps because the name of the options contains the name of the specific part of the legend. ( g_legend &lt;- ggplot() + geom_sf(data = gw_by_county, aes(fill = af_used)) + facet_wrap(. ~ year) + theme_void() ) Let’s first change the color scale to Viridis using scale_fill_viridis_c() (see section 8.3 for picking a color scale). g_legend + scale_fill_viridis_c() Right now, the legend title is af_used, which does not tell the readers what it means. In general, you can change the title of a legend by using the name option inside the scale function for the legend (here, scale_fill_viridis_c()). So, this one works: g_legend + scale_fill_viridis_c(name = &quot;Groundwater pumping (acre-feet)&quot;) Alternatively, you can use labs() function.95. Since the legend is for the fill aesthetic attribute, you should add fill = \"legend title\" as follows: g_legend + scale_fill_viridis_c() + labs(fill = &quot;Groundwater pumping (acre-feet)&quot;) Since the legend title is long, the legend is taking up about the half of the space of the entire figure. So, let’s put the legend below the maps (bottom of the figure) by adding theme(legend.position = \"bottom\"). g_legend + scale_fill_viridis_c() + labs(fill = &quot;Groundwater pumping (acre-feet)&quot;) + theme(legend.position = &quot;bottom&quot;) It would be aesthetically better to have the legend title on top of the color bar. This can be done by using the guides() function. Since we would like to alter the aesthetics of the legend for fill involving a color bar, we use fill = guide_colorbar(). To place the legend title on top, you add title.position=\"top\" inside the guide_colorbar() function as follows: g_legend + scale_fill_viridis_c() + labs(fill = &quot;Groundwater pumping (acre-feet)&quot;) + theme(legend.position = &quot;bottom&quot;) + guides(fill = guide_colorbar(title.position=&quot;top&quot;)) This looks better. But, the legend labels are too close to each other so it is hard to read them because the color bar is too short. Let’s elongate the color bar so that we have enough space between legend labels using legend.key.width = option for theme(). Let’s also make the legend thinner using legend.key.height = option. g_legend + scale_fill_viridis_c() + labs(fill = &quot;Groundwater pumping (acre-feet)&quot;) + theme( legend.position = &quot;bottom&quot;, #--- NEW LINES HERE!! ---# legend.key.height = unit(0.5, &quot;cm&quot;), legend.key.width = unit(2, &quot;cm&quot;) ) + guides(fill = guide_colorbar(title.position=&quot;top&quot;)) If the journal you are submitting an article to is requesting a specific font family for the texts in the figure, you can use legend.text = element_text() and legend.title = element_text() inside theme() for the legend labels and legend title, respectively. The following code uses the font family of “Times” and font size of 12 for both the labels and the title. g_legend + scale_fill_viridis_c() + labs(fill = &quot;Groundwater pumping (acre-feet)&quot;) + theme( legend.position = &quot;bottom&quot;, legend.key.height = unit(0.5, &quot;cm&quot;), legend.key.width = unit(2, &quot;cm&quot;), legend.text = element_text(size = 12, family = &quot;Times&quot;), legend.title = element_text(size = 12, family = &quot;Times&quot;) #--- NEW LINES HERE!! ---# ) + guides(fill = guide_colorbar(title.position=&quot;top&quot;)) For the other options to control the legend with a color bar, see here. When the legend is made for discrete values, you can use guide_legend(). Let’s use the following map as a starting point. #--- convert af_used to a discrete variable ---# gw_Stevens &lt;- mutate(gw_Stevens, af_used_cat = cut_number(af_used, n = 5)) ( g_legend_2 &lt;- ggplot(data = gw_Stevens) + geom_sf(aes(color = af_used_cat), size = 2) + scale_color_viridis(discrete = TRUE, option = &quot;C&quot;) + labs(color = &quot;Groundwater pumping (acre-feet)&quot;) + theme_void() + theme(legend.position = &quot;bottom&quot;) ) The legend is too long, so first put the legend title on top of the legend labels using the code below: g_legend_2 + guides( color = guide_legend(title.position=&quot;top&quot;) ) Since the legend is for the color aesthetic attribute, color = guide_legend() was used. The legend labels are still a bit too long, so let’s arrange them in two rows using the nrow = option. g_legend_2 + guides( color = guide_legend(title.position=&quot;top&quot;, nrow = 2) ) For the other options for guide_legend(), see here. 8.5.3 Facet strips Facet strips refer to the area of boxed where the values of faceting variables are printed. In the figure below, it’s gray strips on top of the maps. You can change how they look using strip.* options in theme() and also partially inside facet_wrap() and facet_grid(). Here is the list of available options: strip.background, strip.background.x, strip.background.y strip.placement strip.text, strip.text.x, strip.text.y strip.switch.pad.grid strip.switch.pad.wrap ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap((af_used &gt; 500) ~ year) + theme_void() + scale_color_viridis_c() + theme(legend.position = &quot;bottom&quot;) To make texts in the strips more descriptive of what they actually mean you can make variable that have texts you want to show on the map as their values. gw_KS_sf &lt;- gw_KS_sf %&gt;% mutate( high_low = ifelse(af_used &gt; 500, &quot;High water use&quot;, &quot;Low water use&quot;), year_txt = paste(&quot;Year: &quot;, year) ) ( g_facet &lt;- ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap(high_low ~ year_txt) + theme_void() + scale_color_viridis_c() + theme(legend.position = &quot;bottom&quot;) ) You probably noticed that the high water use cases are now appear on top. This is because the panels of figures are arranged in a way that the strip texts are alphabetically ordered. High water use precedes Low water use. Sometimes, this is not desirable. To force a specific order, you can turn the faceting variable (here high_low) into a factor with the order of its values defined using levels =. The following code converts high_low into a factor where “Low water use” is the first level and “High water use” is the second level. gw_KS_sf &lt;- mutate(gw_KS_sf, high_low = factor(high_low, levels = c(&quot;Low water use&quot;, &quot;High water use&quot;))) Now, “Low water use” cases appear first (on top). g_facet &lt;- ggplot() + #--- KS county boundary ---# geom_sf(data = st_transform(KS_county, 32614)) + #--- wells ---# geom_sf(data = gw_KS_sf, aes(color = af_used)) + #--- facet by year (side by side) ---# facet_wrap(high_low ~ year_txt) + theme_void() + scale_color_viridis_c() + theme(legend.position = &quot;bottom&quot;) g_facet You can control how strip texts and strip boxes appear using strip.text and strip.background options. Here is an example: g_facet + theme( strip.text.x = element_text(size = 12, family = &quot;Times&quot;, color = &quot;blue&quot;), strip.background = element_rect(fill = &quot;red&quot;, color = &quot;black&quot;) ) Instead of having descriptions of cases on top of the figures, you could have one of the descriptions on the right side of the figures using facet_grid(). g_facet + #--- this overrides facet_wrap(high_low ~ year_txt) ---# facet_grid(high_low ~ year_txt) Now case descriptions for high_low are too long and it is squeezing the space for maps. Let’s flip high_low and year. g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) This is slightly better than before, but not much. Let’s rotate strip texts for year using the angle = option. g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) + theme( strip.text.y = element_text(angle = - 90) ) Since we only want to change the angle of strip texts for the second faceting variable, we need to work on strip.text.y (if you want to work on the first one, you use strip.text.x.). Let’s change the size of the strip texts to 12 and use Times font family. g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) + theme( strip.text.y = element_text(angle = - 90, size = 12, family = &quot;Times&quot;), #--- moves up the strip texts ---# strip.text.x = element_text(size = 12, family = &quot;Times&quot;) ) The strip texts for high_low are too close to maps that letter “g” in “High” is truncated. Let’s move them up. g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) + theme( strip.text.y = element_text(angle = - 90, size = 12, family = &quot;Times&quot;), #--- moves up the strip texts ---# strip.text.x = element_text(vjust = 2, size = 12, family = &quot;Times&quot;) ) Now the upper part of the letters is truncated. We could just put more margin below the texts using the margin = margin(top, right, bottom, left, unit in text) option. g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) + theme( strip.text.y = element_text(angle = - 90, size = 12, family = &quot;Times&quot;), strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, &quot;cm&quot;), size = 12, family = &quot;Times&quot;) ) For completeness, let’s make the legend look better as well (this is discussed in section 8.5.1). ( g_facet &lt;- g_facet + #--- this overrides facet_grid(high_low ~ year_txt) ---# facet_grid(year_txt ~ high_low) + theme( strip.text.y = element_text(angle = - 90, size = 12, family = &quot;Times&quot;), strip.text.x = element_text(margin = margin(0, 0, 0.2, 0, &quot;cm&quot;), size = 12, family = &quot;Times&quot;) ) + theme(legend.position = &quot;bottom&quot;) + labs(color = &quot;Groundwater pumping (acre-feet)&quot;) + theme( legend.position = &quot;bottom&quot;, legend.key.height = unit(0.5, &quot;cm&quot;), legend.key.width = unit(2, &quot;cm&quot;), legend.text = element_text(size = 12, family = &quot;Times&quot;), legend.title = element_text(size = 12, family = &quot;Times&quot;) #--- NEW LINES HERE!! ---# ) + guides(color = guide_colorbar(title.position=&quot;top&quot;)) ) Alright, setting aside the problem of whether the information provided in the maps is meaningful or not, the maps look great at least. 8.5.4 North arrow and scale bar The ggspatial package lets you put a north arrow and scale bar on a map using annotation_scale() and annotation_north_arrow(). #--- get North Carolina county borders ---# nc &lt;- st_read(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)) Reading layer `nc&#39; from data source `/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sf/shape/nc.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 100 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 ( #--- create a simple map ---# g_nc &lt;- ggplot(nc) + geom_sf() + theme_void() ) Here is an example code that adds a scale bar: #--- load ggspatial ---# library(ggspatial) #--- add scale bar ---# g_nc + annotation_scale( location = &quot;bl&quot;, width_hint = 0.2 ) location determines where the scale bar is. The first letter is either t (top) or b (bottom), and the second letter is either l (left) or r (right). width_hint is the length of the scale bar relative to the plot. The distance number (200 km) was generated automatically according to the length of the bar. You can add pads from the plot border to fine tune the location of the scale bar: A positive number means that the scale bar will be placed further away from closest border of the plot. You can add a north arrow using annotation_north_arrow(). Accepted arguments are similar to those for annotation_scale(). g_nc + annotation_scale( location = &quot;bl&quot;, width_hint = 0.2, pad_x = unit(3, &quot;cm&quot;) ) + #--- add north arrow ---# annotation_north_arrow( location = &quot;tl&quot;, pad_x = unit(0.5, &quot;in&quot;), pad_y = unit(0.1, &quot;in&quot;), style = north_arrow_fancy_orienteering ) There are several styles you can pick from. Run ?north_arrow_orienteering to see other options. labs() can also be used to specify the x- and y-axis titles.↩︎ "],
["ggsave.html", "8.6 Saving a ggplot object as an image", " 8.6 Saving a ggplot object as an image Maps created with ggplot2 can be saved using ggsave() with the following syntax: ggsave(filename = file name, plot = ggplot object) #--- or just this ---# ggsave(file name, ggplot object) Many different file formats are supported including pdf, svg, eps, png, jpg, tif, etc. One thing you want to keep in mind is the type of graphics: vector graphics (pdf, svg, eps) raster graphics (jpg, png, tif) While vector graphics are scalable, raster graphics are not. If you enlarge raster graphics, the cells making up the figure become visible, making the figure unappealing. So, unless it is required to save figures as raster graphics, it is encouraged to save figures as vector graphics.96 Let’s try to save the following ggplot object. #--- get North Carolina county borders ---# nc &lt;- st_read(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)) Reading layer `nc&#39; from data source `/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sf/shape/nc.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 100 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 geographic CRS: NAD27 ( #--- create a map ---# g_nc &lt;- ggplot(nc) + geom_sf() ) ggsave() automatically detects the file format from the file name. For example, the following code saves g_nc as nc.pdf. ggsave() knows the ggplot object was intended to be saved as a pdf file from the extension of the specified file name. ggsave(&quot;nc.pdf&quot;, g_nc) Similarly, #--- save as an eps file ---# ggsave(&quot;nc.eps&quot;, g_nc) #--- save as an eps file ---# ggsave(&quot;nc.svg&quot;, g_nc) You can change the output size with height and width options. For example, the following code creates a pdf file of height = 5 inches and width = 7 inches. ggsave(&quot;nc.pdf&quot;, g_nc, height = 5, width = 7) You change the unit with the units option. By default, in (inches) is used. You can control the resolution of the output image by specifying DPI (dots per inch) using the dpi option. The default DPI value is 300, but you can specify any value suitable for the output image, including “retina” (320) or “screen” (72). 600 or higher is recommended when a high resolution output is required. #--- dpi = 320 ---# ggsave(&quot;nc_dpi_320.png&quot;, g_nc, height = 5, width = 7, dpi = 320) #--- dpi = 72 ---# ggsave(&quot;nc_dpi_screen.png&quot;, g_nc, height = 5, width = 7, dpi = &quot;screen&quot;) One potential advantage of the eps format is that you can adjust parts of the output figure or change the font of characters using a vector graphics editor application such as the Adobe Illustrator. I personally prefer to do everything in R, but you may find this feature appealing and often much quicker.↩︎ "],
["download-data.html", "Chapter 9 Download and process spatial datasets from within R ", " Chapter 9 Download and process spatial datasets from within R "],
["before-you-start-8.html", "Before you start", " Before you start There are many publicly available spatial datasets that can be downloaded using R. Programming data downloading using R instead of manually downloading data from websites can save lots of time and also enhances the reproducibility of your analysis. In this section, we will introduce some of such datasets and show how to download and process those data. Direction for replication Datasets No datasets to download for this Chapter. Packages Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( stars, # spatiotemporal data handling terra, # raster data handling raster, # raster data handling sf, # vector data handling dplyr, # data wrangling stringr, # string manipulation lubridate, # dates handling data.table, # data wrangling tidyr, # reshape tidyUSDA, # download USDA NASS data keyring, # API key management FedData, # download Daymet data daymetr, # download Daymet data ggplot2, # make maps tmap, # make maps future.apply, # parallel processing CropScapeR, # download CDL data prism # download PRISM data ) Run the following code to define the theme for map: theme_set(theme_bw()) theme_for_map &lt;- theme( axis.ticks = element_blank(), axis.text= element_blank(), axis.line = element_blank(), panel.border = element_blank(), panel.grid.major = element_line(color=&#39;transparent&#39;), panel.grid.minor = element_line(color=&#39;transparent&#39;), panel.background = element_blank(), plot.background = element_rect(fill = &quot;transparent&quot;,color=&#39;transparent&#39;) ) "],
["nass-quick.html", "9.1 USDA NASS QuickStat with tidyUSDA", " 9.1 USDA NASS QuickStat with tidyUSDA There are several packages that lets you download data from the USDA NASS QuickStat. Here we use the tidyUSDA package (Lindblad 2020). A nice thing about tidyUSDA is that it gives you an option to download data as an sf object, which means you can immediately visualize the data or spatially interact it with other spatial objects. First thing you want to do is to get an API key from this website, which you need to actually download data. You can download data using getQuickstat(). There are number of options you can use to narrow down the scope of the data you are downloading including data_item, geographic_level, year, commodity, and so on. See its manual for the full list of parameters you can set. As an example, the code below download corn-related data by county in Illinois for year 2016 as an sf object. ( IL_corn_yield &lt;- getQuickstat( #--- put your API key in place of key_get() ---# key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, commodity = &quot;CORN&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2016&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 384 features and 5 fields (with 16 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.51308 ymin: 36.9703 xmax: -87.01993 ymax: 42.50848 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2016 Bureau 011 CORN - ACRES PLANTED 273500 2 2016 Carroll 015 CORN - ACRES PLANTED 147500 3 2016 Henry 073 CORN - ACRES PLANTED 235000 4 2016 Jo Daviess 085 CORN - ACRES PLANTED 100500 5 2016 Lee 103 CORN - ACRES PLANTED 258500 6 2016 Mercer 131 CORN - ACRES PLANTED 142500 7 2016 Ogle 141 CORN - ACRES PLANTED 228000 8 2016 Putnam 155 CORN - ACRES PLANTED 37200 9 2016 Rock Island 161 CORN - ACRES PLANTED 65000 10 2016 Stephenson 177 CORN - ACRES PLANTED 179500 geometry 1 MULTIPOLYGON (((-89.8569 41... 2 MULTIPOLYGON (((-90.16133 4... 3 MULTIPOLYGON (((-90.43227 4... 4 MULTIPOLYGON (((-90.50668 4... 5 MULTIPOLYGON (((-89.63118 4... 6 MULTIPOLYGON (((-90.99255 4... 7 MULTIPOLYGON (((-89.68598 4... 8 MULTIPOLYGON (((-89.33303 4... 9 MULTIPOLYGON (((-90.33573 4... 10 MULTIPOLYGON (((-89.9205 42... As you can see, it is an sf object with geometry column due to geometry = TRUE option. This means that you can immediately create a map with the data (Figure 9.1): ggplot() + geom_sf( data = filter(IL_corn_yield, short_desc == &quot;CORN, GRAIN - YIELD, MEASURED IN BU / ACRE&quot;), aes(fill = Value) ) + theme_for_map Figure 9.1: Corn Yield (bu/acre) in Illinois in 2016 You can download data for multiple states and years at the same time like below (of course, if you want the whole U.S., don’t specify the state parameter.). ( IL_CO_NE_corn &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, commodity = &quot;CORN&quot;, geographic_level = &quot;COUNTY&quot;, state = c(&quot;ILLINOIS&quot;, &quot;COLORADO&quot;, &quot;NEBRASKA&quot;), year = paste(2014:2018), geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 6384 features and 5 fields (with 588 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -109.0459 ymin: 36.9703 xmax: -87.01993 ymax: 43.00171 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2017 Boulder 013 CORN - ACRES PLANTED 3000 2 2016 Boulder 013 CORN - ACRES PLANTED 3300 3 2016 Larimer 069 CORN - ACRES PLANTED 12800 4 2015 Larimer 069 CORN - ACRES PLANTED 14900 5 2014 Larimer 069 CORN - ACRES PLANTED 13600 6 2015 Logan 075 CORN - ACRES PLANTED 74500 7 2014 Logan 075 CORN - ACRES PLANTED 86100 8 2018 Morgan 087 CORN - ACRES PLANTED 78300 9 2017 Morgan 087 CORN - ACRES PLANTED 78200 10 2015 Morgan 087 CORN - ACRES PLANTED 60200 geometry 1 MULTIPOLYGON (((-105.6486 4... 2 MULTIPOLYGON (((-105.6486 4... 3 MULTIPOLYGON (((-105.8225 4... 4 MULTIPOLYGON (((-105.8225 4... 5 MULTIPOLYGON (((-105.8225 4... 6 MULTIPOLYGON (((-103.5737 4... 7 MULTIPOLYGON (((-103.5737 4... 8 MULTIPOLYGON (((-103.7148 4... 9 MULTIPOLYGON (((-103.7148 4... 10 MULTIPOLYGON (((-103.7148 4... 9.1.1 Look for parameter values This package has a function that lets you see all the possible parameter values you can use for many of these parameters. For example, suppose you know you would like irrigated corn yields in Colorado, but you are not sure what parameter value (string) you should supply to the data_item parameter. Then, you can do this:97 #--- get all the possible values for data_item ---# all_items &lt;- tidyUSDA::allDataItem #--- take a look at the first six ---# head(all_items) short_desc1 &quot;AG LAND - ACRES&quot; short_desc2 &quot;AG LAND - NUMBER OF OPERATIONS&quot; short_desc3 &quot;AG LAND - OPERATIONS WITH TREATED&quot; short_desc4 &quot;AG LAND - TREATED, MEASURED IN ACRES&quot; short_desc5 &quot;AG LAND, (EXCL CROPLAND &amp; PASTURELAND &amp; WOODLAND) - ACRES&quot; short_desc6 &quot;AG LAND, (EXCL CROPLAND &amp; PASTURELAND &amp; WOODLAND) - AREA, MEASURED IN PCT OF AG LAND&quot; You can use key words like “CORN”, “YIELD”, “IRRIGATED” to narrow the entire list using grep()98: all_items %&gt;% grep(pattern = &quot;CORN&quot;, ., value = TRUE) %&gt;% grep(pattern = &quot;YIELD&quot;, ., value = TRUE) %&gt;% grep(pattern = &quot;IRRIGATED&quot;, ., value = TRUE) short_desc9227 &quot;CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE&quot; short_desc9228 &quot;CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE&quot; short_desc9233 &quot;CORN, GRAIN, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN BU / ACRE&quot; short_desc9236 &quot;CORN, GRAIN, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN BU / ACRE&quot; short_desc9238 &quot;CORN, GRAIN, IRRIGATED, PART OF CROP - YIELD, MEASURED IN BU / ACRE&quot; short_desc9249 &quot;CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / ACRE&quot; short_desc9250 &quot;CORN, GRAIN, NON-IRRIGATED - YIELD, MEASURED IN BU / NET PLANTED ACRE&quot; short_desc9291 &quot;CORN, SILAGE, IRRIGATED - YIELD, MEASURED IN TONS / ACRE&quot; short_desc9296 &quot;CORN, SILAGE, IRRIGATED, ENTIRE CROP - YIELD, MEASURED IN TONS / ACRE&quot; short_desc9299 &quot;CORN, SILAGE, IRRIGATED, NONE OF CROP - YIELD, MEASURED IN TONS / ACRE&quot; short_desc9301 &quot;CORN, SILAGE, IRRIGATED, PART OF CROP - YIELD, MEASURED IN TONS / ACRE&quot; short_desc9307 &quot;CORN, SILAGE, NON-IRRIGATED - YIELD, MEASURED IN TONS / ACRE&quot; short_desc28557 &quot;SWEET CORN, IRRIGATED - YIELD, MEASURED IN CWT / ACRE&quot; short_desc28564 &quot;SWEET CORN, NON-IRRIGATED - YIELD, MEASURED IN CWT / ACRE&quot; Looking at the list, we know the exact text value we want, which is the first entry of the vector. ( CO_ir_corn_yield &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, data_item = &quot;CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;COLORADO&quot;, year = &quot;2018&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 4 features and 5 fields (with 1 geometry empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -103.7071 ymin: 38.61245 xmax: -102.0448 ymax: 40.43851 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs year NAME county_code 1 2018 Cheyenne 017 2 2018 Kit Carson 063 3 2018 Washington 121 4 2018 &lt;NA&gt; 998 short_desc Value 1 CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE 189.3 2 CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE 186.2 3 CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE 148.8 4 CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE 218.6 geometry 1 MULTIPOLYGON (((-102.5769 3... 2 MULTIPOLYGON (((-102.4257 3... 3 MULTIPOLYGON (((-103.472 40... 4 MULTIPOLYGON EMPTY Here is the complete list of functions that gives you the possible values of the parameters for getQuickstat(). tidyUSDA::allCategory tidyUSDA::allSector tidyUSDA::allGroup tidyUSDA::allCommodity tidyUSDA::allDomain tidyUSDA::allCounty tidyUSDA::allProgram tidyUSDA::allDataItem tidyUSDA::allState tidyUSDA::allGeogLevel 9.1.2 Caveats You cannot retrieve more than \\(50,000\\) (the limit is set by QuickStat) rows of data. The query below requests much more than \\(50,000\\) observations, and fail. In this case, you need to narrow the search and chop the task into smaller tasks. many_states_corn &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, commodity = &quot;CORN&quot;, geographic_level = &quot;COUNTY&quot;, state = c(&quot;ILLINOIS&quot;, &quot;COLORADO&quot;, &quot;NEBRASKA&quot;, &quot;IOWA&quot;, &quot;KANSAS&quot;), year = paste(1995:2018), geometry = TRUE ) Error in open.connection(con, &quot;rb&quot;): HTTP error 413. Another caveat is that the query returns an error when there is no observation that satisfy your query criteria. For example, even though “CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE” is one of the values you can use for data_item, there is no entry for the statistic in Illinois in 2018. Therefore, the following query fails. many_states_corn &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;), program = &quot;SURVEY&quot;, data_item = &quot;CORN, GRAIN, IRRIGATED - YIELD, MEASURED IN BU / ACRE&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2018&quot;, geometry = TRUE ) Error in open.connection(con, &quot;rb&quot;): HTTP error 400. References "],
["CropScapeR.html", "9.2 CDL with CropScapeR", " 9.2 CDL with CropScapeR The Cropland Data Layer (CDL) is a data product produced by the National Agricultural Statistics Service of U.S. Department of Agriculture. CDL provides geo-referenced, high accuracy, 30 (after 2007) or 56 (in 2006 and 2007) meter resolution, crop-specific cropland land cover information for up to 48 contiguous states in the U.S. from 1997 to the present. This data product has been extensively used in agricultural research. CropScape is an interactive Web CDL exploring system (https://nassgeodata.gmu.edu/CropScape/), and it was developed to query, visualize, disseminate, and analyze CDL data geospatially through standard geospatial web services in a publicly accessible on-line environment (Han et al., 2012). This section shows how to use the CropScapeR package (Chen 2020) to download and explore the CDL data. The package implements some of the most useful geospatial processing services provided by the CropScape, and it allows users to efficiently process the CDL data within the R environment. Specifically, the CropScapeR package provides four functions that implement different kinds of geospatial processing services provided by the CropScape. This section introduces these functions while providing some examples. GetCDLData() in particular is the most important function as it lets you download the raw CDL data. The other functions provide the users with the CDL data summarized or transformed in particular manners that may suit the need of some users. Note: There is a known problem with Mac users requesting CDL data services using the CropScape API, which causes errors when using the functions provided by the package. Please see section 9.2.4 for a workaround. The CropScapeR package can be installed directly from ‘CRAN’: install.packages(&quot;CropScapeR&quot;) The development version of the package can be downloaded from its GitHub website using the following codes: library(devtools) devtools::install_github(&quot;cbw1243/CropScapeR&quot;) Let’s load the package. library(CropScapeR) Acknowledgment: The development of the CropScapeR package was supported by USDA-NRCS Agreement No. NR193A750016C001 through the Cooperative Ecosystem Studies Units network. Any opinions, findings, conclusions, or recommendations expressed are those of the author(s) and do not necessarily reflect the view of the U.S. Department of Agriculture. 9.2.1 GetCDLData: Download the CDL data as raster data GetCDLData() allows us to obtain CDL data for any Area of Interest (AOI) in a given year. It requires three parameters to make a valid data request: aoi: Area of Interest (AOI). year: Year of the data to request. type: Type of AOI. The following AOI-type combinations are accepted: any spatial object as an sf or sfc object - type = \"b\" county (defined by a 5-digit county FIPS code) - type = \"f\" state (defined by a 2-digit state FIPS code) - type = \"f\" bounding box (defined by four corner points) - type = \"b\" polygon area (defined by at least three coordinates) - type = \"ps\" single point (defined by a coordinate) - type = \"p\" This section discusses how to download data for an sf object, county, and state as they are likely to be the most common AOI. See the package github site (https://github.com/cbw1243/CropScapeR) to see how the other options work. 9.2.1.1 Downloading CDL data for sf, county, and state Downloading CDL data for sf Let’s download the 2018 CDL data for the area that covers Champaign, Vermilion, Ford, and Iroquois Counties in Illinois (a map below). library(tigris) #--- get the sf for the four counties ---# IL_4_county &lt;- counties(state = &quot;Illinois&quot;, cb = TRUE) %&gt;% st_as_sf() %&gt;% filter(NAME %in% c(&quot;Champaign&quot;, &quot;Vermilion&quot;, &quot;Ford&quot;, &quot;Iroquois&quot;)) ggplot() + geom_sf(data = IL_county) + geom_sf(data = IL_county_4, fill = &quot;lightblue&quot;) + theme_void() When you use an sf object for aoi, CDL data will be downloaded for the bounding box (this is why type = \"b\") that encompasses the entire geographic area of the sf object irrespective of the type of objects in the sf object (whether they are points, polygons, lines). In this case, CDL data is downloaded for the red area in the map below. ggplot() + geom_sf(data = IL_county) + geom_sf(data = st_as_sfc(st_bbox(IL_county_4)), fill = &quot;red&quot;, alpha = 0.4) + theme_void() Let’s download CDL data for the four counties: ( cdl_IL_4 &lt;- GetCDLData( aoi = IL_county_4, year = &quot;2018&quot;, type = &quot;b&quot; ) ) class : RasterLayer dimensions : 4431, 2826, 12522006 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : 631935, 716715, 1898745, 2031675 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/Data/IL4.tif names : IL4 values : 0, 255 (min, max) As you can see, the downloaded data is a RasterLayer object99. Note that the CDL data uses the Albers equal-area conic projection. projection(cdl_IL_4) [1] &quot;+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot; Take a look at the downloaded CDL data. plot(cdl_IL_4) If you do not want to have values outside of the sf object, you can use raster::mask() to turn them into NA as follows: cdl_IL_4_masked &lt;- IL_county_4 %&gt;% #--- change the CRS first to that of the raster data ---# st_transform(., projection(cdl_IL_4)) %&gt;% #--- mask the values outside the sf (turn them into NA) ---# raster::mask(cdl_IL_4, .) As you can see below, values outside the four counties are now NA (black area): plot(cdl_IL_4_masked) Downloading CDL data for county The following code makes a request to download the CDL data for Champaign county in Illinois in 2018. ( cdl_Champaign &lt;- GetCDLData(aoi = 17019, year = 2018, type = &#39;f&#39;) ) class : RasterLayer dimensions : 2060, 1626, 3349560 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : 633825, 682605, 1898745, 1960545 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /Users/tmieno2/Dropbox/TeachingUNL/R_as_GIS/ch.tif names : ch values : 0, 255 (min, max) In the above code, the FIPS code for Champaign County (17019) was supplied to the aoi option. Because a county is used here, the type argument is specified as ‘f’. plot(cdl_Champaign) Downloading CDL data for state The following code makes a request to download the CDL data for the state of Illinois in 2018. ( cdl_IL &lt;- GetCDLData(aoi = 17, year = 2018, type = &#39;f&#39;) ) In the above code, the state FIPS code for Illinois (\\(17\\)) was supplied to the aoi option. Because a county is used here, the type argument is specified as ‘f’. plot(cdl_IL) 9.2.1.2 Other format options GeoTiff You could save the downloaded CDL data as a tif file by adding save_path = option to GetCDLData() as follows: ( cdl_IL_4 &lt;- GetCDLData( aoi = IL_county_4, year = &quot;2018&quot;, type = &quot;b&quot;, save_path = &quot;/Data/IL_4.tif&quot; ) ) With this code, the downloaded data will be saved as “IL_4.tif” in the “Data” folder residing in the current working directory. sf The GetCDLData function lets you download CDL data as an sf of points, where the coordinates of the points are the coordinates of the centroid of the raster cells. This can be done by adding format = sf as an option. ( cdl_sf &lt;- GetCDLData(aoi = 17019, year = 2018, type = &#39;f&#39;, format = &quot;sf&quot;) ) The first column (value) is the crop code. Of course, you can manually convert a RasterLayer to an sf of points as follows: as.data.frame(cdl_Champaign, xy = TRUE) %&gt;% #--- to sf consisting of points ---# st_as_sf(coords = c(&quot;x&quot;, &quot;y&quot;)) %&gt;% #--- Albert conic projection ---# st_set_crs(&quot;+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs&quot;) Simple feature collection with 3349560 features and 1 field geometry type: POINT dimension: XY bbox: xmin: 633840 ymin: 1898760 xmax: 682590 ymax: 1960530 CRS: +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs First 10 features: ch geometry 1 0 POINT (633840 1960530) 2 0 POINT (633870 1960530) 3 0 POINT (633900 1960530) 4 0 POINT (633930 1960530) 5 0 POINT (633960 1960530) 6 0 POINT (633990 1960530) 7 0 POINT (634020 1960530) 8 0 POINT (634050 1960530) 9 0 POINT (634080 1960530) 10 0 POINT (634110 1960530) The format = sf option makes GetCDLData() do this conversion internally for those who want CDL data as an sf consisting of points instead of a RasterLayer. 9.2.2 Data processing after downloading data The downloaded raster data itself is not readily usable immediately for your economic analysis. Typically the variable of interest is the frequency of land use types or their shares. You can use raster::freq() to get the frequency (the number of raster cells) of each land use type. ( crop_freq &lt;- freq(cdl_Champaign) ) value count [1,] 0 476477 [2,] 1 1211343 [3,] 4 15 [4,] 5 1173150 [5,] 23 8 [6,] 24 8869 [7,] 26 1168 [8,] 27 34 [9,] 28 52 [10,] 36 4418 [11,] 37 6804 [12,] 43 2 [13,] 59 1064 [14,] 60 79 [15,] 61 54 [16,] 111 6112 [17,] 121 111191 [18,] 122 155744 [19,] 123 38898 [20,] 124 12232 [21,] 131 1333 [22,] 141 49012 [23,] 142 15 [24,] 143 7 [25,] 152 77 [26,] 176 84463 [27,] 190 6545 [28,] 195 339 [29,] 222 1 [30,] 229 16 [31,] 241 38 Clearly, once frequencies are found, you can easily get shares as well: ( crop_data &lt;-crop_freq %&gt;% #--- matrix to data.frame ---# data.frame(.) %&gt;% #--- find share ---# mutate(share = count/sum(count)) ) value count share 1 0 476477 1.422506e-01 2 1 1211343 3.616424e-01 3 4 15 4.478200e-06 4 5 1173150 3.502400e-01 5 23 8 2.388373e-06 6 24 8869 2.647810e-03 7 26 1168 3.487025e-04 8 27 34 1.015059e-05 9 28 52 1.552443e-05 10 36 4418 1.318979e-03 11 37 6804 2.031312e-03 12 43 2 5.970933e-07 13 59 1064 3.176537e-04 14 60 79 2.358519e-05 15 61 54 1.612152e-05 16 111 6112 1.824717e-03 17 121 111191 3.319570e-02 18 122 155744 4.649685e-02 19 123 38898 1.161287e-02 20 124 12232 3.651823e-03 21 131 1333 3.979627e-04 22 141 49012 1.463237e-02 23 142 15 4.478200e-06 24 143 7 2.089827e-06 25 152 77 2.298809e-05 26 176 84463 2.521615e-02 27 190 6545 1.953988e-03 28 195 339 1.012073e-04 29 222 1 2.985467e-07 30 229 16 4.776747e-06 31 241 38 1.134477e-05 At this point, the data does not tell you which value corresponds to which crop. To find the crop names associated with the crop codes (value), you can get the reference table using data(linkdata) from the CropScapeR package.100 #--- load the crop code reference data ---# data(&quot;linkdata&quot;) You can merge the two data sets using value from the CDL data and MasterCat from linkdata as the merging keys: ( crop_data &lt;- dplyr::left_join(crop_data, linkdata, by = c(&#39;value&#39; = &#39;MasterCat&#39;)) ) value count share Crop 1 0 476477 1.422506e-01 NoData 2 1 1211343 3.616424e-01 Corn 3 4 15 4.478200e-06 Sorghum 4 5 1173150 3.502400e-01 Soybeans 5 23 8 2.388373e-06 Spring_Wheat 6 24 8869 2.647810e-03 Winter_Wheat 7 26 1168 3.487025e-04 Dbl_Crop_WinWht/Soybeans 8 27 34 1.015059e-05 Rye 9 28 52 1.552443e-05 Oats 10 36 4418 1.318979e-03 Alfalfa 11 37 6804 2.031312e-03 Other_Hay/Non_Alfalfa 12 43 2 5.970933e-07 Potatoes 13 59 1064 3.176537e-04 Sod/Grass_Seed 14 60 79 2.358519e-05 Switchgrass 15 61 54 1.612152e-05 Fallow/Idle_Cropland 16 111 6112 1.824717e-03 Open_Water 17 121 111191 3.319570e-02 Developed/Open_Space 18 122 155744 4.649685e-02 Developed/Low_Intensity 19 123 38898 1.161287e-02 Developed/Med_Intensity 20 124 12232 3.651823e-03 Developed/High_Intensity 21 131 1333 3.979627e-04 Barren 22 141 49012 1.463237e-02 Deciduous_Forest 23 142 15 4.478200e-06 Evergreen_Forest 24 143 7 2.089827e-06 Mixed_Forest 25 152 77 2.298809e-05 Shrubland 26 176 84463 2.521615e-02 Grassland/Pasture 27 190 6545 1.953988e-03 Woody_Wetlands 28 195 339 1.012073e-04 Herbaceous_Wetlands 29 222 1 2.985467e-07 Squash 30 229 16 4.776747e-06 Pumpkins 31 241 38 1.134477e-05 Dbl_Crop_Corn/Soybeans NoData in Crop corresponds to the black area in the above figure, which is the portion of the raster data that does not overlap with the boundary of Champaign County. These points with NoData can be removed by using the filter function. 9.2.3 Other forms of CDL data Instead of downloading the raw CDL data, CropScape provides an option to download summarized CDL data. GetCDLComp: request data on land use changes GetCDLStat: get acreage estimates from the CDL GetCDLImage: download the image files of the CDL data These may come handy if they satisfy your needs because you can skip post-downloading processing steps. GetCDLComp(): request data on land use changes The GetCDLComp function allows users to request data on changes in land cover over time from the CDL. Specifically, this function returns acres changed from one crop category to another crop category between two years for a user-defined AOI. Let’s see an example. The following codes request data on acreage changes in land cover for Champaign County (FIPS = 17019) from 2017 (year1 = 2017) to 2018 (year2 = 2018). ( data_change &lt;- GetCDLComp(aoi = &#39;17019&#39;, year1 = 2017, year2 = 2018, type = &#39;f&#39;) ) From To Count Acreage aoi 1: Corn Corn 181490 40362.4 17019 2: Corn Sorghum 1 0.2 17019 3: Corn Soybeans 1081442 240506.9 17019 4: Corn Winter Wheat 1950 433.7 17019 5: Corn Dbl Crop WinWht/Soybeans 110 24.5 17019 --- 241: Herbaceous Wetlands Herbaceous Wetlands 18 4.0 17019 242: Dbl Crop WinWht/Corn Corn 1 0.2 17019 243: Pumpkins Corn 69 15.3 17019 244: Pumpkins Sorghum 2 0.4 17019 245: Pumpkins Soybeans 62 13.8 17019 What is returned is a data.frame (data.table) that has 5 columns. The columns “From” and “To” are crop names. The column “Count” is the pixel count, and “Acreage” is the acres corresponding to the pixel counts. The last column “aoi” is the selected AOI. The first row of the returned data table shows the acreage (i.e., 40,362 acres) of continuous corn during 2017 and 2018. The third row shows the acreage (i.e., 240,506 acres) rotated from corn to soybeans during 2017 and 2018. Remember that the spatial resolution changes from 56 meters to 30 meters starting 2008. This means that when data is requested for land use changes from 2007 to 2008, two CDL raster layers have different spatial resolutions. Consequently, CropScape API fails to resolve the issue and return an error message saying “Mismatch size of file 1 and file 2.” The GetCDLComp() function manually resolves this problem by resampling two CDL raster files using the nearest neighbor resampling technique such that both rasters have the same spatial resolutions. The finer resolution raster is downscaled to the lower resolution. Then, the resampled raster layers are merged together to calculate cropland changes. Users can turn off this default behavior by adding manual_try = FALSE option. In this case, an error message from CropScape API will be returned with no land use changes results. data_change &lt;- GetCDLComp(aoi = &#39;17019&#39;, year1 = 2007, year2 = 2008, type = &#39;f&#39;, `manual_try` = FALSE) Error in GetCDLCompF(fips = aoi, year1 = year1, year2 = year2, mat = mat, : Error: The requested data might not exist in the CDL database. Error message from CropScape is :&lt;faultstring&gt;Error: Mismatch size of file 1 and file 2. &lt;/faultstring&gt; GetCDLStat(): get acreage estimates from the CDL The GetCDLStat function allows users to get acreage by land cover category for a user defined AOI in a year. For example, the following codes request data on acreage by land cover categories for Champaign County in Illinois in 2018. You can see that the pixel counts are already converted to acres and category names are attached. ( data_stat &lt;- GetCDLStat(aoi = 17019, year = 2018, type = &#39;f&#39;) ) Value Category Acreage 1: 1 Corn 269396.2 2: 4 Sorghum 3.3 3: 5 Soybeans 260902.3 4: 23 Spring Wheat 1.8 5: 24 Winter Wheat 1972.4 6: 26 Dbl Crop WinWht/Soybeans 259.8 7: 27 Rye 7.6 8: 28 Oats 11.6 9: 36 Alfalfa 982.5 10: 37 Other Hay/Non Alfalfa 1513.2 11: 43 Potatoes 0.4 12: 59 Sod/Grass Seed 236.6 13: 60 Switchgrass 17.6 14: 61 Fallow/Idle Cropland 12.0 15: 111 Open Water 1359.3 16: 121 Developed/Open Space 24728.3 17: 122 Developed/Low Intensity 34636.6 18: 123 Developed/Medium Intensity 8650.7 19: 124 Developed/High Intensity 2720.3 20: 131 Barren 296.5 21: 141 Deciduous Forest 10900.0 22: 142 Evergreen Forest 3.3 23: 143 Mixed Forest 1.6 24: 152 Shrubland 17.1 25: 176 Grass/Pasture 18784.1 26: 190 Woody Wetlands 1455.6 27: 195 Herbaceous Wetlands 75.4 28: 222 Squash 0.2 29: 229 Pumpkins 3.6 30: 241 Dbl Crop Corn/Soybeans 8.5 Value Category Acreage GetCDLImage(): Download the image files of the CDL data The GetCDLImage function allows users to download the image files of the CDL data. This function is very similar to the GetCDLData function, except that image files are returned. This function can be helpful if you only want to look at the picture of the CDL data. By default, the picture is saved as the ‘png’ format. You can also save it in the ‘kml’ format. GetCDLImage(aoi = 17019, year = 2018, type = &#39;f&#39;, verbose = F) 9.2.4 SSL certificate problem on Mac SSL refers to the Secure Sockets Layer, and SSL certificate displays important information for verifying the owner of a website and encrypting web traffic with SSL/TLS for securing connection. It is a known problem that Mac users encounter the following error when GetCDLData() is used: ‘SSL certificate problem: SSL certificate expired’. As the name suggests, this is because the CropScape server has an expired certificate. While this affects the Mac users, Windows users should not expect this issue. To avoid the problem for Mac users, the CropScapeR has a workaround that involves downloading the GeoTiff file for the requested AOI first, and then read the file using the raster() function as a RasterLayer.101 You first need to run the following code before requesting data from CDL. #--- Skip the SSL check ---# httr::set_config(httr::config(ssl_verifypeer = 0L)) Now, you can download CDL data by specifying the file path with the save_path option like below: #--- Download the raster TIF file into specified path, also read into R ---# data &lt;- GetCDLData(aoi = 17019, year = 2018, type = &#39;f&#39;, save_path = &quot;ch.tif&quot;) Hopefully, this problem is fixed by the maintainer of CropScape so that this workaround is not necessary for Mac users. References "],
["download-prism.html", "9.3 PRISM with prism", " 9.3 PRISM with prism 9.3.1 Basics PRISM dataset provide model-based estimates of precipitation, tmax, and tmin for the U.S. at the 4km by 4km spatial resolution. Here, we use get_prism_dailys() from the prism package (Hart and Bell 2015) to download daily data. Here is its general syntax: #--- NOT RUN ---# get_prism_dailys( type = variable type, minDate = starting date as character, maxDate = ending date as character, keepZip = TRUE or FALSE ) The variables types you can select from is “ppt” (precipitation), “tmean” (mean temperature), “tmin” (minimum temperature), and “tmax” (maximum temperature). For minDate and maxDate, the dates must be specified in a specific format of “YYYY-MM-DD”. keepZip = FALSE does not keep the zipped folders of the downloaded files as the name suggests. Before you download PRISM data using the function, it is recommended that you set the path to folder in which the downloaded PRISM will be stored using options(prism.path = \"path\"). For example, the following set the path to “Data/PRISM/” relative to the current working directory. options(prism.path = &quot;./Data/PRISM/&quot;) The following code downloads daily precipitation data from January 1, 2000 to Jan 10, 2000. #--- NOT RUN ---# get_prism_dailys( type = &quot;ppt&quot;, minDate = &quot;2000-01-01&quot;, maxDate = &quot;2000-01-10&quot;, keepZip = FALSE ) When you download data using the above code, you will notice that it creates one folder for one day. For example, for precipitation data for “2000-01-01”, you can get the path to the downloaded file as follows: var_type &lt;- &quot;ppt&quot; # variable type dates_prism_txt &lt;- str_remove_all(&quot;2000-01-01&quot;, &quot;-&quot;) # date without dashes #--- folder name ---# folder_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil&quot;) #--- file name of the downloaded data inside the above folder ---# file_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil.bil&quot;) #--- path to the file relative to the designated data folder (here, it&#39;s &quot;./Data/PRISM/&quot;) ---# ( file_path &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) ) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20000101_bil/PRISM_ppt_stable_4kmD2_20000101_bil.bil&quot; We can then easily read the data using terra::rast() or stars::read_stars() if you prefer the stars way. #--- as SpatRaster ---# ( prism_2000_01_01_sr &lt;- rast(file_path) ) class : SpatRaster dimensions : 621, 1405, 1 (nrow, ncol, nlyr) resolution : 0.04166667, 0.04166667 (x, y) extent : -125.0208, -66.47917, 24.0625, 49.9375 (xmin, xmax, ymin, ymax) coord. ref. : +proj=longlat +ellps=GRS80 +no_defs data source : PRISM_ppt_stable_4kmD2_20000101_bil.bil names : PRISM_ppt_stable_4kmD2_20000101_bil min values : 0 max values : 49.848 #--- as stars ---# ( prism_2000_01_01_stars &lt;- read_stars(file_path) ) stars object with 2 dimensions and 1 attribute attribute(s): PRISM_ppt_stable_4kmD2_20000101_bil.bil Min. : 0.0 1st Qu.: 0.0 Median : 0.0 Mean : 0.5 3rd Qu.: 0.0 Max. :49.8 NA&#39;s :390874 dimension(s): from to offset delta refsys point values x 1 1405 -125.021 0.0416667 NAD83 NA NULL [x] y 1 621 49.9375 -0.0416667 NAD83 NA NULL [y] Here is a quick visualization of the data (Figure 9.2): plot(prism_2000_01_01_stars) Figure 9.2: PRISM precipitation data for January 1, 2000 As you can see, the data covers the entire contiguous U.S. 9.3.2 Download daily PRISM data for many years and build your own datasets Here, an example of how to create your own sets of PRISM datasets is presented. Creating such datasets and have them locally can be useful if you expect to use the data for many different projects in the future. Suppose we are interested in saving daily PRISM precipitation data by year-month from 1980 to 2018. We will write a loop that loops over all the year-month combinations. Before writing a loop, let’s work on the code for a particular year-month combination: 1990-December. However, we will write codes in a way that can be easily translated into a looped operations later. Specifically, we will define the following variables and use them as if they are variables to be looped over in a loop. #--- month to work on ---# temp_month &lt;- 12 #--- year to work on ---# temp_year &lt;- 1990 We first need to set the path to the folder in which daily PRISM files will be downloaded. #--- set your own path ---# options(prism.path = &quot;./Data/PRISM/&quot;) We then set the start and end dates for get_prism_dailys(). #--- starting date of the working month-year ---# ( start_date &lt;- dmy(paste0(&quot;1/&quot;, temp_month, &quot;/&quot;, temp_year)) ) [1] &quot;1990-12-01&quot; #--- ending date: add a month and then go back 1 day ---# ( end_date &lt;- start_date %m+% months(1) - 1 ) [1] &quot;1990-12-31&quot; We now download PRISM data for the year-month we are working on. #--- download daily PRISM data for the working month-year ---# get_prism_dailys( type = &quot;ppt&quot;, minDate = as.character(start_date), maxDate = as.character(end_date), keepZip = FALSE ) Once all the data are downloaded, we will read and import them onto R. To do so, we will need the path to all the downloaded files. #--- list of dates of the working month-year ---# dates_ls &lt;- seq(start_date, end_date, &quot;days&quot;) #--- remove dashes ---# dates_prism_txt &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- folder names ---# folder_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil&quot;) #--- the file name of the downloaded data ---# file_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil.bil&quot;) #--- complete path to the downloaded files ---# ( file_path &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) ) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901201_bil/PRISM_ppt_stable_4kmD2_19901201_bil.bil&quot; [2] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901202_bil/PRISM_ppt_stable_4kmD2_19901202_bil.bil&quot; [3] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901203_bil/PRISM_ppt_stable_4kmD2_19901203_bil.bil&quot; [4] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901204_bil/PRISM_ppt_stable_4kmD2_19901204_bil.bil&quot; [5] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901205_bil/PRISM_ppt_stable_4kmD2_19901205_bil.bil&quot; [6] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901206_bil/PRISM_ppt_stable_4kmD2_19901206_bil.bil&quot; [7] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901207_bil/PRISM_ppt_stable_4kmD2_19901207_bil.bil&quot; [8] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901208_bil/PRISM_ppt_stable_4kmD2_19901208_bil.bil&quot; [9] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901209_bil/PRISM_ppt_stable_4kmD2_19901209_bil.bil&quot; [10] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901210_bil/PRISM_ppt_stable_4kmD2_19901210_bil.bil&quot; [11] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901211_bil/PRISM_ppt_stable_4kmD2_19901211_bil.bil&quot; [12] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901212_bil/PRISM_ppt_stable_4kmD2_19901212_bil.bil&quot; [13] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901213_bil/PRISM_ppt_stable_4kmD2_19901213_bil.bil&quot; [14] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901214_bil/PRISM_ppt_stable_4kmD2_19901214_bil.bil&quot; [15] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901215_bil/PRISM_ppt_stable_4kmD2_19901215_bil.bil&quot; [16] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901216_bil/PRISM_ppt_stable_4kmD2_19901216_bil.bil&quot; [17] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901217_bil/PRISM_ppt_stable_4kmD2_19901217_bil.bil&quot; [18] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901218_bil/PRISM_ppt_stable_4kmD2_19901218_bil.bil&quot; [19] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901219_bil/PRISM_ppt_stable_4kmD2_19901219_bil.bil&quot; [20] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901220_bil/PRISM_ppt_stable_4kmD2_19901220_bil.bil&quot; [21] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901221_bil/PRISM_ppt_stable_4kmD2_19901221_bil.bil&quot; [22] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901222_bil/PRISM_ppt_stable_4kmD2_19901222_bil.bil&quot; [23] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901223_bil/PRISM_ppt_stable_4kmD2_19901223_bil.bil&quot; [24] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901224_bil/PRISM_ppt_stable_4kmD2_19901224_bil.bil&quot; [25] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901225_bil/PRISM_ppt_stable_4kmD2_19901225_bil.bil&quot; [26] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901226_bil/PRISM_ppt_stable_4kmD2_19901226_bil.bil&quot; [27] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901227_bil/PRISM_ppt_stable_4kmD2_19901227_bil.bil&quot; [28] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901228_bil/PRISM_ppt_stable_4kmD2_19901228_bil.bil&quot; [29] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901229_bil/PRISM_ppt_stable_4kmD2_19901229_bil.bil&quot; [30] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901230_bil/PRISM_ppt_stable_4kmD2_19901230_bil.bil&quot; [31] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_19901231_bil/PRISM_ppt_stable_4kmD2_19901231_bil.bil&quot; We now read them as a stars object, set the third dimension as date using Dates object class, and then save it as an R dataset (This will ensure that date dimensions is kept. See Chapter 7.4). ( #--- combine all the PRISM files as stars ---# temp_stars &lt;- read_stars(file_path, along = 3) #--- set the third dimension as data ---# st_set_dimensions(&quot;band&quot;, values = dates_ls, name = &quot;date&quot;) ) #--- save the stars as an rds file ---# saveRDS( temp_stars, paste0(&quot;./Data/PRISM/PRISM_&quot;, var_type, &quot;_y&quot;, temp_year, &quot;_m&quot;, temp_month, &quot;.rds&quot;) ) You could alternatively read the files into a SpatRaster object and save it data as a GeoTIFF file. ( #--- combine all the PRISM files as a RasterStack ---# temp_stars &lt;- terra::rast(file_path) ) #--- save as a multi-band GeoTIFF file ---# writeRaster(temp_stars, paste0(&quot;./Data/PRISM/PRISM_&quot;, var_type, &quot;_y&quot;, temp_year, &quot;_m&quot;, temp_month, &quot;.tif&quot;), overwrite = T) Note that this option of course does not have date as the third dimension. Moreover, the RDS file above takes up only 14 Mb, while the tif file occupies 108 Mb. Finally, if you would like, you can delete all the individual PRISM files: #--- delete all the downloaded files ---# unlink(paste0(&quot;./Data/PRISM/&quot;, folder_name), recursive = TRUE) Okay, now that we know what to do with a particular year-month combination, we can easily write a loop to go over all the year-month combinations for the period of interest. Since all the processes we observed above for a single year-month combination is embarrassingly parallel, it is easy to parallelize using future.apply::future_lapply() or parallel::mclapply() (Linux/Mac users only). Here we use future_lapply(). Let’s first get the number of logical cores. library(parallel) num_cores &lt;- detectCores() plan(multiprocess, workers = num_cores) The following function goes through all the steps we saw above for a single year-month combination. #--- define a function to download and save PRISM data stacked by month ---# get_save_prism &lt;- function(i, var_type) { print(paste0(&quot;working on &quot;, i)) temp_month &lt;- month_year_data[i, month] # working month temp_year &lt;- month_year_data[i, year] # working year #--- starting date of the working month-year ---# start_date &lt;- dmy(paste0(&quot;1/&quot;, temp_month, &quot;/&quot;, temp_year)) #--- end date ---# end_date &lt;- start_date %m+% months(1) - 1 #--- download daily PRISM data for the working month-year ---# get_prism_dailys( type = var_type, minDate = as.character(start_date), maxDate = as.character(end_date), keepZip = FALSE ) #--- list of dates of the working month-year ---# dates_ls &lt;- seq(start_date, end_date, &quot;days&quot;) #--- remove dashes ---# dates_prism_txt &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- folder names ---# folder_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil&quot;) #--- the file name of the downloaded data ---# file_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil.bil&quot;) #--- complete path to the downloaded files ---# file_path &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- combine all the PRISM files as a RasterStack ---# temp_stars &lt;- stack(file_path) %&gt;% #--- convert to stars ---# st_as_stars() %&gt;% #--- set the third dimension as data ---# st_set_dimensions(&quot;band&quot;, values = dates_ls, name = &quot;date&quot;) #--- save the stars as an rds file ---# saveRDS( temp_stars, paste0(&quot;./Data/PRISM/PRISM_&quot;, var_type, &quot;_y&quot;, temp_year, &quot;_m&quot;, temp_month, &quot;.rds&quot;) ) #--- delete all the downloaded files ---# unlink(paste0(&quot;./Data/PRISM/&quot;, folder_name), recursive = TRUE) } We then create a data.frame of all the year-month combinations: ( #--- create a set of year-month combinations to loop over ---# month_year_data &lt;- expand.grid(month = 1:12, year = 1990:2018) %&gt;% data.table() ) month year 1: 1 1990 2: 2 1990 3: 3 1990 4: 4 1990 5: 5 1990 --- 344: 8 2018 345: 9 2018 346: 10 2018 347: 11 2018 348: 12 2018 We now do parallelized loop over all the year-month combinations (by looping over the rows of the month_year_data): #--- run the above code in parallel ---# future_lapply( 1:nrow(month_year_data), function (x) get_save_prism(x, &quot;ppt&quot;) ) That’s it. Of course, you can do the same thing for tmax by this: #--- run the above code in parallel ---# future_lapply( 1:nrow(month_year_data), function (x) get_save_prism(x, &quot;tmax&quot;) ) Now that you have PRISM datasets, you can extract values from the raster layers for vector data for your analysis, which is covered extensively in Chapters 5 and 7 (for stars objects). If you want to save the data by year (each file would be about 168 Mb). You could do this. #--- define a function to download and save PRISM data stacked by year ---# get_save_prism_y &lt;- function(temp_year, var_type) { print(paste0(&quot;working on &quot;, temp_year)) #--- starting date of the working month-year ---# start_date &lt;- dmy(paste0(&quot;1/1/&quot;, temp_year)) #--- end date ---# end_date &lt;- dmy(paste0(&quot;1/1/&quot;, temp_year + 1)) - 1 #--- download daily PRISM data for the working month-year ---# get_prism_dailys( type = var_type, minDate = as.character(start_date), maxDate = as.character(end_date), keepZip = FALSE ) #--- list of dates of the working month-year ---# dates_ls &lt;- seq(start_date, end_date, &quot;days&quot;) #--- remove dashes ---# dates_prism_txt &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- folder names ---# folder_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil&quot;) #--- the file name of the downloaded data ---# file_name &lt;- paste0(&quot;PRISM_&quot;, var_type, &quot;_stable_4kmD2_&quot;, dates_prism_txt, &quot;_bil.bil&quot;) #--- complete path to the downloaded files ---# file_path &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- combine all the PRISM files as a RasterStack ---# temp_stars &lt;- stack(file_path) %&gt;% #--- convert to stars ---# st_as_stars() %&gt;% #--- set the third dimension as data ---# st_set_dimensions(&quot;band&quot;, values = dates_ls, name = &quot;date&quot;) #--- save the stars as an rds file ---# saveRDS( temp_stars, paste0(&quot;./Data/PRISM/PRISM_&quot;, var_type, &quot;_y&quot;, temp_year, &quot;.rds&quot;) ) #--- delete all the downloaded files ---# unlink(paste0(&quot;./Data/PRISM/&quot;, folder_name), recursive = TRUE) } #--- run the above code in parallel ---# future_lapply( 1990:2018, function (x) get_save_prism_y(x, &quot;tmax&quot;) ) References "],
["daymet-with-daymetr-and-feddata.html", "9.4 Daymet with daymetr and FedData", " 9.4 Daymet with daymetr and FedData For this section, we use the daymetr (Hufkens et al. 2018) and FedData packages (Bocinsky 2016). library(daymetr) library(FedData) Daymet data consists of “tiles,” each of which consisting of raster cells of 1km by 1km. Here is the map of the tiles (Figure 9.3) Figure 9.3: Daymet Tiles Here is the list of weather variables: vapor pressure minimum and maximum temperature snow water equivalent solar radiation precipitation day length So, Daymet provides information about more weather variables than PRISM, which helps to find some weather-dependent metrics, such as evapotranspiration. The easiest way find Daymet values for your vector data depends on whether it is points or polygons data. For points data, daymetr::download_daymet() is the easiest because it will directly return weather values to the point of interest. Internally, it finds the cell in which the point is located, and then return the values of the cell for the specified length of period. daymetr::download_daymet() does all this for you. For polygons, we need to first download pertinent Daymet data for the region of interest first and then extract values for each of the polygons ourselves, which we learned in Chapter 5. Unfortunately, the netCDF data downloaded from daymetr::download_daymet_ncss() cannot be easily read by the raster package nor the stars package. On the contrary, FedData::get_daymet() download the the requested Daymet data and save it as a RasterBrick object, which can be easily turned into stars object using st_as_stars(). 9.4.1 For points data For points data, the easiest way to associate daily weather values to them is to use download_daymet(). download_daymet() can download daily weather data for a single point at a time by finding which cell of a tile the point is located within. Here are key parameters for the function: lat: latitude lon: longitude start: start_year end: end_year internal: TRUE (dafault) or FALSE For example, the code below downloads daily weather data for a point (lat = \\(36\\), longitude = \\(-100\\)) starting from 2000 to 2002 and assigns the downloaded data to temp_daymet. #--- download daymet data ---# temp_daymet &lt;- download_daymet( lat = 36, lon = -100, start = 2000, end = 2002 ) #--- structure ---# str(temp_daymet) List of 7 $ site : chr &quot;Daymet&quot; $ tile : num 11380 $ latitude : num 36 $ longitude: num -100 $ altitude : num 746 $ tile : num 11380 $ data :&#39;data.frame&#39;: 1095 obs. of 9 variables: ..$ year : num [1:1095] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ... ..$ yday : num [1:1095] 1 2 3 4 5 6 7 8 9 10 ... ..$ dayl..s. : num [1:1095] 34560 34560 34560 34560 34560 ... ..$ prcp..mm.day.: num [1:1095] 0 0 0 0 0 0 0 0 0 0 ... ..$ srad..W.m.2. : num [1:1095] 317 298 282 262 314 ... ..$ swe..kg.m.2. : num [1:1095] 16 16 12 12 12 12 12 12 12 12 ... ..$ tmax..deg.c. : num [1:1095] 18 20 13.5 4 10 13 10 11 12.5 14.5 ... ..$ tmin..deg.c. : num [1:1095] -2.5 1.5 -2.5 -10 -9 -5 -9 -8.5 -7 -4.5 ... ..$ vp..Pa. : num [1:1095] 520 680 520 280 320 440 320 320 360 440 ... - attr(*, &quot;class&quot;)= chr &quot;daymetr&quot; As you can see, temp_daymet has bunch of site information other than the download Daymet data. #--- get the data part ---# temp_daymet_data &lt;- temp_daymet$data #--- take a look ---# head(temp_daymet_data) year yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c. 1 2000 1 34560.0 0 316.8 16 18.0 2 2000 2 34560.0 0 297.6 16 20.0 3 2000 3 34560.0 0 281.6 12 13.5 4 2000 4 34560.0 0 262.4 12 4.0 5 2000 5 34560.0 0 313.6 12 10.0 6 2000 6 34905.6 0 304.0 12 13.0 tmin..deg.c. vp..Pa. 1 -2.5 520 2 1.5 680 3 -2.5 520 4 -10.0 280 5 -9.0 320 6 -5.0 440 As you might have noticed, yday is not the date of each observation, but the day of the year. You can easily convert it into dates like this: temp_daymet_data &lt;- mutate(temp_daymet_data, date = as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)) One dates are obtained, you can use the lubridate package to extract day, month, and year using day(), month(), and year(), respectively. library(lubridate) temp_daymet_data &lt;- mutate(temp_daymet_data, day = day(date), month = month(date), #--- this is already there though ---# year = year(date) ) #--- take a look ---# dplyr::select(temp_daymet_data, year, month, day) %&gt;% head year month day 1 2000 1 1 2 2000 1 2 3 2000 1 3 4 2000 1 4 5 2000 1 5 6 2000 1 6 This helps you find group statistics like monthly precipitation. temp_daymet_data %&gt;% group_by(month) %&gt;% summarize(prcp = mean(prcp..mm.day.)) # A tibble: 12 x 2 month prcp &lt;dbl&gt; &lt;dbl&gt; 1 1 0.968 2 2 1.09 3 3 1.94 4 4 1.49 5 5 3.56 6 6 3.52 7 7 1.69 8 8 0.989 9 9 1.31 10 10 4.69 11 11 0.556 12 12 0.739 Downloading Daymet data for many points is just applying the same operations above to them using a loop. Let’s create random points within California and get their coordinates. random_points &lt;- maps::map(&quot;state&quot;, &quot;california&quot;, plot = FALSE, fill = TRUE) %&gt;% st_as_sf() %&gt;% #--- 10 points ---# st_sample(10) %&gt;% #--- get the coordinates ---# st_coordinates() %&gt;% #--- as tibble (data.frame) ---# as_tibble() %&gt;% #--- assign site id ---# mutate(site_id = 1:n()) To loop over the points, you can first write a function like this: get_daymet &lt;- function(i){ temp_lat &lt;- random_points[i, ] %&gt;% pull(Y) temp_lon &lt;- random_points[i, ] %&gt;% pull(X) temp_site &lt;- random_points[i, ] %&gt;% pull(site_id) temp_daymet &lt;- download_daymet( lat = temp_lat, lon = temp_lon, start = 2000, end = 2002 ) %&gt;% #--- just get the data part ---# .$data %&gt;% #--- convert to tibble (not strictly necessary) ---# as_tibble() %&gt;% #--- assign site_id so you know which record is for which site_id ---# mutate(site_id = temp_site) %&gt;% #--- get date from day of the year ---# mutate(date = as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)) return(temp_daymet) } Here is what the function returns for the 1st row of random_points: get_daymet(1) # A tibble: 1,095 x 11 year yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c. &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2000 1 33523. 0 230. 0 12 2 2000 2 33523. 0 240 0 11.5 3 2000 3 33523. 0 243. 0 13.5 4 2000 4 33523. 1 230. 0 13 5 2000 5 33523. 0 243. 0 14 6 2000 6 33523. 0 243. 0 13 7 2000 7 33523. 0 246. 0 14 8 2000 8 33869. 0 230. 0 13 9 2000 9 33869. 0 227. 0 12.5 10 2000 10 33869. 0 214. 0 14 # … with 1,085 more rows, and 4 more variables: tmin..deg.c. &lt;dbl&gt;, # vp..Pa. &lt;dbl&gt;, site_id &lt;int&gt;, date &lt;date&gt; You can now simply loop over the rows. ( daymet_all_points &lt;- lapply(1:nrow(random_points), get_daymet) %&gt;% #--- need to combine the list of data.frames into a single data.frame ---# bind_rows() ) # A tibble: 10,950 x 11 year yday dayl..s. prcp..mm.day. srad..W.m.2. swe..kg.m.2. tmax..deg.c. &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2000 1 33523. 0 230. 0 12 2 2000 2 33523. 0 240 0 11.5 3 2000 3 33523. 0 243. 0 13.5 4 2000 4 33523. 1 230. 0 13 5 2000 5 33523. 0 243. 0 14 6 2000 6 33523. 0 243. 0 13 7 2000 7 33523. 0 246. 0 14 8 2000 8 33869. 0 230. 0 13 9 2000 9 33869. 0 227. 0 12.5 10 2000 10 33869. 0 214. 0 14 # … with 10,940 more rows, and 4 more variables: tmin..deg.c. &lt;dbl&gt;, # vp..Pa. &lt;dbl&gt;, site_id &lt;int&gt;, date &lt;date&gt; Or better yet, you can easily parallelize this process as follows (see Chapter A if you are not familiar with parallelization in R): library(future.apply) library(parallel) #--- parallelization planning ---# plan(multiprocess, workers = detectCores() - 1) #--- parallelized lapply ---# daymet_all_points &lt;- future_lapply(1:nrow(random_points), get_daymet) %&gt;% #--- need to combine the list of data.frames into a single data.frame ---# bind_rows() 9.4.2 For polygons data Suppose you are interested in getting Daymet data for select counties in Michigan (Figure 9.4). #--- entire MI ---# MI_counties &lt;- maps::map(&quot;county&quot;, &quot;michigan&quot;, plot = FALSE, fill = TRUE) %&gt;% st_as_sf() #--- select counties ---# MI_counties_select &lt;- filter(MI_counties, ID %in% c(&quot;michigan,luce&quot;, &quot;michigan,chippewa&quot;, &quot;michigan,mackinac&quot;)) Figure 9.4: Select Michigan counties for which we download Daymet data We can use FedData::get_daymet() to download Daymet data that covers the spatial extent of the polygons data. The downloaded dataset can be assigned to an R object as RasterBrick (you could alternatively write the downloaded data to a file). In order to let the function know the spatial extent for which it download Daymet data, we supply a SpatialPolygonsDataFrame object supported by the sp package. Since our main vector data handling package is sf we need to convert the sf object to a sp object. The code below downloads prcp and tmax for the spatial extent of Michigan counties for 2000 and 2001: ( MI_daymet_select &lt;- FedData::get_daymet( #--- supply the vector data in sp ---# template = as(MI_counties_select, &quot;Spatial&quot;), #--- label ---# label = &quot;MI_counties_select&quot;, #--- variables to download ---# elements = c(&quot;prcp&quot;,&quot;tmax&quot;), #--- years ---# years = 2000:2001 ) ) $prcp class : RasterBrick dimensions : 96, 156, 14976, 730 (nrow, ncol, ncell, nlayers) resolution : 1000, 1000 (x, y) extent : 1027250, 1183250, 455500, 551500 (xmin, xmax, ymin, ymax) crs : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 source : memory names : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... min values : 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... max values : 6, 26, 19, 17, 6, 9, 6, 1, 0, 13, 15, 6, 0, 4, 7, ... $tmax class : RasterBrick dimensions : 96, 156, 14976, 730 (nrow, ncol, ncell, nlayers) resolution : 1000, 1000 (x, y) extent : 1027250, 1183250, 455500, 551500 (xmin, xmax, ymin, ymax) crs : +proj=lcc +lon_0=-100 +lat_0=42.5 +x_0=0 +y_0=0 +lat_1=25 +lat_2=60 +ellps=WGS84 source : memory names : X2000.01.01, X2000.01.02, X2000.01.03, X2000.01.04, X2000.01.05, X2000.01.06, X2000.01.07, X2000.01.08, X2000.01.09, X2000.01.10, X2000.01.11, X2000.01.12, X2000.01.13, X2000.01.14, X2000.01.15, ... min values : 0.5, -4.5, -8.0, -8.5, -7.0, -2.5, -6.5, -1.5, 1.5, 2.0, -1.5, -8.5, -14.0, -9.5, -3.0, ... max values : 3.0, 3.0, 0.5, -2.5, -2.5, 0.0, 0.5, 1.5, 4.0, 3.0, 3.5, 0.5, -6.5, -4.0, 0.0, ... As you can see, Daymet prcp and tmax data are stored separately as RasterBrick. Now that you have stars objects, you can extract values for your target polygons data using the methods described in Chapter 5. If you use the stars package for raster data handling (see Chapter 7), you can convert them into stars object using st_as_stars(). #--- tmax as stars ---# tmax_stars &lt;- st_as_stars(MI_daymet_select$tmax) #--- prcp as stars ---# prcp_stars &lt;- st_as_stars(MI_daymet_select$prcp) Now, the third dimension (band) is not recognized as dates. We can use st_set_dimension() to change that (see Chapter 7.5). Before that, we first need to recover Date values from the “band” values as follows: date_values &lt;- tmax_stars %&gt;% #--- get band values ---# st_get_dimension_values(., &quot;band&quot;) %&gt;% #--- remove X ---# gsub(&quot;X&quot;, &quot;&quot;, .) %&gt;% #--- convert to date ---# ymd(.) #--- take a look ---# head(date_values) [1] &quot;2000-01-01&quot; &quot;2000-01-02&quot; &quot;2000-01-03&quot; &quot;2000-01-04&quot; &quot;2000-01-05&quot; [6] &quot;2000-01-06&quot; #--- tmax ---# st_set_dimensions(tmax_stars, 3, values = date_values, names = &quot;date&quot; ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: X2000.01.01 Min. :-8.500 1st Qu.:-4.000 Median :-2.000 Mean :-1.884 3rd Qu.: 0.000 Max. : 3.000 NA&#39;s :198 dimension(s): from to offset delta refsys point x 1 156 1027250 1000 +proj=lcc +lon_0=-100 +la... NA y 1 96 551500 -1000 +proj=lcc +lon_0=-100 +la... NA date 1 730 NA NA Date NA values x NULL [x] y NULL [y] date 2000-01-01,...,2001-12-31 #--- prcp ---# st_set_dimensions(prcp_stars, 3, values = date_values, names = &quot;date&quot; ) stars object with 3 dimensions and 1 attribute attribute(s), summary of first 1e+05 cells: X2000.01.01 Min. : 0.000 1st Qu.: 0.000 Median : 3.000 Mean : 6.232 3rd Qu.:12.000 Max. :26.000 NA&#39;s :198 dimension(s): from to offset delta refsys point x 1 156 1027250 1000 +proj=lcc +lon_0=-100 +la... NA y 1 96 551500 -1000 +proj=lcc +lon_0=-100 +la... NA date 1 730 NA NA Date NA values x NULL [x] y NULL [y] date 2000-01-01,...,2001-12-31 Notice that the date dimension has NA for delta. This is because Daymet removes observations for December 31 in leap years to make the time dimension 365 consistently across years. This means that there is a one-day gap between “2000-12-30” and “2000-01-01” as you can see below: date_values[364:367] [1] &quot;2000-12-29&quot; &quot;2000-12-30&quot; &quot;2001-01-01&quot; &quot;2001-01-02&quot; References "],
["usgs-under-construction.html", "9.5 USGS (under construction)", " 9.5 USGS (under construction) Under construction "],
["sentinel-with-sen2r-under-construction.html", "9.6 Sentinel with sen2r (under construction)", " 9.6 Sentinel with sen2r (under construction) Under construction "],
["census-with-tidycensus-under-construction.html", "9.7 Census with tidycensus (under construction)", " 9.7 Census with tidycensus (under construction) Under construction "],
["par-comp.html", "A Loop and Parallel Computing ", " A Loop and Parallel Computing "],
["before-you-start-9.html", "Before you start", " Before you start Here we will learn how to program repetitive operations effectively and fast. We start from the basics of a loop for those who are not familiar with the concept. We then cover parallel computation using the future.lapply and parallel package. Those who are familiar with lapply() can go straight to Chapter A.2. Here are the specific learning objectives of this chapter. Learn how to use for loop and lapply() to complete repetitive jobs Learn how not to loop things that can be easily vectorized Learn how to parallelize repetitive jobs using the future_lapply() function from the future.apply package Direction for replication All the data in this Chapter is generated. Packages to install and load Run the following code to install or load (if already installed) the pacman package, and then install or load (if already installed) the listed package inside the pacman::p_load() function. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load( dplyr, # data wrangling data.table # data wrangling ) There are other packages that will be loaded during the demonstration. "],
["repetitive-processes-and-looping.html", "A.1 Repetitive processes and looping", " A.1 Repetitive processes and looping A.1.1 What is looping? We sometimes need to run the same process over and over again often with slight changes in parameters. In such a case, it is very time-consuming and messy to write all of the steps one bye one. For example, suppose you are interested in knowing the square of 1 through 5 in with a step of 1 (\\([1, 2, 3, 4, 5]\\)). The following code certainly works: 1^2 [1] 1 2^2 [1] 4 3^2 [1] 9 4^2 [1] 16 5^2 [1] 25 However, imagine you have to do this for 1000 integers. Yes, you don’t want to write each one of them one by one as that would occupy 1000 lines of your code, and it would be time-consuming. Things will be even worse if you need to repeat much more complicated processes like Monte Carlo simulations. So, let’s learn how to write a program to do repetitive jobs effectively using loop. Looping is repeatedly evaluating the same (except parameters) process over and over again. In the example above, the same process is the action of squaring. This does not change among the processes you run. What changes is what you square. Looping can help you write a concise code to implement these repetitive processes. A.1.2 For loop Here is how for loop works in general: #--- NOT RUN ---# for (x in a_list_of_values){ you do what you want to do with x } As an example, let’s use this looping syntax to get the same results as the manual squaring of 1 through 5: for (x in 1:5){ print(x^2) } [1] 1 [1] 4 [1] 9 [1] 16 [1] 25 Here, a list of values is \\(1, 2, 3, 4, 5]\\). For each value in the list, you square it (x^2) and then print it (print()). If you want to get the square of \\(1:1000\\), the only thing you need to change is the list of values to loop over as in: #--- evaluation not reported as it&#39;s too long ---# for (x in 1:1000){ print(x^2) } So, the length of the code does not depend on how many repeats you do, which is an obvious improvement over manual typing of every single process one by one. Note that you do not have to use \\(x\\) to refer to an object you are going to use. It could be any combination of letters as long as you use it when you code what you want to do inside the loop. So, this would work just fine, for (bluh_bluh_bluh in 1:5){ print(bluh_bluh_bluh^2) } [1] 1 [1] 4 [1] 9 [1] 16 [1] 25 A.1.3 For loop using the lapply() function You can do for loop using the lapply() function as well.102 Here is how it works: #--- NOT RUN ---# lapply(A,B) where \\(A\\) is the list of values you go through one by one in the order the values are stored, and \\(B\\) is the function you would like to apply to each of the values in \\(A\\). For example, the following code does exactly the same thing as the above for loop example. lapply(1:5, function(x){x^2}) [[1]] [1] 1 [[2]] [1] 4 [[3]] [1] 9 [[4]] [1] 16 [[5]] [1] 25 Here, \\(A\\) is \\([1, 2, 3, 4, 5]\\). In \\(B\\) you have a function that takes \\(x\\) and square it. So, the above code applies the function to each of \\([1, 2, 3, 4, 5]\\) one by one. In many circumstances, you can write the same looping actions in a much more concise manner using the lapply function than explicitly writing out the loop process as in the above for loop examples. You might have noticed that the output is a list. Yes, lapply() returns the outcomes in a list. That is where l in lapply() comes from. When the operation you would like to repeat becomes complicated (almost always the case), it is advisable that you create a function of that process first. #--- define the function first ---# square_it &lt;- function(x){ return(x^2) } #--- lapply using the pre-defined function ---# lapply(1:5, square_it) [[1]] [1] 1 [[2]] [1] 4 [[3]] [1] 9 [[4]] [1] 16 [[5]] [1] 25 Finally, it is a myth that you should always use lapply() instead of the explicit for loop syntax because lapply() (or other apply() families) is faster. They are basically the same.103 A.1.4 Looping over multiple variables using lapply() lapply() allows you to loop over only one variable. However, it is often the case that you want to loop over multiple variables104. However, it is easy to achieve this. The trick is to create a data.frame of the variables where the complete list of the combinations of the variables are stored, and then loop over row of the data.frame. As an example, suppose we are interested in understanding the sensitivity of corn revenue to corn price and applied nitrogen amount. We consider the range of $3.0/bu to $5.0/bu for corn price and 0 lb/acre to 300/acre for applied nitrogen applied. #--- corn price vector ---# corn_price_vec &lt;- seq(3, 5, by = 1) #--- nitrogen vector ---# nitrogen_vec &lt;- seq(0, 300, by = 100) After creating vectors of the parameters, you combine them to create a complete combination of the parameters using the expand.grid() function, and then convert it to a data.frame object105. #--- crate a data.frame that holds parameter sets to loop over ---# parameters_data &lt;- expand.grid(corn_price = corn_price_vec, nitrogen = nitrogen_vec) %&gt;% #--- convert the matrix to a data.frame ---# data.frame() #--- take a look ---# parameters_data corn_price nitrogen 1 3 0 2 4 0 3 5 0 4 3 100 5 4 100 6 5 100 7 3 200 8 4 200 9 5 200 10 3 300 11 4 300 12 5 300 We now define a function that takes a row number, refer to parameters_data to extract the parameters stored at the row number, and then calculate corn yield and revenue based on the extracted parameters. gen_rev_corn &lt;- function(i) { #--- define corn price ---# corn_price &lt;- parameters_data[i,&#39;corn_price&#39;] #--- define nitrogen ---# nitrogen &lt;- parameters_data[i,&#39;nitrogen&#39;] #--- calculate yield ---# yield &lt;- 240 * (1 - exp(0.4 - 0.02 * nitrogen)) #--- calculate revenue ---# revenue &lt;- corn_price * yield #--- combine all the information you would like to have ---# data_to_return &lt;- data.frame( corn_price = corn_price, nitrogen = nitrogen, revenue = revenue ) return(data_to_return) } This function takes \\(i\\) (act as a row number within the function), extract corn price and nitrogen from the \\(i\\)th row of parameters_mat, which are then used to calculate yield and revenue106. Finally, it returns a data.frame of all the information you used (the parameters and the outcomes). #--- loop over all the parameter combinations ---# rev_data &lt;- lapply(1:nrow(parameters_data), gen_rev_corn) #--- take a look ---# rev_data [[1]] corn_price nitrogen revenue 1 3 0 -354.1138 [[2]] corn_price nitrogen revenue 1 4 0 -472.1517 [[3]] corn_price nitrogen revenue 1 5 0 -590.1896 [[4]] corn_price nitrogen revenue 1 3 100 574.6345 [[5]] corn_price nitrogen revenue 1 4 100 766.1793 [[6]] corn_price nitrogen revenue 1 5 100 957.7242 [[7]] corn_price nitrogen revenue 1 3 200 700.3269 [[8]] corn_price nitrogen revenue 1 4 200 933.7692 [[9]] corn_price nitrogen revenue 1 5 200 1167.212 [[10]] corn_price nitrogen revenue 1 3 300 717.3375 [[11]] corn_price nitrogen revenue 1 4 300 956.4501 [[12]] corn_price nitrogen revenue 1 5 300 1195.563 Successful! Now, for us to use the outcome for other purposes like further analysis and visualization, we would need to have all the results combined into a single data.frame instead of a list of data.frames. To do this, use either bind_rows() from the dplyr package or rbindlist() from the data.table package. #--- bind_rows ---# bind_rows(rev_data) corn_price nitrogen revenue 1 3 0 -354.1138 2 4 0 -472.1517 3 5 0 -590.1896 4 3 100 574.6345 5 4 100 766.1793 6 5 100 957.7242 7 3 200 700.3269 8 4 200 933.7692 9 5 200 1167.2115 10 3 300 717.3375 11 4 300 956.4501 12 5 300 1195.5626 #--- rbindlist ---# rbindlist(rev_data) corn_price nitrogen revenue 1: 3 0 -354.1138 2: 4 0 -472.1517 3: 5 0 -590.1896 4: 3 100 574.6345 5: 4 100 766.1793 6: 5 100 957.7242 7: 3 200 700.3269 8: 4 200 933.7692 9: 5 200 1167.2115 10: 3 300 717.3375 11: 4 300 956.4501 12: 5 300 1195.5626 A.1.5 Do you really need to loop? Actually, we should not have used for loop or lapply() in any of the examples above in practice107 This is because they can be easily vectorized. Vectorized operations are those that take vectors as inputs and work on each element of the vectors in parallel108. A typical example of a vectorized operation would be this: #--- define numeric vectors ---# x &lt;- 1:1000 y &lt;- 1:1000 #--- element wise addition ---# z_vec &lt;- x + y A non-vectorized version of the same calculation is this: z_la &lt;- lapply(1:1000, function(i) x[i] + y[i]) %&gt;% unlist #--- check if identical with z_vec ---# all.equal(z_la, z_vec) [1] TRUE Both produce the same results. However, R is written in a way that is much better at doing vectorized operations. Let’s time them using the microbenchmark() function from the microbenchmark package. Here, we do not unlist() after lapply() to just focus on the multiplication part. library(microbenchmark) microbenchmark( #--- vectorized ---# &quot;vectorized&quot; = { x + y }, #--- not vectorized ---# &quot;not vectorized&quot; = { lapply(1:1000, function(i) x[i] + y[i])}, times = 100, unit = &quot;ms&quot; ) Unit: milliseconds expr min lq mean median uq max neval vectorized 0.002657 0.0032925 0.00498103 0.004842 0.0053465 0.016967 100 not vectorized 0.516832 0.5274390 0.57548362 0.534773 0.5419155 2.881145 100 As you can see, the vectorized version is faster. The time difference comes from R having to conduct many more internal checks and hidden operations for the non-vectorized one109. Yes, we are talking about a fraction of milliseconds here. But, as the objects to operate on get larger, the difference between vectorized and non-vectorized operations can become substantial110. The lapply() examples can be easily vectorized. Instead of this: lapply(1:1000 square_it) You can just do this: square_it(1:1000) You can also easily vectorize the revenue calculation demonstrated above. First, define the function differently so that revenue calculation can take corn price and nitrogen vectors and return a revenue vector. gen_rev_corn_short &lt;- function(corn_price, nitrogen) { #--- calculate yield ---# yield &lt;- 240 * (1 - exp(0.4 - 0.02 * nitrogen)) #--- calculate revenue ---# revenue &lt;- corn_price * yield return(revenue) } Then use the function to calculate revenue and assign it to a new variable in the parameters_data data. rev_data_2 &lt;- mutate( parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen) ) Let’s compare the two: microbenchmark( #--- vectorized ---# &quot;vectorized&quot; = { rev_data &lt;- mutate(parameters_data, revenue = gen_rev_corn_short(corn_price, nitrogen)) }, #--- not vectorized ---# &quot;not vectorized&quot; = { parameters_data$revenue &lt;- lapply(1:nrow(parameters_data), gen_rev_corn) }, times = 100, unit = &quot;ms&quot; ) Unit: milliseconds expr min lq mean median uq max neval vectorized 1.237152 1.310964 1.564188 1.440602 1.677993 8.388844 100 not vectorized 2.110333 2.210324 2.496429 2.300123 2.956518 3.276768 100 Yes, the vectorized version is faster. So, the lesson here is that if you can vectorize, then vectorize instead of using lapply(). But, of course, things cannot be vectorized in many cases. lpply() in only one of the family of apply() functions. We do not talk about other types of apply() functions here (e.g., apply(), spply(), mapply(),, tapply()). Personally, I found myself only rarely using them. But, if you are interested in learning those, take a look at here or here.↩︎ Check this discussion on StackOverflow. You might want to check out this video at 6:10 as well.↩︎ the map() function from the purrr package allows you to loop over two variable.↩︎ Converting to a data.frame is not strictly necessary.↩︎ Yield is generated based on the Mitscherlich-Baule functional form. Yield increases at the decreasing rate as you apply more nitrogen, and yield eventually hits the plateau.↩︎ By the way, note that lapply() is no magic. It’s basically a for loop and not rally any faster than for loop.↩︎ This does not mean that the process is parallelized by using multiple cores.↩︎ See this or this to have a better understanding of why non-vectorized operations can be slower than vectorized operations.↩︎ See here for a good example of such a case. R is often regarded very slow compared to other popular software. But, many of such claims come from not vectorizing operations that can be vectorized. Indeed, many of the base and old R functions are written in C. More recent functions relies on C++ via the Rcpp package.↩︎ "],
["parcomp.html", "A.2 Parallelization of embarrassingly parallel processes", " A.2 Parallelization of embarrassingly parallel processes Parallelization of computation involves distributing the task at hand to multiple cores so that multiple processes are done in parallel. Here, we learn how to parallelize computation in R. Our focus is on the so called embarrassingly parallel processes. Embarrassingly parallel processes refer to a collection of processes where each process is completely independent of any another. That is, one process does not use the outputs of any of the other processes. The example of integer squaring is embarrassingly parallel. In order to calculate \\(1^2\\), you do not need to use the result of \\(2^2\\) or any other squares. Embarrassingly parallel processes are very easy to parallelize because you do not have to worry about which process to complete first to make other processes happen. Fortunately, most of the processes you are interested in parallelizing fall under this category111. We will use the future.apply package for parallelization112. Using the package, parallelizing is really a piece of cake as it is basically the same syntactically as lapply(). #--- load packages ---# library(future.apply) You can find out how many cores you have available for parallel computation on your computer using the get_num_procs() function from the RhpcBLASctl package. library(RhpcBLASctl) #--- number of all cores ---# get_num_procs() [1] 16 Before we implement parallelized lapply(), we need to declare what backend process we will be using by plan(). Here, we use plan(multiprocess)113. In the plan() function, we can specify the number of workers. Here I will use the total number of cores less 1114. plan(multiprocess, workers = get_num_procs() - 1) future_lapply() works exactly like lapply(). sq_ls &lt;- future_lapply(1:1000, function(x) x^2) This is it. The only difference you see from the serialized processing using lapply() is that you changed the function name to future_lapply(). Okay, now we know how we parallelize computation. Let’s check how much improvement in implementation time we got by parallelization. microbenchmark( #--- parallelized ---# &quot;parallelized&quot; = { sq_ls &lt;- future_lapply(1:1000, function(x) x^2) }, #--- non-parallelized ---# &quot;not parallelized&quot; = { sq_ls &lt;- lapply(1:1000, function(x) x^2) }, times = 100, unit = &quot;ms&quot; ) Unit: milliseconds expr min lq mean median uq parallelized 94.067704 100.163816 111.7353122 102.1741575 129.437737 not parallelized 0.403711 0.430872 0.5416883 0.4650985 0.680966 max neval 148.87072 100 2.10543 100 Hmmmm, okay, so parallelization made the code slower… How could this be? This is because communicating jobs to each core takes some time as well. So, if each of the iterative processes is super fast (like this example where you just square a number), the time spent on communicating with the cores outweighs the time saving due to parallel computation. Parallelization is more beneficial when each of the repetitive processes takes long. One of the very good use cases of parallelization is MC simulation. The following MC simulation tests whether the correlation between an independent variable and error term would cause bias (yes, we know the answer). The MC_sim function first generates a dataset (50,000 observations) according to the following data generating process: \\[ y = 1 + x + v \\] where \\(\\mu \\sim N(0,1)\\), \\(x \\sim N(0,1) + \\mu\\), and \\(v \\sim N(0,1) + \\mu\\). The \\(\\mu\\) term cause correlation between \\(x\\) (the covariate) and \\(v\\) (the error term). It then estimates the coefficient on \\(x\\) vis OLS, and return the estimate. We would like to repeat this process 1,000 times to understand the property of the OLS estimators under the data generating process. This Monte Carlo simulation is embarrassingly parallel because each process is independent of any other. #--- repeat steps 1-3 B times ---# MC_sim &lt;- function(i){ N &lt;- 50000 # sample size #--- steps 1 and 2: ---# mu &lt;- rnorm(N) # the common term shared by both x and u x &lt;- rnorm(N) + mu # independent variable v &lt;- rnorm(N) + mu # error y &lt;- 1 + x + v # dependent variable data &lt;- data.table(y = y, x = x) #--- OLS ---# reg &lt;- lm(y~x, data = data) # OLS #--- return the coef ---# return(reg$coef[&#39;x&#39;]) } Let’s run one iteration, tic() MC_sim(1) toc() x 1.503353 elapsed 0.023 Okay, so it takes 0.023 second for one iteration. Now, let’s run this 1000 times with or without parallelization. Not parallelized #--- non-parallel ---# tic() MC_results &lt;- lapply(1:1000, MC_sim) toc() elapsed 21.399 Parallelized #--- parallel ---# tic() MC_results &lt;- future_lapply(1:1000, MC_sim) toc() elapsed 2.637 As you can see, parallelization makes it much quicker with a noticeable difference in elapsed time. We made the code 8.11 times faster. However, we did not make the process 15 times faster even though we used 15 cores for the parallelized process. This is because of the overhead associated with distributing tasks to the cores. The relative advantage of parallelization would be greater if each iteration took more time. For example, if you are running a process that takes about 2 minutes for 1000 times, it would take approximately 33 hours and 20 minutes. But, it may take only 4 hours if you parallelize it on 15 cores, or maybe even 2 hours if you run it on 30 cores. A.2.1 Mac or Linux users For Mac users, parallel::mclapply() is just as compelling (or pbmclapply::pbmclapply() if you want to have a nice progress report, which is very helpful particularly when the process is long). It is just as easy to use as future_lapply() because its syntax is the same as lapply(). You can control the number of cores to employ by adding mc.cores option. Here is an example code that does the same MC simulations we conducted above: #--- mclapply ---# library(parallel) MC_results &lt;- mclapply(1:1000, MC_sim, mc.cores = get_num_procs() - 1) #--- or with progress bar ---# library(pbmclapply) MC_results &lt;- pbmclapply(1:1000, MC_sim, mc.cores = get_num_procs() - 1) A good example of non-embarrassingly parallel process is dynamic optimization via backward induction. You need to know the optimal solution at \\(t = T\\), before you find the optimal solution at \\(t = T-1\\).↩︎ There are many other options including the parallel, foreach packages.↩︎ If you are a Mac or Linux user, then the multicore process will be used, while the multisession process will be used if you are using a Windows machine. The multicore process is superior to the multisession process. See this lecture note on parallel programming using R by Dr. Grant McDermott’s at the University of Oregon for the distinctions between the two and many other useful concepts for parallelization. At the time of this writing, if you run R through RStudio, multiprocess option is always redirected to multisession option because of the instability in doing multiprocess. If you use Linux or Mac and want to take the full advantage of future_apply, you should not run your R programs through RStudio at least for now.↩︎ This way, you can have one more core available to do other tasks comfortably. However, if you don’t mind having your computer completely devoted to the processing task at hand, then there is no reason not to use all the cores.↩︎ "],
["ggplot2-minimals.html", "B ggplot2 minimals", " B ggplot2 minimals Note: This section does not provide a complete treatment of the basics of the ggplot2 package. Rather, it provides the minimal knowledge of the package so that readers who are not familiar with the package can still understand the codes for map making presented in Chapter 8. The ggplot2 package is a general and extensive data visualization tool. It is very popular among R users due to its elegance in and ease of use in generating high-quality figures. The ggplot2 package is designed following the “grammar of graphics,” which makes it possible to visualize data in an easy and consistent manner irrespective of the type of figures generated, whether it is a simple scatter plot or a complicated map. This means that learning the basics of how ggplot2 works directly helps in creating maps as well. This chapter goes over the basics of how ggplot2 works in general. In ggplot2, you first specify what data to use and then specify how to use the data for visualization depending on what types of figures you intend to make using geom_*(). As a simple example, let’s use mpg data to create a simple scatter plot. Here is what mpg dataset looks like: mpg # A tibble: 234 x 11 manufacturer model displ year cyl trans drv cty hwy fl class &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 1 audi a4 1.8 1999 4 auto(l… f 18 29 p comp… 2 audi a4 1.8 1999 4 manual… f 21 29 p comp… 3 audi a4 2 2008 4 manual… f 20 31 p comp… 4 audi a4 2 2008 4 auto(a… f 21 30 p comp… 5 audi a4 2.8 1999 6 auto(l… f 16 26 p comp… 6 audi a4 2.8 1999 6 manual… f 18 26 p comp… 7 audi a4 3.1 2008 6 auto(a… f 18 27 p comp… 8 audi a4 quat… 1.8 1999 4 manual… 4 18 26 p comp… 9 audi a4 quat… 1.8 1999 4 auto(l… 4 16 25 p comp… 10 audi a4 quat… 2 2008 4 manual… 4 20 28 p comp… # … with 224 more rows The code below creates a scatter plot of displ and hwy variables in the mpg dataset. ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = class)) Figure B.1: Scatter plot where observations are color-differentiated by class However, this one does not work because color = class is outside of aes() and R does not look for class object inside mpg. ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy), color = class) Error in rep(value[[k]], length.out = n): attempt to replicate an object of type &#39;builtin&#39; You can still specify the color that is applied universally to all the observations in the dataset like this: ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy), color = &quot;blue&quot;) These examples should clarify what aes() does: it makes the aesthetics of the figure data-dependent. In the code to create Figure B.1, the default color option was used for color-differentiation by class. You can specify the color scheme using scale_*(). The scale_*() function generally takes the form o fscale_x_y(), where x is the type of aesthetics you want to control, and y is the method for specifying the color scheme. For example, in the code above, the type of aesthetics is color. And suppose we would like to use the brewer method. Then the scale function we should be using is scale_color_brewer(). The code below uses scale_color_brewer() and the palette option to specify the color scheme by ourselves. ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = class)) + scale_color_brewer(palette = 1) Figure B.2: Scatter plot where the color scheme is defined by the user As you can see the color scheme is now changed. There are many other different types of pallets available. See ask Uri kun . To create a different type of figure than scatter plot, you can pick a different geom_*(). For example, geom_histogram() creates a histogram. ggplot(data = mpg) + geom_histogram(aes(x = hwy), color = &quot;blue&quot;, fill = &quot;white&quot;) You can save a created figure (or more precisely the data underpins the figure) as an R object as follows: #--- save the figure to g_plot ---# g_plot &lt;- ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = class)) + scale_color_brewer(palette = 1) #--- see the class ---# class(g_plot) [1] &quot;gg&quot; &quot;ggplot&quot; You can call the saved object to see the figure. g_plot Another important feature of ggplot2 is that you can add layers to an existing ggplot object by + geom_*(). For example, the following code adds the linear regression line to the plot: g_plot + geom_smooth(aes(x = displ, y = hwy), method = &quot;lm&quot;) This feature makes it very easy to plot different spatial objects in a single map as we will find out later. “Faceting” is another useful feature of the package. Faceting splits the data into groups and generates a figure for each group where the aesthetics of the figures are consistent across the groups. Faceting can be done using facet_wrap() or facet_grid(). Here is an example using facet_wrap(): ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = class)) + geom_smooth(aes(x = displ, y = hwy), method = &quot;lm&quot;) + scale_color_brewer(palette = 1) + facet_wrap(year ~ .) year ~ . inside facet_wrap() tells R to split the data by year. The . in year ~ . means “no variable.”115 So, the above code splits the mpg data by year, applies the geom_point() and geom_smooth(), applies scale_color_brewer() to each of them, and then creates a figure for each group. The created figures are then presented side-by-side.116 This feature can be handy, for example, when you would like to display changes in land use over time where faceting is done by year. While there are other important ggplot2 features to be aware of to make informative maps, I will not discuss them here. Rather, I will introduce them when they first appear in the lecture through examples. For those who are interested in learning the basics of ggplot2, there are numerous books written about it on the market. Some prominent ones are You can do two-way splits by supplying another categorical variable instead of . in year ~ ..↩︎ You can change the orientation by using facet_wrap(. ~ year).↩︎ "],
["references.html", "References", " References Bocinsky, R. Kyle. 2016. FedData: Functions to Automate Downloading Geospatial Data Available from Several Federated Data Sources. http://CRAN.R-project.org/package=FedData. Chen, Bowen. 2020. CropScapeR: Access Cropland Data Layer Data via the ’Cropscape’ Web Service. Gaure, Simen. 2013. “Lfe: Linear Group Fixed Effects.” The R Journal 5 (2): 104–17. Hart, Edmund M., and Kendon Bell. 2015. Prism: Download Data from the Oregon Prism Project. https://doi.org/10.5281/zenodo.33663. Hufkens, Koen, David Basler, Tom Milliman, Eli K. Melaas, and Andrew D. Richardson. 2018. “An Integrated Phenology Modelling Framework in R: Modelling Vegetation Phenology with phenor.” Methods in Ecology &amp; Evolution 9: 1–10. http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12970/full. Lindblad, Brad. 2020. tidyUSDA: A Minimal Tool Set for Gathering Usda Quick Stat Data for Analysis and Visualization. https://bradlindblad.github.io/tidyUSDA/. Pebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” R Journal 10 (1). ———. 2020. Stars: Spatiotemporal Arrays, Raster and Vector Data Cubes. https://CRAN.R-project.org/package=stars. Wickham, Hadley, and Jay Hesselberth. 2020. Pkgdown: Make Static Html Documentation for a Package. https://CRAN.R-project.org/package=pkgdown. Xie, Yihui. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/bookdown. Xie, Yihui, Alison Presmanes Hill, and Amber Thomas. 2017. Blogdown: Creating Websites with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://github.com/rstudio/blogdown. "]
]
