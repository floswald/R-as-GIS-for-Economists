[
["index.html", "R as GIS for Economists Welcome", " R as GIS for Economists Taro Mieno 2020-05-14 Welcome This book is being developed as part of my effort to put together course materials for my data science course targeted at upper-level undergraduate and graduate students at the University of Nebraska Lincoln. This books aims particularly at spatial data processing for an econometric project, where spatial variables become part of an econometric analysis. Over the years, I have seen so many students and researchers who spend so much time just processing spatial data (often involving clicking the ArcGIS (or QGIS) user interface to death), which is a waste of time from the perspective of academic productivity. My hope is that this book will help researchers become more proficient in spatial data processing and enhance the overall productivity of the fields of economics for which spatial data are essential. About me I am an Assistant Professor at the Department of Agricultural Economics at University of Nebraska Lincoln, where I also teach Econometrics for Master’s students. My research interests lie in precision agriculture, water economics, and agricultural policy. My personal website is here. Comments and Suggestions? Any constructive comments and suggestions about how I can improve the book are all welcome. Please send me an email at tmieno2@unl.edu or create an issue on the github page of this book. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
<<<<<<< HEAD
["preface.html", "Preface Why R as GIS for Economists? How is this book different from other online books and resources? What is going to be covered in this book? Conventions of the book and some notes Session Information", " Preface Why R as GIS for Economists? R has extensive capabilities as GIS software. In my opinion, \\(99\\%\\) of your spatial data processing needs as an economist will be satisfied by R. But, there are several popular options for GIS tasks other than R: Python ArcGIS QGIS Here I compare them briefly and discuss why R is a good option. R vs Python Both R and Python are actually heavily dependent on open source software GDAL and GEOS for their core GIS operations (GDAL for reading spatial data, and GEOS for geometrical operations like intersecting two spatial layers).1 So, when you run GIS tasks on R or Python you basically tell R or Python what you want to do and they talk to the software, let it do the job, and return the results to you. This means that R and Python are much different in their capability at GIS tasks as they are dependent on the common open source software for many GIS tasks. When GDAL and GEOS get better, R and Python get better (with a short lag). Both of them have good spatial visualization tools as well. Moreover, both R and Python can communicate with QGIS and ArcGIS (as long you as have them installed of course) and use their functionalities from within R and Python via the bridging packages: RQGIS and PyQGIS for QGIS, and R-ArcGIS and ArcPy.2 So, if you are more familiar with Python than R, go ahead and go with Python. From now on, my discussions assume that you are going for the R option, as otherwise, you would not be reading the rest of the book anyway. R vs ArcGIS or QGIS ArcGIS is commercial software and it is quite expensive (you are likely to be able to get a significant discount if you are a student at or work for a University). On the other hand, QGIS is open source and free. It has seen significant development over the decade, and I would say it is just as competitive as ArcGIS. QGIS also uses open source geospatial software GDAL, GEOS, and others (SAGA, GRASS GIS). Both of them have a graphical interface that helps you implement various GIS tasks unlike R which requires programming. Now, since R can use ArcGIS and QGIS through the bridging packages, a more precise question we should be asking is whether you should program GIS tasks using R (possibly using the bridging packages) or manually implement GIS tasks using the graphical interface of ArcGIS or QGIS. The answer is programming GIS tasks using R. First, manual GIS operations are hard to repeat. It is often the case that in the course of a project you need to redo the same GIS task except that the underlying datasets have changed. If you have programmed the process with R, you just run the same code and that’s it. You get the desired results. If you did not program it, you need to go through many clicks on the graphical interface all over again, potentially trying to remember how you actually did it the last time.3 Second and more important, manual operations are not scalable. It has become much more common that we need to process many large spatial datasets. Imagine you are doing the same operations on \\(1,000\\) files using a graphical interface, or even \\(50\\) files. Do you know what is good at doing the same tasks over and over again without complaining? A computer. Just let it do what it likes to do. You have better things do. Finally, should you learn ArcGIS or QGIS in addition to (or before) R? I am doubtful. As economists, the GIS tasks we need to do are not super convoluted most of the time. Suppose \\(\\Omega_R\\) and \\(\\Omega_{AQ}\\) represent the set of GIS tasks R and \\(ArcGIS/QGIS\\) can implement, respectively. Further, let \\(\\Omega_E\\) represent the set of skills economists need to implement. Then, \\(\\Omega_E \\in \\Omega_R\\) \\(99\\%\\) (or maybe \\(95\\%\\) to be safe) of the time and \\(\\Omega_E \\not\\subset \\Omega_{AQ}\\setminus\\Omega_R\\) \\(99\\%\\) of the time. Personally, I have never had to rely on either ArcGIS or QGIS for my research projects after I learned how to use R as GIS. One of the things ArcGIS and QGIS can do but R cannot do (\\(\\Omega_{AQ}\\setminus\\Omega_R\\)) is create spatial objects by hand using a graphical user interface, like drawing polygons and lines. Another thing that R lags behind ArcGIS and QGIS is 3D data visualization. But, I must say neither of them is essential for economists at the moment. Finally, sometime it is easier and faster to make a map using ArcGIS and QGIS especially for a complicated map.4 Summary You have never used any GIS software? Learn R first. If you find out you really cannot complete the tasks you would like to do using R, then turn to other options. You have used ArcGIS or QGIS and do not like them because they crash often? Why don’t you try R?5 You may realize you actually do not need them. You have used ArcGIS or QGIS before and are very comfortable with them, but you need to program repetitive GIS tasks? Learn R and maybe take advantage of R-ArcGIS or RQGIS, which this book does not cover. You know for sure that you need to run only a simple GIS task once and never have to do any GIS tasks ever again? Stop reading and ask one of your friends to do the job. Pay him/her \\(\\$20\\) per hour, which is way below the opportunity cost of setting up either ArcGIS or QGI and learning to do that simple task on them. How is this book different from other online books and resources? We are seeing an explosion of online (and free) resources that teach how to use R for spatial data processing.6 Here is an incomplete list of such resources: Geocomputation with R Spatial Data Science Spatial Data Science with R Introduction to GIS using R Code for An Introduction to Spatial Analysis and Mapping in R Introduction to GIS in R Intro to GIS and Spatial Analysis Introduction to Spatial Data Programming with R Reproducible GIS analysis with R R for Earth-System Science Rspatial NEON Data Skills Simple Features for R Thanks to all these resources, it has become much easier to self-teach R for GIS work than six or seven years ago when I first started using R for GIS. Even though I have not read through all these resources carefully, I am pretty sure every topic found in this book can also be found somewhere in these resources (except the demonstrations). So, you may wonder why on earth you can benefit from reading this book. It all boils down to search costs. Researchers in different disciplines require different sets of spatial data skills. The available resources are either very general covering so many topics that economists are very unlikely to use. It is particularly hard for those who do not have much experience in GIS to identify whether particular skills are essential or not. So, they could spend so much time learning something that is not really useful. The value of this book lies in its deliberate incomprehensiveness. It only packages materials that satisfy the need of most economists, cutting out many topics that are likely to be of limited use for economists. For those who are looking for more comprehensive treatments of spatial data handling and processing in one book, I personally like Geocomputation with R a lot. Increasingly, the developer of R packages created a website dedicated to their R packages, where you can often find vignettes (tutorials), like Simple Features for R. What is going to be covered in this book? The book starts with the very basics of spatial data handling (e.g., importing and exporting spatial datasets) and moves on to more practical spatial data operations (e.g., spatial data join) that are useful for research projects. This books is still under development. Right now, only Chapter 1 is available. I will work on the rest of the book over the summer. The “coming soon” chapters are close to be done. I just need to add finishing touches to those chapters. The “wait a bit” chapters need some more work, adding contents, etc. Chapter 1: Demonstrations of R as GIS (available) groundwater pumping and groundwater level precision agriculture land use and weather corn planted acreage and railroads groundwater pumping and weather Chapter 2: The basics of vector data handling using sf package (coming soon) spatial data structure in sf import and export vector data (re)projection of spatial datasets single-layer geometrical operations (e.g., create buffers, find centroids) other miscellaneous basic operations Chapter 3: Spatial interactions of vector datasets (coming soon) spatially subsetting one layer based on another layer extracting values from one layer to another layer7 Chapter 4: The basics of raster data handling using raster and terra packages (coming soon) import and export raster data stacking raster data Chapter 5: Spatial interactions of vector and raster datasets (wait a bit) extracting values from a raster layer to a vector layer Chapter 6: Efficient spatial data processing (wait a bit) parallelization Chapter 7: Downloading publicly available spatial datasets (wait a bit) Sentinel 2 (sen2r) USDA NASS QuickStat (tidyUSDA) PRISM (prism) Daymet (daymetr) USGS (dataRetrieval) Chapter 8: Parallel computation (wait a bit) As you can see above, this book does not spend any time on the very basics of GIS concepts. Before you start reading the book, you should know the followings at least (it’s not much): What Geographic Coordinate System (GCS), Coordinate Reference System (CRS), and projection are (this is a good resource) Distinctions between vector and raster data (this is a simple summary of the difference) Finally, this book does not cover spatial statistics or spatial econometrics at all. This book is about spatial data processing. Spatial analysis is something you do after you have processed spatial data. Conventions of the book and some notes Here are some notes of the conventions of this book and notes for R beginners and those who are not used to reading rmarkdown-generated html documents. Texts in gray boxes They are one of the following: objects defined on R during demonstrations R functions R packages When it is a function, I always put parentheses at the end like this: st_read().8 Sometimes, I combine a package and function in one like this: sf::st_read(). This means it is a function called st_read() from the sf package. Colored Boxes Codes are in blue boxes, and outcomes are in red boxes. Codes: runif(5) Outcomes: ## [1] 0.8593125 0.4995141 0.7393268 0.2941384 0.5333933 Parentheses around codes Sometimes you will see codes enclosed by parenthesis like this: ( a &lt;- runif(5) ) ## [1] 0.942510175 0.467471505 0.366884113 0.002588629 0.057113403 The parentheses prints what’s inside of a newly created object (here a) without explicitly evaluating the object. So, basically I am signaling that we will be looking inside of the object that was just created. This one prints nothing. a &lt;- runif(5) Footnotes Footnotes appear at the bottom of the page. You can easily get to a footnote by clicking on the footnote number. You can also go back to the main narrative where the footnote number is by clicking on the curved arrow at the end of the footnote. So, don’t worry about having to scroll all the way up to where you were after reading footnotes. Session Information Here is the session information when compiling the book: sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Mojave 10.14.1 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.0 magrittr_1.5 bookdown_0.18 tools_4.0.0 ## [5] htmltools_0.4.0 rstudioapi_0.11 yaml_2.2.1 Rcpp_1.0.4.6 ## [9] stringi_1.4.6 rmarkdown_2.1 knitr_1.28 stringr_1.4.0 ## [13] xfun_0.13 digest_0.6.25 rlang_0.4.6 evaluate_0.14 For example, see the very first sentence of this page↩ We do not learn them in this lecture note because I do not see the benefits of using them.↩ You could take a step-by-step note of what you did though.↩ Let me know if you know something that is essential for economists that only ArcGIS or QGIS can do. I will add that to the list here.↩ I am not saying R does not crash. R does crash. But, often times, the fault is yours, rather than the software’s.↩ This phenomenon is largely thanks to packages like bookdown (Xie 2016), blogdown (Xie, Hill, and Thomas 2017), and pkgdown (Wickham and Hesselberth 2020) that has lowered the cost of professional contents creation much much lower than before. Indeed, this book was built taking advantage of the bookdown package.↩ over function in sp language↩ This is a function that draws values randomly from the uniform distribution.↩ "],
["demo.html", "Chapter 1 R as GIS: Demonstrations Before you start 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table 1.2 Demonstration 2: Precision Agriculture 1.3 Demonstration 3: Land Use and Weather 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage 1.5 Demonstration 5: Groundwater use for agricultural irrigation", " Chapter 1 R as GIS: Demonstrations Before you start The primary objective of this chapter is to showcase the power of R as GIS through demonstrations using mock-up econometric research projects9. Each project consists of a project overview (objective, datasets used, econometric model, and GIS tasks involved) and demonstration. This is really not a place you learn the nuts and bolts of how R does spatial operations. Indeed, we intentionally do not explain all the details of how the R codes work. We reiterate that the main purpose of the demonstrations is to get you a better idea of how R can be used to process spatial data to help your research projects involving spatial datasets. Finally, note that these mock-up projects use extremely simple econometric models that completely lacks careful thoughts you would need in real research projects. So, don’t waste your time judging the econometric models, and just focus on GIS tasks. If you are not familiar with html documents generated by rmarkdown, you might benefit from reading the conventions of the book in the Preface. Target Audience The target audience of this chapter is those who are not very familiar with R as GIS. Knowledge of R certainly helps. But, I tried to write in a way that R beginners can still understand the power of R as GIS10. Do not get bogged down by all the complex-looking R codes. Just focus on the narratives and figures to get a sense of what R can do. Direction for replication Running the codes in this chapter involves reading datasets from a disk. All the datasets that will be imported are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:11 set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory download the pertinent datasets from here and put them in the “Data” folder run Chap_1_Demonstration.R which is included in the datasets folder you have downloaded source(&quot;Data/Chap_1_Demonstration.R&quot;) Note that the data folder includes 183-day worth of PRISM precipitation data for Demonstration 3, which are quite large in size (slightly less than 1 GB). If you are not replicating Demonstration 3, you can either choose not to download them or discard them if you have downloaded them already. 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.1.1 Project Overview Objective: Understand the impact of groundwater pumping on groundwater level. Datasets Groundwater pumping by irrigation wells in Chase, Dundy, and Perkins Counties in the southwest corner of Nebraska Groundwater levels observed at USGS monitoring wells located in the three counties and retrieved from the National Water Information System (NWIS) maintained by USGS using the dataRetrieval package. Econometric Model In order to achieve the project objective, we will estimate the following model: \\[ y_{i,t} - y_{i,t-1} = \\alpha + \\beta gw_{i,t-1} + v \\] where \\(y_{i,t}\\) is the depth to groundwater table12 in March13 in year \\(t\\) at USGS monitoring well \\(i\\), and \\(gw_{i,t-1}\\) is the total amount of groundwater pumping that happened within the 2-mile radius of the monitoring well \\(i\\). GIS tasks read an ESRI shape file as an sf (spatial) object use sf::st_read() download depth to water table data using the dataRetrieval package developed by USGS use dataRetrieval::readNWISdata() and dataRetrieval::readNWISsite() create a buffer around USGS monitoring wells use sf::st_buffer() convert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects use sf::st_as_sf() and sf::st_set_crs() reproject an sf object to another CRS use sf::st_transform() identify irrigation wells located inside the buffers and calculate total pumping use sf::st_join() packages Load (install first if you have not) the following packages if you intend to replicate the demonstration. library(sf) library(dplyr) library(lubridate) library(stargazer) There are other packages that will be loaded during the demonstration. 1.1.2 Project Demonstration The geographic focus of the project is the southwest corner of Nebraska consisting of Chase, Dundy, and Perkins County (see Figure 1.1 for their locations within Nebraska). Let’s read a shape file of the three counties represented as polygons. We will use it later to spatially filter groundwater level data downloaded from NWIS. three_counties &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;urnrd&quot;) %&gt;% #--- project to WGS84/UTM 14N ---# st_transform(32614) Reading layer `urnrd&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/RGIS_Econ/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395 CRS: 4269 Figure 1.1: The location of Chase, Dundy, and Perkins County in Nebraska We have already collected groundwater pumping data, so let’s import it. #--- groundwater pumping data ---# ( urnrd_gw &lt;- readRDS(&quot;./Data/urnrd_gw_pumping.rds&quot;) ) well_id year vol_af lon lat 1: 1706 2007 182.566 245322.3 4542717 2: 2116 2007 46.328 245620.9 4541125 3: 2583 2007 38.380 245660.9 4542523 4: 2597 2007 70.133 244816.2 4541143 5: 3143 2007 135.870 243614.0 4541579 --- 18668: 2006 2012 148.713 284782.5 4432317 18669: 2538 2012 115.567 284462.6 4432331 18670: 2834 2012 15.766 283338.0 4431341 18671: 2834 2012 381.622 283740.4 4431329 18672: 4983 2012 NA 284636.0 4432725 well_id is the unique irrigation well identifier, and vol_af is the amount of groundwater pumped in acre-feet. This dataset is just a regular data.frame with coordinates. We need to convert this dataset into a object of class sf so that we can later identify irrigation wells located within a 2-mile radius of USGS monitoring wells (see Figure 1.2 for the spatial distribution of the irrigation wells). urnrd_gw_sf &lt;- urnrd_gw %&gt;% #--- convert to sf ---# st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---# st_set_crs(32614) #--- now sf ---# urnrd_gw_sf Simple feature collection with 18672 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146 CRS: EPSG:32614 First 10 features: well_id year vol_af geometry 1 1706 2007 182.566 POINT (245322.3 4542717) 2 2116 2007 46.328 POINT (245620.9 4541125) 3 2583 2007 38.380 POINT (245660.9 4542523) 4 2597 2007 70.133 POINT (244816.2 4541143) 5 3143 2007 135.870 POINT (243614 4541579) 6 5017 2007 196.799 POINT (243539.9 4543146) 7 1706 2008 171.250 POINT (245322.3 4542717) 8 2116 2008 171.650 POINT (245620.9 4541125) 9 2583 2008 46.100 POINT (245660.9 4542523) 10 2597 2008 124.830 POINT (244816.2 4541143) Figure 1.2: Spatial distribution of irrigation wells Here are the rest of the steps we will take to obtain a regression-ready dataset for our analysis. download groundwater level data observed at USGS monitoring wells from National Water Information System (NWIS) using the dataRetrieval package identify the irrigation wells located within the 2-mile radius of the USGS wells and calculate the total groundwater pumping that occurred around each of the USGS wells by year merge the groundwater pumping data to the groundwater level data Let’s download groundwater level data from NWIS first. The following code downloads groundwater level data for Nebraska from Jan 1, 1990, through Jan 1, 2016. #--- load the dataRetrieval package ---# library(dataRetrieval) #--- download groundwater level data ---# NE_gwl &lt;- readNWISdata( stateCd=&quot;Nebraska&quot;, startDate = &quot;1990-01-01&quot;, endDate = &quot;2016-01-01&quot;, service = &quot;gwlevels&quot; ) %&gt;% dplyr::select(site_no, lev_dt, lev_va) %&gt;% rename(date = lev_dt, dwt = lev_va) #--- take a look ---# head(NE_gwl, 10) site_no date dwt 1 400008097545301 2000-11-08 17.40 2 400008097545301 2008-10-09 13.99 3 400008097545301 2009-04-09 11.32 4 400008097545301 2009-10-06 15.54 5 400008097545301 2010-04-12 11.15 6 400008100050501 1990-03-15 24.80 7 400008100050501 1990-10-04 27.20 8 400008100050501 1991-03-08 24.20 9 400008100050501 1991-10-07 26.90 10 400008100050501 1992-03-02 24.70 site_no is the unique monitoring well identifier, date is the date of groundwater level monitoring, and dwt is depth to water table. We calculate the average groundwater level in March by USGS monitoring well (right before the irrigation season starts):14 #--- Average depth to water table in March ---# NE_gwl_march &lt;- NE_gwl %&gt;% mutate( date = as.Date(date), month = month(date), year = year(date), ) %&gt;% #--- select observation in March ---# filter(year &gt;= 2007, month == 3) %&gt;% #--- gwl average in March ---# group_by(site_no, year) %&gt;% summarize(dwt = mean(dwt)) #--- take a look ---# head(NE_gwl_march, 10) # A tibble: 10 x 3 # Groups: site_no [2] site_no year dwt &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400032101022901 2008 118. 2 400032101022901 2009 117. 3 400032101022901 2010 118. 4 400032101022901 2011 118. 5 400032101022901 2012 118. 6 400032101022901 2013 118. 7 400032101022901 2014 116. 8 400032101022901 2015 117. 9 400038099244601 2007 24.3 10 400038099244601 2008 21.7 Since NE_gwl is missing geographic coordinates for the monitoring wells, we will download them using the readNWISsite() function and select only the monitoring wells that are inside the three counties. #--- get the list of site ids ---# NE_site_ls &lt;- NE_gwl$site_no %&gt;% unique() #--- get the locations of the site ids ---# sites_info &lt;- readNWISsite(siteNumbers = NE_site_ls) %&gt;% dplyr::select(site_no, dec_lat_va, dec_long_va) %&gt;% #--- turn the data into an sf object ---# st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;)) %&gt;% #--- NAD 83 ---# st_set_crs(4269) %&gt;% #--- project to WGS UTM 14 ---# st_transform(32614) %&gt;% #--- keep only those located inside the three counties ---# .[three_counties, ] We now identify irrigation wells that are located within the 2-mile radius of the monitoring wells15. We first create polygons of 2-mile radius circles around the monitoring wells (see Figure 1.3). buffers &lt;- st_buffer(sites_info, dist = 2*1609.34) # in meter Figure 1.3: 2-mile buffers around USGS monitoring wells We now identify which irrigation wells are inside each of the buffers and get the associated groundwater pumping values. The st_join() function from the sf package will do the trick. #--- find irrigation wells inside the buffer and calculate total pumping ---# pumping_neaby &lt;- st_join(buffers, urnrd_gw_sf) Let’s take a look at a USGS monitoring well (site_no = \\(400012101323401\\)). filter(pumping_neaby, site_no == 400012101323401, year == 2010) Simple feature collection with 7 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444 CRS: EPSG:32614 site_no well_id year vol_af geometry 1 400012101323401 6331 2010 NA POLYGON ((286128 4431225, 2... 2 400012101323401 1883 2010 180.189 POLYGON ((286128 4431225, 2... 3 400012101323401 2006 2010 79.201 POLYGON ((286128 4431225, 2... 4 400012101323401 2538 2010 68.205 POLYGON ((286128 4431225, 2... 5 400012101323401 2834 2010 NA POLYGON ((286128 4431225, 2... 6 400012101323401 2834 2010 122.981 POLYGON ((286128 4431225, 2... 7 400012101323401 4983 2010 NA POLYGON ((286128 4431225, 2... As you can see, this well has seven irrigation wells within its 2-mile radius in 2010. Now, we will get total nearby pumping by monitoring well and year. ( total_pumping_nearby &lt;- pumping_neaby %&gt;% #--- calculate total pumping by monitoring well ---# group_by(site_no, year) %&gt;% summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %&gt;% #--- NA means 0 pumping ---# mutate( nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping) ) ) Simple feature collection with 2396 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 237904.5 ymin: 4428006 xmax: 313476.5 ymax: 4545687 CRS: EPSG:32614 # A tibble: 2,396 x 4 # Groups: site_no [401] site_no year nearby_pumping geometry * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;POLYGON [m]&gt; 1 4000121013… 2007 571. ((286128 4431225, 286123.6 4431057, 286110.… 2 4000121013… 2008 772. ((286128 4431225, 286123.6 4431057, 286110.… 3 4000121013… 2009 500. ((286128 4431225, 286123.6 4431057, 286110.… 4 4000121013… 2010 451. ((286128 4431225, 286123.6 4431057, 286110.… 5 4000121013… 2011 545. ((286128 4431225, 286123.6 4431057, 286110.… 6 4000121013… 2012 1028. ((286128 4431225, 286123.6 4431057, 286110.… 7 4001301013… 2007 485. ((278847.4 4433844, 278843 4433675, 278829.… 8 4001301013… 2008 515. ((278847.4 4433844, 278843 4433675, 278829.… 9 4001301013… 2009 351. ((278847.4 4433844, 278843 4433675, 278829.… 10 4001301013… 2010 374. ((278847.4 4433844, 278843 4433675, 278829.… # … with 2,386 more rows We now merge nearby pumping data to the groundwater level data, and transform the data to obtain the dataset ready for regression analysis. #--- regression-ready data ---# reg_data &lt;- NE_gwl_march %&gt;% #--- pick monitoring wells that are inside the three counties ---# filter(site_no %in% unique(sites_info$site_no)) %&gt;% #--- merge with the nearby pumping data ---# left_join(., total_pumping_nearby, by = c(&quot;site_no&quot;, &quot;year&quot;)) %&gt;% #--- lead depth to water table ---# arrange(site_no, year) %&gt;% group_by(site_no) %&gt;% mutate( #--- lead depth ---# dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year), #--- first order difference in dwt ---# dwt_dif = dwt_lead1 - dwt ) #--- take a look ---# dplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping) # A tibble: 2,022 x 4 # Groups: site_no [230] site_no year dwt_dif nearby_pumping &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400130101374401 2011 NA 358. 2 400134101483501 2007 2.87 2038. 3 400134101483501 2008 0.78 2320. 4 400134101483501 2009 -2.45 2096. 5 400134101483501 2010 3.97 2432. 6 400134101483501 2011 1.84 2634. 7 400134101483501 2012 -1.35 985. 8 400134101483501 2013 44.8 NA 9 400134101483501 2014 -26.7 NA 10 400134101483501 2015 NA NA # … with 2,012 more rows Finally, we estimate the model using the lfe package. #--- load the lfe package for regression with fixed effects ---# library(lfe) #--- OLS with site_no and year FEs (error clustered by site_no) ---# reg_dwt &lt;- felm(dwt_dif ~ nearby_pumping | site_no + year | 0 | site_no, data = reg_data) Here is the regression result. stargazer(reg_dwt, type = &quot;html&quot;) Dependent variable: dwt_dif nearby_pumping 0.001*** (0.0001) Observations 1,342 R2 0.409 Adjusted R2 0.286 Residual Std. Error 1.493 (df = 1111) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.2 Demonstration 2: Precision Agriculture .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.2.1 Project Overview Objectives: Understand the impact of nitrogen on corn yield Understand how electric conductivity (EC) affects the marginal impact of nitrogen on corn Datasets: The experimental design of an on-farm randomized nitrogen trail on an 80-acre field Data generated by the experiment As-applied nitrogen rate Yield measures Electric conductivity Econometric Model: Here is the econometric model, we would like to estimate: \\[ yield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i \\] where \\(yield_i\\), \\(N_i\\), \\(EC_i\\), and \\(v_i\\) are corn yield, nitrogen rate, EC, and error term at subplot \\(i\\). Subplots which are obtained by dividing experimental plots into six of equal-area compartments. GIS tasks read spatial data in various formats: R data set (rds), shape file, and GeoPackage file use sf::st_read() create maps using the ggplot2 package use ggplot2::geom_sf() create subplots within experimental plots use-defined function that makes use of st_geometry() identify corn yield, as-applied nitrogen, and electric conductivity (EC) data points within each of the experimental plots and find their averages use sf::st_join() and sf::aggregate() Preparation for replication Source (run) Chap_1_Demonstration.R to define theme_map and gen_subplots() source(&quot;Codes/Chap_1_Demonstration.R&quot;) Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(dplyr) library(ggplot2) library(stargazer) 1.2.2 Project Demonstration We have already run a whole-field randomized nitrogen experiment on a 80-acre field. Let’s import the trial design data #--- read the trial design data ---# trial_design_16 &lt;- readRDS(&quot;./Data/trial_design.rds&quot;) Figure 1.4 is the map of the trial design generated using ggplot2 package.16. #--- map of trial design ---# ggplot(data = trial_design_16) + geom_sf(aes(fill = factor(NRATE))) + scale_fill_brewer(name = &quot;N&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map Figure 1.4: The Experimental Design of the Randomize Nitrogen Trial We have collected yield, as-applied NH3, and EC data. Let’s read in these datasets:17 #--- read yield data (sf data saved as rds) ---# yield &lt;- readRDS(&quot;./Data/yield.rds&quot;) #--- read NH3 data (GeoPackage data) ---# NH3_data &lt;- st_read(&quot;Data/NH3.gpkg&quot;) #--- read ec data (shape file) ---# ec &lt;- st_read(dsn=&quot;Data&quot;, &quot;ec&quot;) Figure 1.5 shows the spatial distribution of the three variables. A map of each variable was made first, and then they are combined into one figure using the patchwork package18. #--- yield map ---# g_yield &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = yield, aes(color = yield), size = 0.5) + scale_color_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_NH3 &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) + scale_color_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_ec &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = ec, aes(color = ec), size = 0.5) + scale_color_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- stack the figures vertically and display ---# library(patchwork) g_yield/g_NH3/g_ec Figure 1.5: Spatial distribution of yield, NH3, and EC Instead of using plot as the observation unit, we would like to create subplots inside each of the plots and make them the unit of analysis because it would avoid masking the within-plot spatial heterogeneity of EC. Here, we divide each plot into six subplots19: #--- generate subplots ---# subplots &lt;- lapply( 1:nrow(trial_design_16), function(x) gen_subplots(trial_design_16[x, ], 6) ) %&gt;% do.call(&#39;rbind&#39;, .) Figure 1.6 is a map of the subplots generated. #--- here is what subplots look like ---# ggplot(subplots) + geom_sf() + theme_for_map Figure 1.6: Map of the subplots We now identify the mean value of corn yield, nitrogen rate, and EC for each of the subplots using sf::aggregate() and sf::st_join(). ( reg_data &lt;- subplots %&gt;% #--- yield ---# st_join(., aggregate(yield, ., mean), join = st_equals) %&gt;% #--- nitrogen ---# st_join(., aggregate(NH3_data, ., mean), join = st_equals) %&gt;% #--- EC ---# st_join(., aggregate(ec, ., mean), join = st_equals) ) Simple feature collection with 816 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734 CRS: EPSG:26914 First 10 features: yield aa_NH3 ec geometry 1 220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,... 2 218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,... 3 220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,... 4 215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,... 5 216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,... 6 227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,... 7 226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,... 8 225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,... 9 221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5... 10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,... Here are the visualization of the subplot-level data (Figure 1.7): (ggplot() + geom_sf(data = reg_data, aes(fill = yield), color = NA) + scale_fill_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) + scale_fill_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = ec), color = NA) + scale_fill_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map) Figure 1.7: Spatial distribution of subplot-level yield, NH3, and EC Let’s estimate the model and see the results: lm(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3*ec) + I(aa_NH3^2*ec), data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: yield aa_NH3 -1.223 (1.308) I(aa_NH32) 0.004 (0.003) I(aa_NH3 * ec) 0.002 (0.003) I(aa_NH32 * ec) -0.00001 (0.00002) Constant 327.993*** (125.638) Observations 784 R2 0.010 Adjusted R2 0.005 Residual Std. Error 5.712 (df = 779) F Statistic 2.023* (df = 4; 779) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.3 Demonstration 3: Land Use and Weather .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.3.1 Project Overview Objective Understand the impact of past precipitation on crop choice in Iowa (IA). Datasets IA county boundary Regular grids over IA, created using sf::st_make_grid() PRISM daily precipitation data downloaded using prism package Land use data from the Cropland Data Layer (CDL) for IA in 2015, downloaded using cdlTools package Econometric Model The econometric model we would like to estimate is: \\[ CS_i = \\alpha + \\beta_1 PrN_{i} + \\beta_2 PrC_{i} + v_i \\] where \\(CS_i\\) is the area share of corn divided by that of soy in 2015 for grid \\(i\\) (we will generate regularly-sized grids in the Demo section), \\(PrN_i\\) is the total precipitation observed in April through May and September in 2014, \\(PrC_i\\) is the total precipitation observed in June through August in 2014, and \\(v_i\\) is the error term. To run the econometric model, we need to find crop share and weather variables observed at the grids. We first tackle the crop share variable, and then the precipitation variable. GIS tasks download Cropland Data Layer (CDL) data by USDA NASS use cdlTools::getCDL() download PRISM weather data use prism::get_prism_dailys() crop PRISM data to the geographic extent of IA use raster::crop() create regular grids within IA, which become the observation units of the econometric analysis use sf::st_make_grid() remove grids that share small area with IA use sf::st_intersection() and sf::st_area assign crop share and weather data to each of the generated IA grids (parallelized) use exactextractr::exact_extract() and future.apply::future_lapply() create maps use tmap package Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(data.table) library(dplyr) library(raster) library(lubridate) library(tmap) library(future.apply) library(stargazer) 1.3.2 Project Demonstration The geographic focus of this project is IA. Let’s get IAs state border (see Figure 1.8 for its map). library(&quot;maps&quot;) #--- IA state boundary ---# IA_boundary &lt;- st_as_sf(map(&quot;state&quot;, &quot;iowa&quot;, plot = FALSE, fill = TRUE)) Figure 1.8: IA state boundary The unit of analysis is artificial grids that we create over IA. The grids are regularly-sized rectangles except around the edge of the IA state border20. So, let’s create grids and remove those that do not overlap much with IA. #--- create regular grids (40 cells by 40 columns) over IA ---# IA_grids &lt;- IA_boundary %&gt;% #--- create grids ---# st_make_grid(, n = c(40, 40)) %&gt;% #--- convert to sf ---# st_as_sf() %&gt;% #--- find the intersections of IA grids and IA polygon ---# st_intersection(., IA_boundary) %&gt;% #--- calculate the area of each grid ---# mutate( area = as.numeric(st_area(.)), area_ratio = area/max(area) ) %&gt;% #--- keep only if the intersected area is large enough ---# filter(area_ratio &gt; 0.8) %&gt;% #--- assign grid id for future merge ---# mutate(grid_id = 1:nrow(.)) Here is what the generated grids look like (Figure 1.9): #--- plot the grids over the IA state border ---# tm_shape(IA_boundary) + tm_polygons(col = &quot;green&quot;) + tm_shape(IA_grids) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 1.9: Map of regular grids generated over IA Let’s work on crop share data. You can download CDL data using the getCDL() function from the cdlTools package. #--- load the cdlTools package ---# library(cdlTools) #--- download the CDL data for IA in 2015 ---# ( IA_cdl_2015 &lt;- getCDL(&quot;Iowa&quot;, 2015)$IA2015 ) class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /private/var/folders/t4/5gnqprbn38nftyxkyk5hdwmd8hnypy/T/RtmpeRWYIW/CDL_2015_19.tif names : CDL_2015_19 values : 0, 255 (min, max) The cells (30 meter by 30 meter) of the imported raster layer take a value ranging from 0 to 255. Corn and soybean are represented by 1 and 5, respectively (visualization of the CDL data is on the right). Figure 1.10 shows the map of one of the IA grids and the CDL cells it overlaps with. Figure 1.10: Spatial overlap of a IA grid and CDL layer We would like to extract all the cell values within the blue border. We use exactextractr::exact_extract() to identify which cells of the CDL raster layer fall within each of the IA grids and extract land use type values. We then find the share of corn and soybean for each of the grids. #--- reproject grids to the CRS of the CDL data ---# IA_grids_rp_cdl &lt;- st_transform(IA_grids, projection(IA_cdl_2015)) #--- load the exactextractr package for fast rater value extractions for polygons ---# library(exactextractr) #--- extract crop type values and find frequencies ---# cdl_extracted &lt;- exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %&gt;% lapply(., function (x) data.table(x)[,.N, by = value]) %&gt;% #--- combine the list of data.tables into one data.table ---# rbindlist(idcol = TRUE) %&gt;% #--- find the share of each land use type ---# .[, share := N/sum(N), by = .id] %&gt;% .[, N := NULL] %&gt;% #--- keep only the share of corn and soy ---# .[value %in% c(1, 5), ] We then find the corn to soy ratio for each of the IA grids. #--- find corn/soy ratio ---# corn_soy &lt;- cdl_extracted %&gt;% #--- long to wide ---# dcast(.id ~ value, value.var = &quot;share&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;.id&quot;, &quot;1&quot;, &quot;5&quot;), c(&quot;grid_id&quot;, &quot;corn_share&quot;, &quot;soy_share&quot;)) %&gt;% #--- corn share divided by soy share ---# .[, c_s_ratio := corn_share / soy_share] We are still missing daily precipitation data at the moment. We have decided to use daily weather data from PRISM. Daily PRISM data is a raster data with the cell size of 4 km by 4 km. Figure 1.11 the right presents precipitation data downloaded for April 1, 2010. It covers the entire contiguous U.S. Figure 1.11: Map of PRISM raster data layer Let’s now download PRISM data21. This can be done using the get_prism_dailys() function from the prism package.22 options(prism.path = &quot;./Data/PRISM&quot;) get_prism_dailys( type = &quot;ppt&quot;, minDate = &quot;2014-04-01&quot;, maxDate = &quot;2014-09-30&quot;, keepZip = FALSE ) When we use get_prism_dailys() to download data23, it creates one folder for each day. So, I have about 180 folders inside the folder I designated as the download destination above with the options() function. We now try to extract precipitation value by day for each of the IA grids by geographically overlaying IA grids onto the PRISM data layer and identify which PRISM cells each of the IA grid encompass. Figure 1.12 shows how the first IA grid overlaps with the PRISM cells24. #--- read a PRISM dataset ---# prism_whole &lt;- raster(&quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot;) #--- align the CRS ---# IA_grids_rp_prism &lt;- st_transform(IA_grids, projection(prism_whole)) #--- crop the PRISM data for the 1st IA grid ---# PRISM_1 &lt;- crop(prism_whole, st_buffer(IA_grids_rp_prism[1, ], dist = 0.05)) #--- map them ---# tm_shape(PRISM_1) + tm_raster() + tm_shape(IA_grids_rp_prism[1, ]) + tm_polygons(alpha = 0) + tm_layout(frame = NA) Figure 1.12: Spatial overlap of an IA grid over PRISM cells As you can see, some PRISM grids are fully inside the analysis grid, while others are partially inside it. So, when assigning precipitation values to grids, we will use the coverage-weighted mean of precipitations25. Unlike the CDL layer, we have 183 raster layers to process. Fortunately, we can process many raster files at the same time very quickly by first “stacking” many raster files first and then applying the exact_extract() function. Using future_lapply(), we let \\(6\\) cores take care of this task with each processing 31 files, except one of them handling only 28 files.26 We first get all the paths to the PRISM files. #--- get all the dates ---# dates_ls &lt;- seq(as.Date(&quot;2014-04-01&quot;), as.Date(&quot;2014-09-30&quot;), &quot;days&quot;) #--- remove hyphen ---# dates_ls_no_hyphen &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- get all the prism file names ---# folder_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil&quot;) file_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil.bil&quot;) file_paths &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- take a look ---# head(file_paths) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot; [2] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil&quot; [3] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil&quot; [4] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil&quot; [5] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil&quot; [6] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil&quot; We now prepare for parallelized extractions and then implement them using future_apply(). #--- define the number of cores to use ---# num_core &lt;- 6 #--- prepare some parameters for parallelization ---# file_len &lt;- length(file_paths) files_per_core &lt;- ceiling(file_len/num_core) #--- prepare for parallel processing ---# plan(multiprocess, workers = num_core) #--- reproject IA grids to the CRS of PRISM data ---# IA_grids_reprojected &lt;- st_transform(IA_grids, projection(prism_whole)) Here is the function that we run in parallel over 6 cores. #--- define the function to extract PRISM values by block of files ---# extract_by_block &lt;- function(i, files_per_core) { #--- files processed by core ---# start_file_index &lt;- (i-1) * files_per_core + 1 #--- indexes for files to process ---# file_index &lt;- seq( from = start_file_index, to = min((start_file_index + files_per_core), file_len), by = 1 ) #--- extract values ---# data_temp &lt;- file_paths[file_index] %&gt;% # get file names #--- stack files ---# stack() %&gt;% #--- extract ---# exact_extract(., IA_grids_reprojected) %&gt;% #--- combine into one data set ---# rbindlist(idcol = &quot;ID&quot;) %&gt;% #--- wide to long ---# melt(id.var = c(&quot;ID&quot;, &quot;coverage_fraction&quot;)) %&gt;% #--- calculate &quot;area&quot;-weighted mean ---# .[, .(value = sum(value * coverage_fraction)/sum(coverage_fraction)), by = .(ID, variable)] return(data_temp) } Now, let’s run the function in parallel and calculate precipitation by period. #--- run the function ---# precip_by_period &lt;- future_lapply(1:num_core, function(x) extract_by_block(x, files_per_core)) %&gt;% rbindlist() %&gt;% #--- recover the date ---# .[, variable := as.Date(str_extract(variable, &quot;[0-9]{8}&quot;), &quot;%Y%m%d&quot;)] %&gt;% #--- change the variable name to date ---# setnames(&quot;variable&quot;, &quot;date&quot;) %&gt;% #--- define critical period ---# .[,critical := &quot;non_critical&quot;] %&gt;% .[month(date) %in% 6:8, critical := &quot;critical&quot;] %&gt;% #--- total precipitation by critical dummy ---# .[, .(precip=sum(value)), by = .(ID, critical)] %&gt;% #--- wide to long ---# dcast(ID ~ critical, value.var = &quot;precip&quot;) We now have grid-level crop share and precipitation data. Let’s merge them and run regression.27 #--- crop share ---# reg_data &lt;- corn_soy[precip_by_period, on = c(grid_id = &quot;ID&quot;)] #--- OLS ---# reg_results &lt;- lm(c_s_ratio ~ critical + non_critical, data = reg_data) Here is the regression results table. #--- regression table ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: c_s_ratio critical -0.002*** (0.0003) non_critical -0.0003 (0.0003) Constant 2.701*** (0.161) Observations 1,218 R2 0.058 Adjusted R2 0.056 Residual Std. Error 0.743 (df = 1215) F Statistic 37.234*** (df = 2; 1215) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Again, do not read into the results as the econometric model is terrible. 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.4.1 Project Overview Objective Understand the impact of railroad on corn planted acreage in Illinois Datasets USDA corn planted acreage for Illinois downloaded from the USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA package US railroads (line data) downloaded from here Econometric Model We will estimate the following model: \\[ y_i = \\beta_0 + \\beta_1 RL_i + v_i \\] where \\(y_i\\) is corn planted acreage in county \\(i\\) in Illinois, \\(RL_i\\) is the total length of railroad, and \\(v_i\\) is the error term. GIS tasks Download USDA corn planted acreage by county as a spatial dataset (sf object) use tidyUSDA::getQuickStat() Import US railroad shape file as a spatial dataset (sf object) use sf:st_read() Spatially subset (crop) the railroad data to the geographic boundary of Illinois use sf_1[sf_2, ] Find railroads for each county (cross-county railroad will be chopped into pieces for them to fit within a single county) use sf::st_intersection() Calculate the travel distance of each railroad piece use sf::st_length() Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(ggplot2) library(dplyr) library(stargazer) 1.4.2 Project Demonstration We first download corn planted acreage data for 2018 from USDA NASS QuickStat service using tidyUSDA package28. library(keyring) library(tidyUSDA) ( IL_corn_planted &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;) , program = &quot;SURVEY&quot;, data_item = &quot;CORN - ACRES PLANTED&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2018&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 90 features and 5 fields (with 6 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2018 Bureau 011 CORN - ACRES PLANTED 264000 2 2018 Carroll 015 CORN - ACRES PLANTED 134000 3 2018 Henry 073 CORN - ACRES PLANTED 226500 4 2018 Jo Daviess 085 CORN - ACRES PLANTED 98500 5 2018 Lee 103 CORN - ACRES PLANTED 236500 6 2018 Mercer 131 CORN - ACRES PLANTED 141000 7 2018 Ogle 141 CORN - ACRES PLANTED 217000 8 2018 Putnam 155 CORN - ACRES PLANTED 32300 9 2018 Rock Island 161 CORN - ACRES PLANTED 68400 10 2018 Stephenson 177 CORN - ACRES PLANTED 166500 geometry 1 MULTIPOLYGON (((-89.8569 41... 2 MULTIPOLYGON (((-90.16133 4... 3 MULTIPOLYGON (((-90.43227 4... 4 MULTIPOLYGON (((-90.50668 4... 5 MULTIPOLYGON (((-89.63118 4... 6 MULTIPOLYGON (((-90.99255 4... 7 MULTIPOLYGON (((-89.68598 4... 8 MULTIPOLYGON (((-89.33303 4... 9 MULTIPOLYGON (((-90.33573 4... 10 MULTIPOLYGON (((-89.9205 42... A nice thing about this function is that the data is downloaded as an sf object with county geometry with geometry = TRUE. So, you can immediately plot it (Figure 1.13) and use it for later spatial interactions without having to merge the downloaded data to an independent county boundary data.29. ggplot(IL_corn_planted) + geom_sf(aes(fill = Value/1000)) + scale_fill_distiller(name = &quot;Planted Acreage (1000 acres)&quot;, palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map Figure 1.13: Map of Con Planted Acreage in Illinois in 2018 Let’s import the U.S. railroad data and reproject to the CRS of IL_corn_planted: rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_transform(st_crs(IL_corn_planted)) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/RGIS_Econ/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 CRS: 4269 Here is what it looks like: ggplot(rail_roads) + geom_sf() + theme_for_map Figure 1.14: Map of Railroads We now crop it to the Illinois state border (Figure 1.15) using sf_1[sf_2, ]: rail_roads_IL &lt;- rail_roads[IL_corn_planted, ] ggplot() + geom_sf(data = rail_roads_IL) + theme_for_map Figure 1.15: Map of railroads in Illinois Let’s now find railroads for each county, where cross-county railroads will be chopped into pieces so each piece fits completely within a single county, using st_intersection(). rails_IL_segmented &lt;- st_intersection(rail_roads_IL, IL_corn_planted) Here are the railroads for Richland County: ggplot() + geom_sf(data = dplyr::filter(IL_corn_planted, NAME == &quot;Richland&quot;)) + geom_sf(data = dplyr::filter(rails_IL_segmented, NAME == &quot;Richland&quot;), aes( color = LINEARID )) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map We now calculate the travel distance (Great-circle distance) of each railroad piece using st_length() and then sum them up by county to find total railroad length by county. ( rail_length_county &lt;- mutate( rails_IL_segmented, length_in_m = as.numeric(st_length(rails_IL_segmented)), ) %&gt;% #--- group by county ID ---# group_by(county_code) %&gt;% #--- sum rail length by county ---# summarize(length_in_m = sum(length_in_m)) %&gt;% #--- geometry no longer needed ---# st_drop_geometry() ) # A tibble: 82 x 2 county_code length_in_m * &lt;chr&gt; &lt;dbl&gt; 1 001 77221. 2 003 77290. 3 007 36764. 4 011 255441. 5 015 161726. 6 017 30585. 7 019 389226. 8 021 155794. 9 023 78587. 10 025 92030. # … with 72 more rows We merge the railroad length data to the corn planted acreage data and estimate the model. reg_data &lt;- left_join(IL_corn_planted, rail_length_county, by = &quot;county_code&quot;) lm(Value ~ length_in_m, data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: Value length_in_m 0.092* (0.047) Constant 108,154.800*** (11,418.900) Observations 82 R2 0.046 Adjusted R2 0.034 Residual Std. Error 69,040.680 (df = 80) F Statistic 3.866* (df = 1; 80) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.5 Demonstration 5: Groundwater use for agricultural irrigation .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.5.1 Project Overview Objective + Understand the impact of monthly precipitation on groundwater use for agricultural irrigation Datasets Annual groundwater pumping by irrigation wells in Kansas for 2010 and 2011 (originally obtained from the Water Information Management &amp; Analysis System (WIMAS) database) Daymet30 daily precipitation and maximum temperature downloaded using daymetr package Econometric Model The econometric model we would like to estimate is: \\[ y_{i,t} = \\alpha + P_{i,t} \\beta + T_{i,t} \\gamma + \\phi_i + \\eta_t + v_{i,t} \\] where \\(y\\) is the total groundwater extracted in year \\(t\\), \\(P_{i,t}\\) and \\(T_{i,t}\\) is the collection of monthly total precipitation and mean maximum temperature April through September in year \\(t\\), respectively, \\(\\phi_i\\) is the well fixed effect, \\(\\eta_t\\) is the year fixed effect, and \\(v_{i,t}\\) is the error term. GIS tasks download Daymet precipitation and maximum temperature data for each well from within R in parallel use daymetr::download_daymet() and future.apply::future_lapply() 1.5.2 Project Demonstration We have already collected annual groundwater pumping data by irrigation wells in 2010 and 2011 in Kansas from the Water Information Management &amp; Analysis System (WIMAS) database. Let’s read in the groundwater use data. #--- read in the data ---# ( gw_KS_sf &lt;- readRDS( &quot;./Data/gw_KS_sf.rds&quot;) ) Simple feature collection with 56225 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191 CRS: EPSG:4269 First 10 features: well_id year af_used geometry 1 1 2010 67.00000 POINT (-100.4423 37.52046) 2 1 2011 171.00000 POINT (-100.4423 37.52046) 3 3 2010 30.93438 POINT (-100.7118 39.91526) 4 3 2011 12.00000 POINT (-100.7118 39.91526) 5 7 2010 0.00000 POINT (-101.8995 38.78077) 6 7 2011 0.00000 POINT (-101.8995 38.78077) 7 11 2010 154.00000 POINT (-101.7114 39.55035) 8 11 2011 160.00000 POINT (-101.7114 39.55035) 9 12 2010 28.17239 POINT (-95.97031 39.16121) 10 12 2011 89.53479 POINT (-95.97031 39.16121) We have 28553 wells in total, and each well has records of groundwater pumping (af_used) for years 2010 and 2011. Here is the spatial distribution of the wells. We now need to get monthly precipitation and maximum temperature data. We have decided that we use Daymet weather data. Here we use the download_daymet() function from the daymetr package31 that allows us to download all the weather variables for a specified geographic location and time period32. We write a wrapper function that downloads Daymet data and then processes it to find monthly total precipitation and mean maximum temperature33. We then loop over the 56225 wells, which is parallelized using the future_apply() function34 from the future.apply package. This process takes about an hour on my Mac with parallelization on 7 cores. The data is available in the data repository for this course (named as “all_daymet.rds”). library(daymetr) library(future.apply) #--- get the geographic coordinates of the wells ---# well_locations &lt;- gw_KS_sf %&gt;% unique(by = &quot;well_id&quot;) %&gt;% dplyr::select(well_id) %&gt;% cbind(., st_coordinates(.)) #--- define a function that downloads Daymet data by well and process it ---# get_daymet &lt;- function(i) { temp_site &lt;- well_locations[i, ]$well_id temp_long &lt;- well_locations[i, ]$X temp_lat &lt;- well_locations[i, ]$Y data_temp &lt;- download_daymet( site = temp_site, lat = temp_lat, lon = temp_long, start = 2010, end = 2011, #--- if TRUE, tidy data is returned ---# simplify = TRUE, #--- if TRUE, the downloaded data can be assigned to an R object ---# internal = TRUE ) %&gt;% data.table() %&gt;% #--- keep only precip and tmax ---# .[measurement %in% c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), ] %&gt;% #--- recover calender date from Julian day ---# .[, date := as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)] %&gt;% #--- get month ---# .[, month := month(date)] %&gt;% #--- keep only April through September ---# .[month %in% 4:9,] %&gt;% .[, .(site, year, month, date, measurement, value)] %&gt;% #--- long to wide ---# dcast(site + year + month + date~ measurement, value.var = &quot;value&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), c(&quot;prcp&quot;, &quot;tmax&quot;)) %&gt;% #--- find the total precip and mean tmax by month-year ---# .[, .(prcp = sum(prcp), tmax = mean(tmax)) , by = .(month, year)] %&gt;% .[, well_id := temp_site] return(data_temp) gc() } Here is what one run (for the first well) of get_daymet() returns #--- one run ---# ( returned_data &lt;- get_daymet(1)[] ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 6: 9 2010 15 31.43333 1 7: 4 2011 25 21.91667 1 8: 5 2011 26 26.30645 1 9: 6 2011 23 35.16667 1 10: 7 2011 35 38.62903 1 11: 8 2011 37 36.90323 1 12: 9 2011 9 28.66667 1 We get the number of cores you can use by RhpcBLASctl::get_num_procs() and parallelize the loop over wells using future_lapply().35 #--- prepare parallelized process ---# library(RhpcBLASctl) num_core &lt;- get_num_procs() - 1 #--- run get_daymet with parallelization ---# ( all_daymet &lt;- future_lapply(1:nrow(well_locations), get_daymet) %&gt;% rbindlist() ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 --- 336980: 5 2011 18 26.11290 78051 336981: 6 2011 25 34.61667 78051 336982: 7 2011 6 38.37097 78051 336983: 8 2011 39 36.66129 78051 336984: 9 2011 23 28.45000 78051 Before merging the Daymet data, we need to reshape the data into a wide format to get monthly precipitation and maximum temperature as columns. #--- long to wide ---# daymet_to_merge &lt;- dcast(all_daymet, well_id + year ~ month, value.var = c(&quot;prcp&quot;, &quot;tmax&quot;)) #--- take a look ---# daymet_to_merge well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56160: 78049 2011 27 6 38 37 34 36 22.81667 26.70968 56161: 78050 2010 35 48 68 111 56 9 21.38333 24.85484 56162: 78050 2011 26 7 44 38 34 35 22.76667 26.70968 56163: 78051 2010 30 62 48 29 76 3 21.05000 24.14516 56164: 78051 2011 33 18 25 6 39 23 21.90000 26.11290 tmax_6 tmax_7 tmax_8 tmax_9 1: 32.51667 33.50000 34.17742 31.43333 2: 35.16667 38.62903 36.90323 28.66667 3: 30.73333 32.80645 33.56452 28.93333 4: 30.08333 35.08065 32.90323 25.81667 5: 31.30000 33.12903 32.67742 30.16667 --- 56160: 35.01667 38.32258 36.54839 28.80000 56161: 33.16667 33.88710 34.40323 32.11667 56162: 34.91667 38.32258 36.54839 28.83333 56163: 32.90000 33.83871 34.38710 31.56667 56164: 34.61667 38.37097 36.66129 28.45000 Now, let’s merge the weather data to the groundwater pumping dataset. ( reg_data &lt;- data.table(gw_KS_sf) %&gt;% #--- keep only the relevant variables ---# .[, .(well_id, year, af_used)] %&gt;% #--- join ---# daymet_to_merge[., on = c(&quot;well_id&quot;, &quot;year&quot;)] ) well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56221: 79348 2011 NA NA NA NA NA NA NA NA 56222: 79349 2011 NA NA NA NA NA NA NA NA 56223: 79367 2011 NA NA NA NA NA NA NA NA 56224: 79372 2011 NA NA NA NA NA NA NA NA 56225: 80930 2011 NA NA NA NA NA NA NA NA tmax_6 tmax_7 tmax_8 tmax_9 af_used 1: 32.51667 33.50000 34.17742 31.43333 67.00000 2: 35.16667 38.62903 36.90323 28.66667 171.00000 3: 30.73333 32.80645 33.56452 28.93333 30.93438 4: 30.08333 35.08065 32.90323 25.81667 12.00000 5: 31.30000 33.12903 32.67742 30.16667 0.00000 --- 56221: NA NA NA NA 76.00000 56222: NA NA NA NA 182.00000 56223: NA NA NA NA 0.00000 56224: NA NA NA NA 134.00000 56225: NA NA NA NA 23.69150 Let’s run regression and display the results. #--- load lfe package ---# library(lfe) #--- run FE ---# reg_results &lt;- felm( af_used ~ prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9 + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |well_id + year| 0 | well_id, data = reg_data ) #--- display regression results ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: af_used prcp_4 -0.053*** (0.017) prcp_5 0.112*** (0.010) prcp_6 -0.073*** (0.008) prcp_7 0.014 (0.010) prcp_8 0.093*** (0.014) prcp_9 -0.177*** (0.025) tmax_4 9.159*** (1.227) tmax_5 -7.505*** (1.062) tmax_6 15.134*** (1.360) tmax_7 3.969** (1.618) tmax_8 3.420*** (1.066) tmax_9 -11.803*** (1.801) Observations 55,754 R2 0.942 Adjusted R2 0.883 Residual Std. Error 46.864 (df = 27659) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 That’s it. Do not bother to try to read into the regression results. Again, this is just an illustration of how R can be used to prepare a regression-ready dataset with spatial variables. "]
=======
["preface.html", "Preface Why R as GIS for Economists? How is this book different from other online books and resources? What is going to be covered in this book? Conventions of the book and some notes Session Information", " Preface Why R as GIS for Economists? R has extensive capabilities as GIS software. In my opinion, \\(99\\%\\) of your spatial data processing needs as an economist will be satisfied by R. But, there are several popular options for GIS tasks other than R: Python ArcGIS QGIS Here I compare them briefly and discuss why R is a good option. R vs Python Both R and Python are actually heavily dependent on open source software GDAL and GEOS for their core GIS operations (GDAL for reading spatial data, and GEOS for geometrical operations like intersecting two spatial layers).1 So, when you run GIS tasks on R or Python you basically tell R or Python what you want to do and they talk to the software, let it do the job, and return the results to you. This means that R and Python are much different in their capability at GIS tasks as they are dependent on the common open source software for many GIS tasks. When GDAL and GEOS get better, R and Python get better (with a short lag). Both of them have good spatial visualization tools as well. Moreover, both R and Python can communicate with QGIS and ArcGIS (as long you as have them installed of course) and use their functionalities from within R and Python via the bridging packages: RQGIS and PyQGIS for QGIS, and R-ArcGIS and ArcPy.2 So, if you are more familiar with Python than R, go ahead and go with Python. From now on, my discussions assume that you are going for the R option, as otherwise, you would not be reading the rest of the book anyway. R vs ArcGIS or QGIS ArcGIS is commercial software and it is quite expensive (you are likely to be able to get a significant discount if you are a student at or work for a University). On the other hand, QGIS is open source and free. It has seen significant development over the decade, and I would say it is just as competitive as ArcGIS. QGIS also uses open source geospatial software GDAL, GEOS, and others (SAGA, GRASS GIS). Both of them have a graphical interface that helps you implement various GIS tasks unlike R which requires programming. Now, since R can use ArcGIS and QGIS through the bridging packages, a more precise question we should be asking is whether you should program GIS tasks using R (possibly using the bridging packages) or manually implement GIS tasks using the graphical interface of ArcGIS or QGIS. The answer is programming GIS tasks using R. First, manual GIS operations are hard to repeat. It is often the case that in the course of a project you need to redo the same GIS task except that the underlying datasets have changed. If you have programmed the process with R, you just run the same code and that’s it. You get the desired results. If you did not program it, you need to go through many clicks on the graphical interface all over again, potentially trying to remember how you actually did it the last time.3 Second and more important, manual operations are not scalable. It has become much more common that we need to process many large spatial datasets. Imagine you are doing the same operations on \\(1,000\\) files using a graphical interface, or even \\(50\\) files. Do you know what is good at doing the same tasks over and over again without complaining? A computer. Just let it do what it likes to do. You have better things do. Finally, should you learn ArcGIS or QGIS in addition to (or before) R? I am doubtful. As economists, the GIS tasks we need to do are not super convoluted most of the time. Suppose \\(\\Omega_R\\) and \\(\\Omega_{AQ}\\) represent the set of GIS tasks R and \\(ArcGIS/QGIS\\) can implement, respectively. Further, let \\(\\Omega_E\\) represent the set of skills economists need to implement. Then, \\(\\Omega_E \\in \\Omega_R\\) \\(99\\%\\) (or maybe \\(95\\%\\) to be safe) of the time and \\(\\Omega_E \\not\\subset \\Omega_{AQ}\\setminus\\Omega_R\\) \\(99\\%\\) of the time. Personally, I have never had to rely on either ArcGIS or QGIS for my research projects after I learned how to use R as GIS. One of the things ArcGIS and QGIS can do but R cannot do (\\(\\Omega_{AQ}\\setminus\\Omega_R\\)) is create spatial objects by hand using a graphical user interface, like drawing polygons and lines. Another thing that R lags behind ArcGIS and QGIS is 3D data visualization. But, I must say neither of them is essential for economists at the moment. Finally, sometime it is easier and faster to make a map using ArcGIS and QGIS especially for a complicated map.4 Summary You have never used any GIS software? Learn R first. If you find out you really cannot complete the tasks you would like to do using R, then turn to other options. You have used ArcGIS or QGIS and do not like them because they crash often? Why don’t you try R?5 You may realize you actually do not need them. You have used ArcGIS or QGIS before and are very comfortable with them, but you need to program repetitive GIS tasks? Learn R and maybe take advantage of R-ArcGIS or RQGIS, which this book does not cover. You know for sure that you need to run only a simple GIS task once and never have to do any GIS tasks ever again? Stop reading and ask one of your friends to do the job. Pay him/her \\(\\$20\\) per hour, which is way below the opportunity cost of setting up either ArcGIS or QGI and learning to do that simple task on them. How is this book different from other online books and resources? We are seeing an explosion of online (and free) resources that teach how to use R for spatial data processing.6 Here is an incomplete list of such resources: Geocomputation with R Spatial Data Science Spatial Data Science with R Introduction to GIS using R Code for An Introduction to Spatial Analysis and Mapping in R Introduction to GIS in R Intro to GIS and Spatial Analysis Introduction to Spatial Data Programming with R Reproducible GIS analysis with R R for Earth-System Science Rspatial NEON Data Skills Simple Features for R Thanks to all these resources, it has become much easier to self-teach R for GIS work than six or seven years ago when I first started using R for GIS. Even though I have not read through all these resources carefully, I am pretty sure every topic found in this book can also be found somewhere in these resources (except the demonstrations). So, you may wonder why on earth you can benefit from reading this book. It all boils down to search costs. Researchers in different disciplines require different sets of spatial data skills. The available resources are either very general covering so many topics that economists are very unlikely to use. It is particularly hard for those who do not have much experience in GIS to identify whether particular skills are essential or not. So, they could spend so much time learning something that is not really useful. The value of this book lies in its deliberate incomprehensiveness. It only packages materials that satisfy the need of most economists, cutting out many topics that are likely to be of limited use for economists. For those who are looking for more comprehensive treatments of spatial data handling and processing in one book, I personally like Geocomputation with R a lot. Increasingly, the developer of R packages created a website dedicated to their R packages, where you can often find vignettes (tutorials), like Simple Features for R. What is going to be covered in this book? The book starts with the very basics of spatial data handling (e.g., importing and exporting spatial datasets) and moves on to more practical spatial data operations (e.g., spatial data join) that are useful for research projects. This books is still under development. Right now, only Chapter 1 is available. I will work on the rest of the book over the summer. The “coming soon” chapters are close to be done. I just need to add finishing touches to those chapters. The “wait a bit” chapters need some more work, adding contents, etc. Chapter 1: Demonstrations of R as GIS (available) groundwater pumping and groundwater level precision agriculture land use and weather corn planted acreage and railroads groundwater pumping and weather Chapter 2: The basics of vector data handling using sf package (coming soon) spatial data structure in sf import and export vector data (re)projection of spatial datasets single-layer geometrical operations (e.g., create buffers, find centroids) other miscellaneous basic operations Chapter 3: Spatial interactions of vector datasets (coming soon) spatially subsetting one layer based on another layer extracting values from one layer to another layer7 Chapter 4: The basics of raster data handling using raster and terra packages (coming soon) import and export raster data stacking raster data Chapter 5: Spatial interactions of vector and raster datasets (wait a bit) extracting values from a raster layer to a vector layer Chapter 6: Efficient spatial data processing (wait a bit) parallelization Chapter 7: Downloading publicly available spatial datasets (wait a bit) Sentinel 2 (sen2r) USDA NASS QuickStat (tidyUSDA) PRISM (prism) Daymet (daymetr) USGS (dataRetrieval) Chapter 8: Parallel computation (wait a bit) As you can see above, this book does not spend any time on the very basics of GIS concepts. Before you start reading the book, you should know the followings at least (it’s not much): What Geographic Coordinate System (GCS), Coordinate Reference System (CRS), and projection are (this is a good resource) Distinctions between vector and raster data (this is a simple summary of the difference) Finally, this book does not cover spatial statistics or spatial econometrics at all. This book is about spatial data processing. Spatial analysis is something you do after you have processed spatial data. Conventions of the book and some notes Here are some notes of the conventions of this book and notes for R beginners and those who are not used to reading rmarkdown-generated html documents. Texts in gray boxes They are one of the following: objects defined on R during demonstrations R functions R packages When it is a function, I always put parentheses at the end like this: st_read().8 Sometimes, I combine a package and function in one like this: sf::st_read(). This means it is a function called st_read() from the sf package. Colored Boxes Codes are in blue boxes, and outcomes are in red boxes. Codes: runif(5) Outcomes: ## [1] 0.5463091 0.0733040 0.4261112 0.1165969 0.1574750 Parentheses around codes Sometimes you will see codes enclosed by parenthesis like this: ( a &lt;- runif(5) ) ## [1] 0.12203153 0.01051125 0.37176512 0.70698067 0.52884845 The parentheses prints what’s inside of a newly created object (here a) without explicitly evaluating the object. So, basically I am signaling that we will be looking inside of the object that was just created. This one prints nothing. a &lt;- runif(5) Footnotes Footnotes appear at the bottom of the page. You can easily get to a footnote by clicking on the footnote number. You can also go back to the main narrative where the footnote number is by clicking on the curved arrow at the end of the footnote. So, don’t worry about having to scroll all the way up to where you were after reading footnotes. Session Information Here is the session information when compiling the book: sessionInfo() ## R version 4.0.0 (2020-04-24) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Mojave 10.14.1 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.0 magrittr_1.5 bookdown_0.18 htmltools_0.4.0 ## [5] tools_4.0.0 yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [9] rmarkdown_2.1 knitr_1.28 stringr_1.4.0 digest_0.6.25 ## [13] xfun_0.13 rlang_0.4.6 evaluate_0.14 For example, see the very first sentence of this page↩ We do not learn them in this lecture note because I do not see the benefits of using them.↩ You could take a step-by-step note of what you did though.↩ Let me know if you know something that is essential for economists that only ArcGIS or QGIS can do. I will add that to the list here.↩ I am not saying R does not crash. R does crash. But, often times, the fault is yours, rather than the software’s.↩ This phenomenon is largely thanks to packages like bookdown (Xie 2016), blogdown (Xie, Hill, and Thomas 2017), and pkgdown (Wickham and Hesselberth 2020) that has lowered the cost of professional contents creation much much lower than before. Indeed, this book was built taking advantage of the bookdown package.↩ over function in sp language↩ This is a function that draws values randomly from the uniform distribution.↩ "],
["demo.html", "Chapter 1 R as GIS: Demonstrations Before you start 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table 1.2 Demonstration 2: Precision Agriculture 1.3 Demonstration 3: Land Use and Weather 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage 1.5 Demonstration 5: Groundwater use for agricultural irrigation", " Chapter 1 R as GIS: Demonstrations Before you start The primary objective of this chapter is to showcase the power of R as GIS through demonstrations using mock-up econometric research projects9. Each project consists of a project overview (objective, datasets used, econometric model, and GIS tasks involved) and demonstration. This is really not a place you learn the nuts and bolts of how R does spatial operations. Indeed, we intentionally do not explain all the details of how the R codes work. We reiterate that the main purpose of the demonstrations is to get you a better idea of how R can be used to process spatial data to help your research projects involving spatial datasets. Finally, note that these mock-up projects use extremely simple econometric models that completely lacks careful thoughts you would need in real research projects. So, don’t waste your time judging the econometric models, and just focus on GIS tasks. If you are not familiar with html documents generated by rmarkdown, you might benefit from reading the conventions of the book in the Preface. Target Audience The target audience of this chapter is those who are not very familiar with R as GIS. Knowledge of R certainly helps. But, I tried to write in a way that R beginners can still understand the power of R as GIS10. Do not get bogged down by all the complex-looking R codes. Just focus on the narratives and figures to get a sense of what R can do. Direction for replication Running the codes in this chapter involves reading datasets from a disk. All the datasets that will be imported are available here. In this chapter, the path to files is set relative to my own working directory (which is hidden). To run the codes without having to mess with paths to the files, follow these steps:11 set a folder (any folder) as the working directory using setwd() create a folder called “Data” inside the folder designated as the working directory download the pertinent datasets from here and put them in the “Data” folder run Chap_1_Demonstration.R which is included in the datasets folder you have downloaded source(&quot;Data/Chap_1_Demonstration.R&quot;) Note that the data folder includes 183-day worth of PRISM precipitation data for Demonstration 3, which are quite large in size (slightly less than 1 GB). If you are not replicating Demonstration 3, you can either choose not to download them or discard them if you have downloaded them already. 1.1 Demonstration 1: The impact of groundwater pumping on depth to water table .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.1.1 Project Overview Objective: Understand the impact of groundwater pumping on groundwater level. Datasets Groundwater pumping by irrigation wells in Chase, Dundy, and Perkins Counties in the southwest corner of Nebraska Groundwater levels observed at USGS monitoring wells located in the three counties and retrieved from the National Water Information System (NWIS) maintained by USGS using the dataRetrieval package. Econometric Model In order to achieve the project objective, we will estimate the following model: \\[ y_{i,t} - y_{i,t-1} = \\alpha + \\beta gw_{i,t-1} + v \\] where \\(y_{i,t}\\) is the depth to groundwater table12 in March13 in year \\(t\\) at USGS monitoring well \\(i\\), and \\(gw_{i,t-1}\\) is the total amount of groundwater pumping that happened within the 2-mile radius of the monitoring well \\(i\\). GIS tasks read an ESRI shape file as an sf (spatial) object use sf::st_read() download depth to water table data using the dataRetrieval package developed by USGS use dataRetrieval::readNWISdata() and dataRetrieval::readNWISsite() create a buffer around USGS monitoring wells use sf::st_buffer() convert a regular data.frame (non-spatial) with geographic coordinates into an sf (spatial) objects use sf::st_as_sf() and sf::st_set_crs() reproject an sf object to another CRS use sf::st_transform() identify irrigation wells located inside the buffers and calculate total pumping use sf::st_join() packages Load (install first if you have not) the following packages if you intend to replicate the demonstration. library(sf) library(dplyr) library(lubridate) library(stargazer) There are other packages that will be loaded during the demonstration. 1.1.2 Project Demonstration The geographic focus of the project is the southwest corner of Nebraska consisting of Chase, Dundy, and Perkins County (see Figure 1.1 for their locations within Nebraska). Let’s read a shape file of the three counties represented as polygons. We will use it later to spatially filter groundwater level data downloaded from NWIS. three_counties &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;urnrd&quot;) %&gt;% #--- project to WGS84/UTM 14N ---# st_transform(32614) Reading layer `urnrd&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/RGIS_Econ/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: -102.0518 ymin: 40.00257 xmax: -101.248 ymax: 41.00395 CRS: 4269 Figure 1.1: The location of Chase, Dundy, and Perkins County in Nebraska We have already collected groundwater pumping data, so let’s import it. #--- groundwater pumping data ---# ( urnrd_gw &lt;- readRDS(&quot;./Data/urnrd_gw_pumping.rds&quot;) ) well_id year vol_af lon lat 1: 1706 2007 182.566 245322.3 4542717 2: 2116 2007 46.328 245620.9 4541125 3: 2583 2007 38.380 245660.9 4542523 4: 2597 2007 70.133 244816.2 4541143 5: 3143 2007 135.870 243614.0 4541579 --- 18668: 2006 2012 148.713 284782.5 4432317 18669: 2538 2012 115.567 284462.6 4432331 18670: 2834 2012 15.766 283338.0 4431341 18671: 2834 2012 381.622 283740.4 4431329 18672: 4983 2012 NA 284636.0 4432725 well_id is the unique irrigation well identifier, and vol_af is the amount of groundwater pumped in acre-feet. This dataset is just a regular data.frame with coordinates. We need to convert this dataset into a object of class sf so that we can later identify irrigation wells located within a 2-mile radius of USGS monitoring wells (see Figure 1.2 for the spatial distribution of the irrigation wells). urnrd_gw_sf &lt;- urnrd_gw %&gt;% #--- convert to sf ---# st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;)) %&gt;% #--- set CRS WGS UTM 14 (you need to know the CRS of the coordinates to do this) ---# st_set_crs(32614) #--- now sf ---# urnrd_gw_sf Simple feature collection with 18672 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: 239959 ymin: 4431329 xmax: 310414.4 ymax: 4543146 CRS: EPSG:32614 First 10 features: well_id year vol_af geometry 1 1706 2007 182.566 POINT (245322.3 4542717) 2 2116 2007 46.328 POINT (245620.9 4541125) 3 2583 2007 38.380 POINT (245660.9 4542523) 4 2597 2007 70.133 POINT (244816.2 4541143) 5 3143 2007 135.870 POINT (243614 4541579) 6 5017 2007 196.799 POINT (243539.9 4543146) 7 1706 2008 171.250 POINT (245322.3 4542717) 8 2116 2008 171.650 POINT (245620.9 4541125) 9 2583 2008 46.100 POINT (245660.9 4542523) 10 2597 2008 124.830 POINT (244816.2 4541143) Figure 1.2: Spatial distribution of irrigation wells Here are the rest of the steps we will take to obtain a regression-ready dataset for our analysis. download groundwater level data observed at USGS monitoring wells from National Water Information System (NWIS) using the dataRetrieval package identify the irrigation wells located within the 2-mile radius of the USGS wells and calculate the total groundwater pumping that occurred around each of the USGS wells by year merge the groundwater pumping data to the groundwater level data Let’s download groundwater level data from NWIS first. The following code downloads groundwater level data for Nebraska from Jan 1, 1990, through Jan 1, 2016. #--- load the dataRetrieval package ---# library(dataRetrieval) #--- download groundwater level data ---# NE_gwl &lt;- readNWISdata( stateCd=&quot;Nebraska&quot;, startDate = &quot;1990-01-01&quot;, endDate = &quot;2016-01-01&quot;, service = &quot;gwlevels&quot; ) %&gt;% dplyr::select(site_no, lev_dt, lev_va) %&gt;% rename(date = lev_dt, dwt = lev_va) #--- take a look ---# head(NE_gwl, 10) site_no date dwt 1 400008097545301 2000-11-08 17.40 2 400008097545301 2008-10-09 13.99 3 400008097545301 2009-04-09 11.32 4 400008097545301 2009-10-06 15.54 5 400008097545301 2010-04-12 11.15 6 400008100050501 1990-03-15 24.80 7 400008100050501 1990-10-04 27.20 8 400008100050501 1991-03-08 24.20 9 400008100050501 1991-10-07 26.90 10 400008100050501 1992-03-02 24.70 site_no is the unique monitoring well identifier, date is the date of groundwater level monitoring, and dwt is depth to water table. We calculate the average groundwater level in March by USGS monitoring well (right before the irrigation season starts):14 #--- Average depth to water table in March ---# NE_gwl_march &lt;- NE_gwl %&gt;% mutate( date = as.Date(date), month = month(date), year = year(date), ) %&gt;% #--- select observation in March ---# filter(year &gt;= 2007, month == 3) %&gt;% #--- gwl average in March ---# group_by(site_no, year) %&gt;% summarize(dwt = mean(dwt)) #--- take a look ---# head(NE_gwl_march, 10) # A tibble: 10 x 3 # Groups: site_no [2] site_no year dwt &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400032101022901 2008 118. 2 400032101022901 2009 117. 3 400032101022901 2010 118. 4 400032101022901 2011 118. 5 400032101022901 2012 118. 6 400032101022901 2013 118. 7 400032101022901 2014 116. 8 400032101022901 2015 117. 9 400038099244601 2007 24.3 10 400038099244601 2008 21.7 Since NE_gwl is missing geographic coordinates for the monitoring wells, we will download them using the readNWISsite() function and select only the monitoring wells that are inside the three counties. #--- get the list of site ids ---# NE_site_ls &lt;- NE_gwl$site_no %&gt;% unique() #--- get the locations of the site ids ---# sites_info &lt;- readNWISsite(siteNumbers = NE_site_ls) %&gt;% dplyr::select(site_no, dec_lat_va, dec_long_va) %&gt;% #--- turn the data into an sf object ---# st_as_sf(coords = c(&quot;dec_long_va&quot;, &quot;dec_lat_va&quot;)) %&gt;% #--- NAD 83 ---# st_set_crs(4269) %&gt;% #--- project to WGS UTM 14 ---# st_transform(32614) %&gt;% #--- keep only those located inside the three counties ---# .[three_counties, ] We now identify irrigation wells that are located within the 2-mile radius of the monitoring wells15. We first create polygons of 2-mile radius circles around the monitoring wells (see Figure 1.3). buffers &lt;- st_buffer(sites_info, dist = 2*1609.34) # in meter Figure 1.3: 2-mile buffers around USGS monitoring wells We now identify which irrigation wells are inside each of the buffers and get the associated groundwater pumping values. The st_join() function from the sf package will do the trick. #--- find irrigation wells inside the buffer and calculate total pumping ---# pumping_neaby &lt;- st_join(buffers, urnrd_gw_sf) Let’s take a look at a USGS monitoring well (site_no = \\(400012101323401\\)). filter(pumping_neaby, site_no == 400012101323401, year == 2010) Simple feature collection with 7 features and 4 fields geometry type: POLYGON dimension: XY bbox: xmin: 279690.7 ymin: 4428006 xmax: 286128 ymax: 4434444 CRS: EPSG:32614 site_no well_id year vol_af geometry 1 400012101323401 6331 2010 NA POLYGON ((286128 4431225, 2... 2 400012101323401 1883 2010 180.189 POLYGON ((286128 4431225, 2... 3 400012101323401 2006 2010 79.201 POLYGON ((286128 4431225, 2... 4 400012101323401 2538 2010 68.205 POLYGON ((286128 4431225, 2... 5 400012101323401 2834 2010 NA POLYGON ((286128 4431225, 2... 6 400012101323401 2834 2010 122.981 POLYGON ((286128 4431225, 2... 7 400012101323401 4983 2010 NA POLYGON ((286128 4431225, 2... As you can see, this well has seven irrigation wells within its 2-mile radius in 2010. Now, we will get total nearby pumping by monitoring well and year. ( total_pumping_nearby &lt;- pumping_neaby %&gt;% #--- calculate total pumping by monitoring well ---# group_by(site_no, year) %&gt;% summarize(nearby_pumping = sum(vol_af, na.rm = TRUE)) %&gt;% #--- NA means 0 pumping ---# mutate( nearby_pumping = ifelse(is.na(nearby_pumping), 0, nearby_pumping) ) ) Simple feature collection with 2396 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 237904.5 ymin: 4428006 xmax: 313476.5 ymax: 4545687 CRS: EPSG:32614 # A tibble: 2,396 x 4 # Groups: site_no [401] site_no year nearby_pumping geometry * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;POLYGON [m]&gt; 1 4000121013… 2007 571. ((286128 4431225, 286123.6 4431057, 286110.… 2 4000121013… 2008 772. ((286128 4431225, 286123.6 4431057, 286110.… 3 4000121013… 2009 500. ((286128 4431225, 286123.6 4431057, 286110.… 4 4000121013… 2010 451. ((286128 4431225, 286123.6 4431057, 286110.… 5 4000121013… 2011 545. ((286128 4431225, 286123.6 4431057, 286110.… 6 4000121013… 2012 1028. ((286128 4431225, 286123.6 4431057, 286110.… 7 4001301013… 2007 485. ((278847.4 4433844, 278843 4433675, 278829.… 8 4001301013… 2008 515. ((278847.4 4433844, 278843 4433675, 278829.… 9 4001301013… 2009 351. ((278847.4 4433844, 278843 4433675, 278829.… 10 4001301013… 2010 374. ((278847.4 4433844, 278843 4433675, 278829.… # … with 2,386 more rows We now merge nearby pumping data to the groundwater level data, and transform the data to obtain the dataset ready for regression analysis. #--- regression-ready data ---# reg_data &lt;- NE_gwl_march %&gt;% #--- pick monitoring wells that are inside the three counties ---# filter(site_no %in% unique(sites_info$site_no)) %&gt;% #--- merge with the nearby pumping data ---# left_join(., total_pumping_nearby, by = c(&quot;site_no&quot;, &quot;year&quot;)) %&gt;% #--- lead depth to water table ---# arrange(site_no, year) %&gt;% group_by(site_no) %&gt;% mutate( #--- lead depth ---# dwt_lead1 = dplyr::lead(dwt, n = 1, default = NA, order_by = year), #--- first order difference in dwt ---# dwt_dif = dwt_lead1 - dwt ) #--- take a look ---# dplyr::select(reg_data, site_no, year, dwt_dif, nearby_pumping) # A tibble: 2,022 x 4 # Groups: site_no [230] site_no year dwt_dif nearby_pumping &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 400130101374401 2011 NA 358. 2 400134101483501 2007 2.87 2038. 3 400134101483501 2008 0.78 2320. 4 400134101483501 2009 -2.45 2096. 5 400134101483501 2010 3.97 2432. 6 400134101483501 2011 1.84 2634. 7 400134101483501 2012 -1.35 985. 8 400134101483501 2013 44.8 NA 9 400134101483501 2014 -26.7 NA 10 400134101483501 2015 NA NA # … with 2,012 more rows Finally, we estimate the model using the lfe package. #--- load the lfe package for regression with fixed effects ---# library(lfe) #--- OLS with site_no and year FEs (error clustered by site_no) ---# reg_dwt &lt;- felm(dwt_dif ~ nearby_pumping | site_no + year | 0 | site_no, data = reg_data) Here is the regression result. stargazer(reg_dwt, type = &quot;html&quot;) Dependent variable: dwt_dif nearby_pumping 0.001*** (0.0001) Observations 1,342 R2 0.409 Adjusted R2 0.286 Residual Std. Error 1.493 (df = 1111) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.2 Demonstration 2: Precision Agriculture .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.2.1 Project Overview Objectives: Understand the impact of nitrogen on corn yield Understand how electric conductivity (EC) affects the marginal impact of nitrogen on corn Datasets: The experimental design of an on-farm randomized nitrogen trail on an 80-acre field Data generated by the experiment As-applied nitrogen rate Yield measures Electric conductivity Econometric Model: Here is the econometric model, we would like to estimate: \\[ yield_i = \\beta_0 + \\beta_1 N_i + \\beta_2 N_i^2 + \\beta_3 N_i \\cdot EC_i + \\beta_4 N_i^2 \\cdot EC_i + v_i \\] where \\(yield_i\\), \\(N_i\\), \\(EC_i\\), and \\(v_i\\) are corn yield, nitrogen rate, EC, and error term at subplot \\(i\\). Subplots which are obtained by dividing experimental plots into six of equal-area compartments. GIS tasks read spatial data in various formats: R data set (rds), shape file, and GeoPackage file use sf::st_read() create maps using the ggplot2 package use ggplot2::geom_sf() create subplots within experimental plots use-defined function that makes use of st_geometry() identify corn yield, as-applied nitrogen, and electric conductivity (EC) data points within each of the experimental plots and find their averages use sf::st_join() and sf::aggregate() Preparation for replication Source (run) Chap_1_Demonstration.R to define theme_map and gen_subplots() source(&quot;Codes/Chap_1_Demonstration.R&quot;) Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(dplyr) library(ggplot2) library(stargazer) 1.2.2 Project Demonstration We have already run a whole-field randomized nitrogen experiment on a 80-acre field. Let’s import the trial design data #--- read the trial design data ---# trial_design_16 &lt;- readRDS(&quot;./Data/trial_design.rds&quot;) Figure 1.4 is the map of the trial design generated using ggplot2 package.16. #--- map of trial design ---# ggplot(data = trial_design_16) + geom_sf(aes(fill = factor(NRATE))) + scale_fill_brewer(name = &quot;N&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map Figure 1.4: The Experimental Design of the Randomize Nitrogen Trial We have collected yield, as-applied NH3, and EC data. Let’s read in these datasets:17 #--- read yield data (sf data saved as rds) ---# yield &lt;- readRDS(&quot;./Data/yield.rds&quot;) #--- read NH3 data (GeoPackage data) ---# NH3_data &lt;- st_read(&quot;Data/NH3.gpkg&quot;) #--- read ec data (shape file) ---# ec &lt;- st_read(dsn=&quot;Data&quot;, &quot;ec&quot;) Figure 1.5 shows the spatial distribution of the three variables. A map of each variable was made first, and then they are combined into one figure using the patchwork package18. #--- yield map ---# g_yield &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = yield, aes(color = yield), size = 0.5) + scale_color_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_NH3 &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = NH3_data, aes(color = aa_NH3), size = 0.5) + scale_color_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- NH3 map ---# g_ec &lt;- ggplot() + geom_sf(data = trial_design_16) + geom_sf(data = ec, aes(color = ec), size = 0.5) + scale_color_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map #--- stack the figures vertically and display ---# library(patchwork) g_yield/g_NH3/g_ec Figure 1.5: Spatial distribution of yield, NH3, and EC Instead of using plot as the observation unit, we would like to create subplots inside each of the plots and make them the unit of analysis because it would avoid masking the within-plot spatial heterogeneity of EC. Here, we divide each plot into six subplots19: #--- generate subplots ---# subplots &lt;- lapply( 1:nrow(trial_design_16), function(x) gen_subplots(trial_design_16[x, ], 6) ) %&gt;% do.call(&#39;rbind&#39;, .) Figure 1.6 is a map of the subplots generated. #--- here is what subplots look like ---# ggplot(subplots) + geom_sf() + theme_for_map Figure 1.6: Map of the subplots We now identify the mean value of corn yield, nitrogen rate, and EC for each of the subplots using sf::aggregate() and sf::st_join(). ( reg_data &lt;- subplots %&gt;% #--- yield ---# st_join(., aggregate(yield, ., mean), join = st_equals) %&gt;% #--- nitrogen ---# st_join(., aggregate(NH3_data, ., mean), join = st_equals) %&gt;% #--- EC ---# st_join(., aggregate(ec, ., mean), join = st_equals) ) Simple feature collection with 816 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 560121.3 ymin: 4533410 xmax: 560758.9 ymax: 4533734 CRS: EPSG:26914 First 10 features: yield aa_NH3 ec geometry 1 220.1789 194.5155 28.33750 POLYGON ((560121.3 4533428,... 2 218.9671 194.4291 29.37667 POLYGON ((560134.5 4533428,... 3 220.3286 195.2903 30.73600 POLYGON ((560147.7 4533428,... 4 215.3121 196.7649 32.24000 POLYGON ((560160.9 4533429,... 5 216.9709 195.2199 36.27000 POLYGON ((560174.1 4533429,... 6 227.8761 184.6362 31.21000 POLYGON ((560187.3 4533429,... 7 226.0991 179.2143 31.99250 POLYGON ((560200.5 4533430,... 8 225.3973 179.0916 31.56500 POLYGON ((560213.7 4533430,... 9 221.1820 178.9585 33.01000 POLYGON ((560227 4533430, 5... 10 219.4659 179.0057 41.89750 POLYGON ((560240.2 4533430,... Here are the visualization of the subplot-level data (Figure 1.7): (ggplot() + geom_sf(data = reg_data, aes(fill = yield), color = NA) + scale_fill_distiller(name = &quot;Yield&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = aa_NH3), color = NA) + scale_fill_distiller(name = &quot;NH3&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map)/ (ggplot() + geom_sf(data = reg_data, aes(fill = ec), color = NA) + scale_fill_distiller(name = &quot;EC&quot;, palette = &quot;OrRd&quot;, direction = 1) + theme_for_map) Figure 1.7: Spatial distribution of subplot-level yield, NH3, and EC Let’s estimate the model and see the results: lm(yield ~ aa_NH3 + I(aa_NH3^2) + I(aa_NH3*ec) + I(aa_NH3^2*ec), data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: yield aa_NH3 -1.223 (1.308) I(aa_NH32) 0.004 (0.003) I(aa_NH3 * ec) 0.002 (0.003) I(aa_NH32 * ec) -0.00001 (0.00002) Constant 327.993*** (125.638) Observations 784 R2 0.010 Adjusted R2 0.005 Residual Std. Error 5.712 (df = 779) F Statistic 2.023* (df = 4; 779) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.3 Demonstration 3: Land Use and Weather .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.3.1 Project Overview Objective Understand the impact of past precipitation on crop choice in Iowa (IA). Datasets IA county boundary Regular grids over IA, created using sf::st_make_grid() PRISM daily precipitation data downloaded using prism package Land use data from the Cropland Data Layer (CDL) for IA in 2015, downloaded using cdlTools package Econometric Model The econometric model we would like to estimate is: \\[ CS_i = \\alpha + \\beta_1 PrN_{i} + \\beta_2 PrC_{i} + v_i \\] where \\(CS_i\\) is the area share of corn divided by that of soy in 2015 for grid \\(i\\) (we will generate regularly-sized grids in the Demo section), \\(PrN_i\\) is the total precipitation observed in April through May and September in 2014, \\(PrC_i\\) is the total precipitation observed in June through August in 2014, and \\(v_i\\) is the error term. To run the econometric model, we need to find crop share and weather variables observed at the grids. We first tackle the crop share variable, and then the precipitation variable. GIS tasks download Cropland Data Layer (CDL) data by USDA NASS use cdlTools::getCDL() download PRISM weather data use prism::get_prism_dailys() crop PRISM data to the geographic extent of IA use raster::crop() create regular grids within IA, which become the observation units of the econometric analysis use sf::st_make_grid() remove grids that share small area with IA use sf::st_intersection() and sf::st_area assign crop share and weather data to each of the generated IA grids (parallelized) use exactextractr::exact_extract() and future.apply::future_lapply() create maps use tmap package Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(data.table) library(dplyr) library(raster) library(lubridate) library(tmap) library(future.apply) library(stargazer) 1.3.2 Project Demonstration The geographic focus of this project is IA. Let’s get IAs state border (see Figure 1.8 for its map). library(&quot;maps&quot;) #--- IA state boundary ---# IA_boundary &lt;- st_as_sf(map(&quot;state&quot;, &quot;iowa&quot;, plot = FALSE, fill = TRUE)) Figure 1.8: IA state boundary The unit of analysis is artificial grids that we create over IA. The grids are regularly-sized rectangles except around the edge of the IA state border20. So, let’s create grids and remove those that do not overlap much with IA. #--- create regular grids (40 cells by 40 columns) over IA ---# IA_grids &lt;- IA_boundary %&gt;% #--- create grids ---# st_make_grid(, n = c(40, 40)) %&gt;% #--- convert to sf ---# st_as_sf() %&gt;% #--- find the intersections of IA grids and IA polygon ---# st_intersection(., IA_boundary) %&gt;% #--- calculate the area of each grid ---# mutate( area = as.numeric(st_area(.)), area_ratio = area/max(area) ) %&gt;% #--- keep only if the intersected area is large enough ---# filter(area_ratio &gt; 0.8) %&gt;% #--- assign grid id for future merge ---# mutate(grid_id = 1:nrow(.)) Here is what the generated grids look like (Figure 1.9): #--- plot the grids over the IA state border ---# tm_shape(IA_boundary) + tm_polygons(col = &quot;green&quot;) + tm_shape(IA_grids) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 1.9: Map of regular grids generated over IA Let’s work on crop share data. You can download CDL data using the getCDL() function from the cdlTools package. #--- load the cdlTools package ---# library(cdlTools) #--- download the CDL data for IA in 2015 ---# ( IA_cdl_2015 &lt;- getCDL(&quot;Iowa&quot;, 2015)$IA2015 ) class : RasterLayer dimensions : 11671, 17795, 207685445 (nrow, ncol, ncell) resolution : 30, 30 (x, y) extent : -52095, 481755, 1938165, 2288295 (xmin, xmax, ymin, ymax) crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs source : /private/var/folders/t4/5gnqprbn38nftyxkyk5hdwmd8hnypy/T/RtmpUSOffv/CDL_2015_19.tif names : CDL_2015_19 values : 0, 255 (min, max) The cells (30 meter by 30 meter) of the imported raster layer take a value ranging from 0 to 255. Corn and soybean are represented by 1 and 5, respectively (visualization of the CDL data is on the right). Figure 1.10 shows the map of one of the IA grids and the CDL cells it overlaps with. Figure 1.10: Spatial overlap of a IA grid and CDL layer We would like to extract all the cell values within the blue border. We use exactextractr::exact_extract() to identify which cells of the CDL raster layer fall within each of the IA grids and extract land use type values. We then find the share of corn and soybean for each of the grids. #--- reproject grids to the CRS of the CDL data ---# IA_grids_rp_cdl &lt;- st_transform(IA_grids, projection(IA_cdl_2015)) #--- load the exactextractr package for fast rater value extractions for polygons ---# library(exactextractr) #--- extract crop type values and find frequencies ---# cdl_extracted &lt;- exact_extract(IA_cdl_2015, IA_grids_rp_cdl) %&gt;% lapply(., function (x) data.table(x)[,.N, by = value]) %&gt;% #--- combine the list of data.tables into one data.table ---# rbindlist(idcol = TRUE) %&gt;% #--- find the share of each land use type ---# .[, share := N/sum(N), by = .id] %&gt;% .[, N := NULL] %&gt;% #--- keep only the share of corn and soy ---# .[value %in% c(1, 5), ] We then find the corn to soy ratio for each of the IA grids. #--- find corn/soy ratio ---# corn_soy &lt;- cdl_extracted %&gt;% #--- long to wide ---# dcast(.id ~ value, value.var = &quot;share&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;.id&quot;, &quot;1&quot;, &quot;5&quot;), c(&quot;grid_id&quot;, &quot;corn_share&quot;, &quot;soy_share&quot;)) %&gt;% #--- corn share divided by soy share ---# .[, c_s_ratio := corn_share / soy_share] We are still missing daily precipitation data at the moment. We have decided to use daily weather data from PRISM. Daily PRISM data is a raster data with the cell size of 4 km by 4 km. Figure 1.11 the right presents precipitation data downloaded for April 1, 2010. It covers the entire contiguous U.S. Figure 1.11: Map of PRISM raster data layer Let’s now download PRISM data21. This can be done using the get_prism_dailys() function from the prism package.22 options(prism.path = &quot;./Data/PRISM&quot;) get_prism_dailys( type = &quot;ppt&quot;, minDate = &quot;2014-04-01&quot;, maxDate = &quot;2014-09-30&quot;, keepZip = FALSE ) When we use get_prism_dailys() to download data23, it creates one folder for each day. So, I have about 180 folders inside the folder I designated as the download destination above with the options() function. We now try to extract precipitation value by day for each of the IA grids by geographically overlaying IA grids onto the PRISM data layer and identify which PRISM cells each of the IA grid encompass. Figure 1.12 shows how the first IA grid overlaps with the PRISM cells24. #--- read a PRISM dataset ---# prism_whole &lt;- raster(&quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot;) #--- align the CRS ---# IA_grids_rp_prism &lt;- st_transform(IA_grids, projection(prism_whole)) #--- crop the PRISM data for the 1st IA grid ---# PRISM_1 &lt;- crop(prism_whole, st_buffer(IA_grids_rp_prism[1, ], dist = 0.05)) #--- map them ---# tm_shape(PRISM_1) + tm_raster() + tm_shape(IA_grids_rp_prism[1, ]) + tm_polygons(alpha = 0) + tm_layout(frame = NA) Figure 1.12: Spatial overlap of an IA grid over PRISM cells As you can see, some PRISM grids are fully inside the analysis grid, while others are partially inside it. So, when assigning precipitation values to grids, we will use the coverage-weighted mean of precipitations25. Unlike the CDL layer, we have 183 raster layers to process. Fortunately, we can process many raster files at the same time very quickly by first “stacking” many raster files first and then applying the exact_extract() function. Using future_lapply(), we let \\(6\\) cores take care of this task with each processing 31 files, except one of them handling only 28 files.26 We first get all the paths to the PRISM files. #--- get all the dates ---# dates_ls &lt;- seq(as.Date(&quot;2014-04-01&quot;), as.Date(&quot;2014-09-30&quot;), &quot;days&quot;) #--- remove hyphen ---# dates_ls_no_hyphen &lt;- str_remove_all(dates_ls, &quot;-&quot;) #--- get all the prism file names ---# folder_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil&quot;) file_name &lt;- paste0(&quot;PRISM_ppt_stable_4kmD2_&quot;, dates_ls_no_hyphen, &quot;_bil.bil&quot;) file_paths &lt;- paste0(&quot;./Data/PRISM/&quot;, folder_name, &quot;/&quot;, file_name) #--- take a look ---# head(file_paths) [1] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140401_bil/PRISM_ppt_stable_4kmD2_20140401_bil.bil&quot; [2] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140402_bil/PRISM_ppt_stable_4kmD2_20140402_bil.bil&quot; [3] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140403_bil/PRISM_ppt_stable_4kmD2_20140403_bil.bil&quot; [4] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140404_bil/PRISM_ppt_stable_4kmD2_20140404_bil.bil&quot; [5] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140405_bil/PRISM_ppt_stable_4kmD2_20140405_bil.bil&quot; [6] &quot;./Data/PRISM/PRISM_ppt_stable_4kmD2_20140406_bil/PRISM_ppt_stable_4kmD2_20140406_bil.bil&quot; We now prepare for parallelized extractions and then implement them using future_apply(). #--- define the number of cores to use ---# num_core &lt;- 6 #--- prepare some parameters for parallelization ---# file_len &lt;- length(file_paths) files_per_core &lt;- ceiling(file_len/num_core) #--- prepare for parallel processing ---# plan(multiprocess, workers = num_core) #--- reproject IA grids to the CRS of PRISM data ---# IA_grids_reprojected &lt;- st_transform(IA_grids, projection(prism_whole)) Here is the function that we run in parallel over 6 cores. #--- define the function to extract PRISM values by block of files ---# extract_by_block &lt;- function(i, files_per_core) { #--- files processed by core ---# start_file_index &lt;- (i-1) * files_per_core + 1 #--- indexes for files to process ---# file_index &lt;- seq( from = start_file_index, to = min((start_file_index + files_per_core), file_len), by = 1 ) #--- extract values ---# data_temp &lt;- file_paths[file_index] %&gt;% # get file names #--- stack files ---# stack() %&gt;% #--- extract ---# exact_extract(., IA_grids_reprojected) %&gt;% #--- combine into one data set ---# rbindlist(idcol = &quot;ID&quot;) %&gt;% #--- wide to long ---# melt(id.var = c(&quot;ID&quot;, &quot;coverage_fraction&quot;)) %&gt;% #--- calculate &quot;area&quot;-weighted mean ---# .[, .(value = sum(value * coverage_fraction)/sum(coverage_fraction)), by = .(ID, variable)] return(data_temp) } Now, let’s run the function in parallel and calculate precipitation by period. #--- run the function ---# precip_by_period &lt;- future_lapply(1:num_core, function(x) extract_by_block(x, files_per_core)) %&gt;% rbindlist() %&gt;% #--- recover the date ---# .[, variable := as.Date(str_extract(variable, &quot;[0-9]{8}&quot;), &quot;%Y%m%d&quot;)] %&gt;% #--- change the variable name to date ---# setnames(&quot;variable&quot;, &quot;date&quot;) %&gt;% #--- define critical period ---# .[,critical := &quot;non_critical&quot;] %&gt;% .[month(date) %in% 6:8, critical := &quot;critical&quot;] %&gt;% #--- total precipitation by critical dummy ---# .[, .(precip=sum(value)), by = .(ID, critical)] %&gt;% #--- wide to long ---# dcast(ID ~ critical, value.var = &quot;precip&quot;) We now have grid-level crop share and precipitation data. Let’s merge them and run regression.27 #--- crop share ---# reg_data &lt;- corn_soy[precip_by_period, on = c(grid_id = &quot;ID&quot;)] #--- OLS ---# reg_results &lt;- lm(c_s_ratio ~ critical + non_critical, data = reg_data) Here is the regression results table. #--- regression table ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: c_s_ratio critical -0.002*** (0.0003) non_critical -0.0003 (0.0003) Constant 2.701*** (0.161) Observations 1,218 R2 0.058 Adjusted R2 0.056 Residual Std. Error 0.743 (df = 1215) F Statistic 37.234*** (df = 2; 1215) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Again, do not read into the results as the econometric model is terrible. 1.4 Demonstration 4: The Impact of Railroad Presence on Corn Planted Acreage .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.4.1 Project Overview Objective Understand the impact of railroad on corn planted acreage in Illinois Datasets USDA corn planted acreage for Illinois downloaded from the USDA NationalAgricultural Statistics Service (NASS) QuickStats service using tidyUSDA package US railroads (line data) downloaded from here Econometric Model We will estimate the following model: \\[ y_i = \\beta_0 + \\beta_1 RL_i + v_i \\] where \\(y_i\\) is corn planted acreage in county \\(i\\) in Illinois, \\(RL_i\\) is the total length of railroad, and \\(v_i\\) is the error term. GIS tasks Download USDA corn planted acreage by county as a spatial dataset (sf object) use tidyUSDA::getQuickStat() Import US railroad shape file as a spatial dataset (sf object) use sf:st_read() Spatially subset (crop) the railroad data to the geographic boundary of Illinois use sf_1[sf_2, ] Find railroads for each county (cross-county railroad will be chopped into pieces for them to fit within a single county) use sf::st_intersection() Calculate the travel distance of each railroad piece use sf::st_length() Preparation for replication Load (install first if you have not) the following packages (There are other packages that will be loaded during the demonstration). library(sf) library(ggplot2) library(dplyr) library(stargazer) 1.4.2 Project Demonstration We first download corn planted acreage data for 2018 from USDA NASS QuickStat service using tidyUSDA package28. library(keyring) library(tidyUSDA) ( IL_corn_planted &lt;- getQuickstat( key = key_get(&quot;usda_nass_qs_api&quot;) , program = &quot;SURVEY&quot;, data_item = &quot;CORN - ACRES PLANTED&quot;, geographic_level = &quot;COUNTY&quot;, state = &quot;ILLINOIS&quot;, year = &quot;2018&quot;, geometry = TRUE ) %&gt;% #--- keep only some of the variables ---# dplyr::select(year, NAME, county_code, short_desc, Value) ) Simple feature collection with 90 features and 5 fields (with 6 geometries empty) geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -91.51308 ymin: 36.9703 xmax: -87.4952 ymax: 42.50848 CRS: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs First 10 features: year NAME county_code short_desc Value 1 2018 Bureau 011 CORN - ACRES PLANTED 264000 2 2018 Carroll 015 CORN - ACRES PLANTED 134000 3 2018 Henry 073 CORN - ACRES PLANTED 226500 4 2018 Jo Daviess 085 CORN - ACRES PLANTED 98500 5 2018 Lee 103 CORN - ACRES PLANTED 236500 6 2018 Mercer 131 CORN - ACRES PLANTED 141000 7 2018 Ogle 141 CORN - ACRES PLANTED 217000 8 2018 Putnam 155 CORN - ACRES PLANTED 32300 9 2018 Rock Island 161 CORN - ACRES PLANTED 68400 10 2018 Stephenson 177 CORN - ACRES PLANTED 166500 geometry 1 MULTIPOLYGON (((-89.8569 41... 2 MULTIPOLYGON (((-90.16133 4... 3 MULTIPOLYGON (((-90.43227 4... 4 MULTIPOLYGON (((-90.50668 4... 5 MULTIPOLYGON (((-89.63118 4... 6 MULTIPOLYGON (((-90.99255 4... 7 MULTIPOLYGON (((-89.68598 4... 8 MULTIPOLYGON (((-89.33303 4... 9 MULTIPOLYGON (((-90.33573 4... 10 MULTIPOLYGON (((-89.9205 42... A nice thing about this function is that the data is downloaded as an sf object with county geometry with geometry = TRUE. So, you can immediately plot it (Figure 1.13) and use it for later spatial interactions without having to merge the downloaded data to an independent county boundary data.29. ggplot(IL_corn_planted) + geom_sf(aes(fill = Value/1000)) + scale_fill_distiller(name = &quot;Planted Acreage (1000 acres)&quot;, palette = &quot;YlOrRd&quot;, trans = &quot;reverse&quot;) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map Figure 1.13: Map of Con Planted Acreage in Illinois in 2018 Let’s import the U.S. railroad data and reproject to the CRS of IL_corn_planted: rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_transform(st_crs(IL_corn_planted)) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Dropbox/TeachingUNL/RGIS_Econ/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 CRS: 4269 Here is what it looks like: ggplot(rail_roads) + geom_sf() + theme_for_map Figure 1.14: Map of Railroads We now crop it to the Illinois state border (Figure 1.15) using sf_1[sf_2, ]: rail_roads_IL &lt;- rail_roads[IL_corn_planted, ] ggplot() + geom_sf(data = rail_roads_IL) + theme_for_map Figure 1.15: Map of railroads in Illinois Let’s now find railroads for each county, where cross-county railroads will be chopped into pieces so each piece fits completely within a single county, using st_intersection(). rails_IL_segmented &lt;- st_intersection(rail_roads_IL, IL_corn_planted) Here are the railroads for Richland County: ggplot() + geom_sf(data = dplyr::filter(IL_corn_planted, NAME == &quot;Richland&quot;)) + geom_sf(data = dplyr::filter(rails_IL_segmented, NAME == &quot;Richland&quot;), aes( color = LINEARID )) + theme( legend.position = &quot;bottom&quot; ) + theme_for_map We now calculate the travel distance (Great-circle distance) of each railroad piece using st_length() and then sum them up by county to find total railroad length by county. ( rail_length_county &lt;- mutate( rails_IL_segmented, length_in_m = as.numeric(st_length(rails_IL_segmented)), ) %&gt;% #--- group by county ID ---# group_by(county_code) %&gt;% #--- sum rail length by county ---# summarize(length_in_m = sum(length_in_m)) %&gt;% #--- geometry no longer needed ---# st_drop_geometry() ) # A tibble: 82 x 2 county_code length_in_m * &lt;chr&gt; &lt;dbl&gt; 1 001 77221. 2 003 77290. 3 007 36764. 4 011 255441. 5 015 161726. 6 017 30585. 7 019 389226. 8 021 155794. 9 023 78587. 10 025 92030. # … with 72 more rows We merge the railroad length data to the corn planted acreage data and estimate the model. reg_data &lt;- left_join(IL_corn_planted, rail_length_county, by = &quot;county_code&quot;) lm(Value ~ length_in_m, data = reg_data) %&gt;% stargazer(type = &quot;html&quot;) Dependent variable: Value length_in_m 0.092* (0.047) Constant 108,154.800*** (11,418.900) Observations 82 R2 0.046 Adjusted R2 0.034 Residual Std. Error 69,040.680 (df = 80) F Statistic 3.866* (df = 1; 80) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 1.5 Demonstration 5: Groundwater use for agricultural irrigation .book .book-body .page-wrapper .page-inner section.normal table { width:auto; } .book .book-body .page-wrapper .page-inner section.normal table td, .book .book-body .page-wrapper .page-inner section.normal table th, .book .book-body .page-wrapper .page-inner section.normal table tr { padding:0; border:0; background-color:#fff; } 1.5.1 Project Overview Objective + Understand the impact of monthly precipitation on groundwater use for agricultural irrigation Datasets Annual groundwater pumping by irrigation wells in Kansas for 2010 and 2011 (originally obtained from the Water Information Management &amp; Analysis System (WIMAS) database) Daymet30 daily precipitation and maximum temperature downloaded using daymetr package Econometric Model The econometric model we would like to estimate is: \\[ y_{i,t} = \\alpha + P_{i,t} \\beta + T_{i,t} \\gamma + \\phi_i + \\eta_t + v_{i,t} \\] where \\(y\\) is the total groundwater extracted in year \\(t\\), \\(P_{i,t}\\) and \\(T_{i,t}\\) is the collection of monthly total precipitation and mean maximum temperature April through September in year \\(t\\), respectively, \\(\\phi_i\\) is the well fixed effect, \\(\\eta_t\\) is the year fixed effect, and \\(v_{i,t}\\) is the error term. GIS tasks download Daymet precipitation and maximum temperature data for each well from within R in parallel use daymetr::download_daymet() and future.apply::future_lapply() 1.5.2 Project Demonstration We have already collected annual groundwater pumping data by irrigation wells in 2010 and 2011 in Kansas from the Water Information Management &amp; Analysis System (WIMAS) database. Let’s read in the groundwater use data. #--- read in the data ---# ( gw_KS_sf &lt;- readRDS( &quot;./Data/gw_KS_sf.rds&quot;) ) Simple feature collection with 56225 features and 3 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99561 xmax: -94.70746 ymax: 40.00191 CRS: EPSG:4269 First 10 features: well_id year af_used geometry 1 1 2010 67.00000 POINT (-100.4423 37.52046) 2 1 2011 171.00000 POINT (-100.4423 37.52046) 3 3 2010 30.93438 POINT (-100.7118 39.91526) 4 3 2011 12.00000 POINT (-100.7118 39.91526) 5 7 2010 0.00000 POINT (-101.8995 38.78077) 6 7 2011 0.00000 POINT (-101.8995 38.78077) 7 11 2010 154.00000 POINT (-101.7114 39.55035) 8 11 2011 160.00000 POINT (-101.7114 39.55035) 9 12 2010 28.17239 POINT (-95.97031 39.16121) 10 12 2011 89.53479 POINT (-95.97031 39.16121) We have 28553 wells in total, and each well has records of groundwater pumping (af_used) for years 2010 and 2011. Here is the spatial distribution of the wells. We now need to get monthly precipitation and maximum temperature data. We have decided that we use Daymet weather data. Here we use the download_daymet() function from the daymetr package31 that allows us to download all the weather variables for a specified geographic location and time period32. We write a wrapper function that downloads Daymet data and then processes it to find monthly total precipitation and mean maximum temperature33. We then loop over the 56225 wells, which is parallelized using the future_apply() function34 from the future.apply package. This process takes about an hour on my Mac with parallelization on 7 cores. The data is available in the data repository for this course (named as “all_daymet.rds”). library(daymetr) library(future.apply) #--- get the geographic coordinates of the wells ---# well_locations &lt;- gw_KS_sf %&gt;% unique(by = &quot;well_id&quot;) %&gt;% dplyr::select(well_id) %&gt;% cbind(., st_coordinates(.)) #--- define a function that downloads Daymet data by well and process it ---# get_daymet &lt;- function(i) { temp_site &lt;- well_locations[i, ]$well_id temp_long &lt;- well_locations[i, ]$X temp_lat &lt;- well_locations[i, ]$Y data_temp &lt;- download_daymet( site = temp_site, lat = temp_lat, lon = temp_long, start = 2010, end = 2011, #--- if TRUE, tidy data is returned ---# simplify = TRUE, #--- if TRUE, the downloaded data can be assigned to an R object ---# internal = TRUE ) %&gt;% data.table() %&gt;% #--- keep only precip and tmax ---# .[measurement %in% c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), ] %&gt;% #--- recover calender date from Julian day ---# .[, date := as.Date(paste(year, yday, sep = &quot;-&quot;), &quot;%Y-%j&quot;)] %&gt;% #--- get month ---# .[, month := month(date)] %&gt;% #--- keep only April through September ---# .[month %in% 4:9,] %&gt;% .[, .(site, year, month, date, measurement, value)] %&gt;% #--- long to wide ---# dcast(site + year + month + date~ measurement, value.var = &quot;value&quot;) %&gt;% #--- change variable names ---# setnames(c(&quot;prcp..mm.day.&quot;, &quot;tmax..deg.c.&quot;), c(&quot;prcp&quot;, &quot;tmax&quot;)) %&gt;% #--- find the total precip and mean tmax by month-year ---# .[, .(prcp = sum(prcp), tmax = mean(tmax)) , by = .(month, year)] %&gt;% .[, well_id := temp_site] return(data_temp) gc() } Here is what one run (for the first well) of get_daymet() returns #--- one run ---# ( returned_data &lt;- get_daymet(1)[] ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 6: 9 2010 15 31.43333 1 7: 4 2011 25 21.91667 1 8: 5 2011 26 26.30645 1 9: 6 2011 23 35.16667 1 10: 7 2011 35 38.62903 1 11: 8 2011 37 36.90323 1 12: 9 2011 9 28.66667 1 We get the number of cores you can use by RhpcBLASctl::get_num_procs() and parallelize the loop over wells using future_lapply().35 #--- prepare parallelized process ---# library(RhpcBLASctl) num_core &lt;- get_num_procs() - 1 #--- run get_daymet with parallelization ---# ( all_daymet &lt;- future_lapply(1:nrow(well_locations), get_daymet) %&gt;% rbindlist() ) month year prcp tmax well_id 1: 4 2010 42 20.96667 1 2: 5 2010 94 24.19355 1 3: 6 2010 70 32.51667 1 4: 7 2010 89 33.50000 1 5: 8 2010 63 34.17742 1 --- 336980: 5 2011 18 26.11290 78051 336981: 6 2011 25 34.61667 78051 336982: 7 2011 6 38.37097 78051 336983: 8 2011 39 36.66129 78051 336984: 9 2011 23 28.45000 78051 Before merging the Daymet data, we need to reshape the data into a wide format to get monthly precipitation and maximum temperature as columns. #--- long to wide ---# daymet_to_merge &lt;- dcast(all_daymet, well_id + year ~ month, value.var = c(&quot;prcp&quot;, &quot;tmax&quot;)) #--- take a look ---# daymet_to_merge well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56160: 78049 2011 27 6 38 37 34 36 22.81667 26.70968 56161: 78050 2010 35 48 68 111 56 9 21.38333 24.85484 56162: 78050 2011 26 7 44 38 34 35 22.76667 26.70968 56163: 78051 2010 30 62 48 29 76 3 21.05000 24.14516 56164: 78051 2011 33 18 25 6 39 23 21.90000 26.11290 tmax_6 tmax_7 tmax_8 tmax_9 1: 32.51667 33.50000 34.17742 31.43333 2: 35.16667 38.62903 36.90323 28.66667 3: 30.73333 32.80645 33.56452 28.93333 4: 30.08333 35.08065 32.90323 25.81667 5: 31.30000 33.12903 32.67742 30.16667 --- 56160: 35.01667 38.32258 36.54839 28.80000 56161: 33.16667 33.88710 34.40323 32.11667 56162: 34.91667 38.32258 36.54839 28.83333 56163: 32.90000 33.83871 34.38710 31.56667 56164: 34.61667 38.37097 36.66129 28.45000 Now, let’s merge the weather data to the groundwater pumping dataset. ( reg_data &lt;- data.table(gw_KS_sf) %&gt;% #--- keep only the relevant variables ---# .[, .(well_id, year, af_used)] %&gt;% #--- join ---# daymet_to_merge[., on = c(&quot;well_id&quot;, &quot;year&quot;)] ) well_id year prcp_4 prcp_5 prcp_6 prcp_7 prcp_8 prcp_9 tmax_4 tmax_5 1: 1 2010 42 94 70 89 63 15 20.96667 24.19355 2: 1 2011 25 26 23 35 37 9 21.91667 26.30645 3: 3 2010 85 62 109 112 83 41 19.93333 21.64516 4: 3 2011 80 104 44 124 118 14 18.40000 22.62903 5: 7 2010 44 83 23 99 105 13 18.81667 22.14516 --- 56221: 79348 2011 NA NA NA NA NA NA NA NA 56222: 79349 2011 NA NA NA NA NA NA NA NA 56223: 79367 2011 NA NA NA NA NA NA NA NA 56224: 79372 2011 NA NA NA NA NA NA NA NA 56225: 80930 2011 NA NA NA NA NA NA NA NA tmax_6 tmax_7 tmax_8 tmax_9 af_used 1: 32.51667 33.50000 34.17742 31.43333 67.00000 2: 35.16667 38.62903 36.90323 28.66667 171.00000 3: 30.73333 32.80645 33.56452 28.93333 30.93438 4: 30.08333 35.08065 32.90323 25.81667 12.00000 5: 31.30000 33.12903 32.67742 30.16667 0.00000 --- 56221: NA NA NA NA 76.00000 56222: NA NA NA NA 182.00000 56223: NA NA NA NA 0.00000 56224: NA NA NA NA 134.00000 56225: NA NA NA NA 23.69150 Let’s run regression and display the results. #--- load lfe package ---# library(lfe) #--- run FE ---# reg_results &lt;- felm( af_used ~ prcp_4 + prcp_5 + prcp_6 + prcp_7 + prcp_8 + prcp_9 + tmax_4 + tmax_5 + tmax_6 + tmax_7 + tmax_8 + tmax_9 |well_id + year| 0 | well_id, data = reg_data ) #--- display regression results ---# stargazer(reg_results, type = &quot;html&quot;) Dependent variable: af_used prcp_4 -0.053*** (0.017) prcp_5 0.112*** (0.010) prcp_6 -0.073*** (0.008) prcp_7 0.014 (0.010) prcp_8 0.093*** (0.014) prcp_9 -0.177*** (0.025) tmax_4 9.159*** (1.227) tmax_5 -7.505*** (1.062) tmax_6 15.134*** (1.360) tmax_7 3.969** (1.618) tmax_8 3.420*** (1.066) tmax_9 -11.803*** (1.801) Observations 55,754 R2 0.942 Adjusted R2 0.883 Residual Std. Error 46.864 (df = 27659) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 That’s it. Do not bother to try to read into the regression results. Again, this is just an illustration of how R can be used to prepare a regression-ready dataset with spatial variables. Note that this lecture does not deal with spatial econometrics at all. This lecture is about spatial data processing, not spatial econometrics. This is a great resource for spatial econometrics in R.↩ I welcome any suggestions to improve the reading experience of unexperienced R users.↩ I thought about using the here package, but I found it a bit confusing for unexperienced R users.↩ the distance from the surface to the top of the aquifer↩ For our geographic focus of southwest Nebraska, corn is the dominant crop type. Irrigation for corn happens typically between April through September. For example, this means that changes in groundwater level (\\(y_{i,2012} - y_{i,2011}\\)) captures the impact of groundwater pumping that occurred April through September in 2011.↩ month() and year() are from the lubridate package. They extract month and year from a Date object.↩ This can alternatively be done using the st_is_within_distance() function.↩ theme_for_map is a user defined object that defines the theme of figures generated using ggplot2 for this section. You can find it in Chap_1_Demonstration.R.↩ Here we are demonstrating that R can read spatial data in different formats. R can read spatial data of many other formats. Here, we are reading a shapefile (.shp) and GeoPackage file (.gpkg).↩ Here is its github page. See the bottom of the page to find vignettes.↩ gen_subplots is a user-defined function. See Chap_1_Demonstration.R.↩ We by no means are saying that this is the right geographical unit of analysis. This is just about demonstrating how R can be used for analysis done at the higher spatial resolution than county.↩ You do not have to run this code to download the data. It is included in the data folder for replication (here).↩ prism github page↩ For this project, I could have just used monthly PRISM data, which can be downloaded using the get_prism_monthlys() function. But, in many applications, daily data is necessary, so I wanted to illustrate how to download and process them.↩ Do not use st_buffer() for spatial objects in geographic coordinates (latitude, longitude) if you intend to use the created buffers for any serious IA (it is difficult to get the right distance parameter anyway.). Significant distortion will be introduced to the buffer due to the fact that one degree in latitude and longitude means different distances at the latitude of IA. Here, I am just creating a buffer to extract PRISM cells to display on the map.↩ In practice, this may not be advisable. The coverage fraction calculation by exact_extract() is done using latitude and longitude. Therefore, the relative magnitude of the fraction numbers incorrectly reflects the actual relative magnitude of the overlapped area. When the spatial resolution of the sources grids (grids from which you extract values) is much smaller relative to that of the target grids (grids to which you assign values to), then a simple average would be very similar to a coverage-weighted mean. For example, CDL consists of 30m by 30m grids, and more than \\(1,000\\) grids are inside one analysis grid.↩ Parallelization of extracting values from many raster layers for polygons are discussed in much more detail in Chapter ??. When I tried stacking all 183 files into one stack and applying exact_extract, it did not finish the job after over five minutes. So, I terminated the process in the middle. The parallelized version gets the job done in about \\(30\\) seconds on my desktop.↩ We can match on grid_id from corn_soy and ID from “precip_by_period” because grid_id is identical with the row number and ID variables were created so that the ID value of \\(i\\) corresponds to \\(i\\) th row of IA_grids.↩ In order to actually download the data, you need to obtain the API key here. Once the API key was obtained, I stored it using set_key() from the keyring package, which was named “usda_nass_qs_api”. In the code to the left, I retrieve the API key using key_get(&quot;usda_nass_qs_api&quot;) in the code.↩ theme_for_map is a user defined object that defines the theme of figures generated using ggplot2 for this section. You can find it in Chap_1_Demonstration.R.↩ Daymet website↩ daymetr vignette↩ See here for a fuller explanation of how to use the daymetr package.↩ This may not be ideal for a real research project because the original raw data is not kept. It is often the case that your econometric plan changes on the course of your project (e.g., using other weather variables or using different temporal aggregation of weather variables instead of monthly aggregation). When this happens, you need to download the same data all over again.↩ For parallelized computation, see Chapter ??↩ For Mac users, mclapply or pbmclapply (mclapply with progress bar) are good alternatives.↩ "],
["vector-basics.html", "Chapter 2 Handle vector data using the sf package Before you start 2.1 Spatial Data Structure 2.2 Simple feature geometry, simple feature geometry list-column, and simple feature 2.3 Reading and writing vector data 2.4 Projection with a different Coordinate Reference Systems 2.5 Non-spatial transformation of sf 2.6 Turning a data.frame of points into an sf 2.7 Conversion to and from sp 2.8 Geometrical operations", " Chapter 2 Handle vector data using the sf package Before you start In this chapter we learn how to use the sf package to handle and operate on spatial datasets. The sf package uses the class of simple feature (sf)36 for spatial objects in R. We first learn how sf objects store and represent spatial datasets. We then move on to the following practical topics: read and write a shapefile and spatial data in other formats (and why you might not want to use the shapefile system any more, but use other alternative formats) project and reproject spatial objects convert sf objects into sp objective, vice versa confirm that dplyr works well with sf objects implement non-interactive (does not involve two sf objects) geometric operations on sf objects create buffers find the area of polygons find the centroid of polygons calculate the length of lines sf or sp? The sf package was designed to replace the sp package, which has been one of the most popular and powerful spatial packages in R for more than a decade. It has been about four years since sf package was first registered on CRAN. A couple of years back, many other spatial packages did not have support for the package yet. In this blog post that asked the question of whether one should learn sp of sf, the author said: &quot;That’s a tough question. If you have time, I would say, learn to use both. sf is pretty new, so a lot of packages that depend on spatial classes still rely on sp. So you will need to know sp if you want to do any integration with many other packages, including raster (as of March 2018). However, in the future we should see an increasing shift toward the sf package and greater use of sf classes in other packages. I also think that sf is easier to learn to use than sp.&quot; The future has come, and it’s not a tough question anymore. I cannot think of any major spatial packages that do not support sf package, and sf has largely becomes the standard for handling vector data in \\(R\\). Thus, this lecture note does not cover how to use sp at all.37 sf has several advantages over sp package (Pebesma 2018).38 First, it cut off the tie that sp had with ESRI shapefile system, which has somewhat loose way of representing spatial data. Instead, it uses simple feature access, which is an open standard supported by Open Geospatial Consortium (OGC). Another important benefit is its compatibility with the tidyverse package, which include widely popular packages like ggplot2 and dplyr. Consequently, map-making with ggplot() and data wrangling with a family of dplyr functions come very natural to many \\(R\\) users. sp objects have different slots for spatial information and attributes data, and they are not amenable to dplyr way of data transformation. 2.1 Spatial Data Structure Here, we learn how the sf package stores spatial data, along with the definition of three key sf object classes: simple feature geometry (sfg), simple feature geometry list-column (sfc), and simple feature (sf). The sf package provides a simply way of storing geographic information and the attributes of the geographic units in a single dataset. This special type of dataset is called simple feature (sf). It is best to take a look at an example to see how this is achieved. We use North Carolina county boundaries with county attributes (Figure 2.1). #--- a dataset that comes with the sf package ---# nc &lt;- st_read(system.file(&quot;shape/nc.shp&quot;, package=&quot;sf&quot;)) Reading layer `nc&#39; from data source `/Library/Frameworks/R.framework/Versions/4.0/Resources/library/sf/shape/nc.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 100 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 CRS: 4267 Figure 2.1: North Carolina county boundary As you can see below, this dataset is of class sf (and data.frame at the same time). class(nc) [1] &quot;sf&quot; &quot;data.frame&quot; Now, let’s take a look inside of nc. #--- take a look at the data ---# head(nc) Simple feature collection with 6 features and 14 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 CRS: 4267 AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 SID74 1 0.114 1.442 1825 1825 Ashe 37009 37009 5 1091 1 2 0.061 1.231 1827 1827 Alleghany 37005 37005 3 487 0 3 0.143 1.630 1828 1828 Surry 37171 37171 86 3188 5 4 0.070 2.968 1831 1831 Currituck 37053 37053 27 508 1 5 0.153 2.206 1832 1832 Northampton 37131 37131 66 1421 9 6 0.097 1.670 1833 1833 Hertford 37091 37091 46 1452 7 NWBIR74 BIR79 SID79 NWBIR79 geometry 1 10 1364 0 19 MULTIPOLYGON (((-81.47276 3... 2 10 542 3 12 MULTIPOLYGON (((-81.23989 3... 3 208 3616 6 260 MULTIPOLYGON (((-80.45634 3... 4 123 830 2 145 MULTIPOLYGON (((-76.00897 3... 5 1066 1606 3 1197 MULTIPOLYGON (((-77.21767 3... 6 954 1838 5 1237 MULTIPOLYGON (((-76.74506 3... Just like a regular data.frame, you see a number of variables (attributes) except that you have a variable called geometry at the end. Each row represents a single geographic unit (here, county). Ashe County (1st row) has area of \\(0.114\\), FIPS code of \\(37009\\), and so on. And the entry in geometry column at the first row represents the geographic information of Ashe County. An entry in the geometry column is a simple feature geometry (sfg), which is an \\(R\\) object that represents the geographic information of a single geometric feature (county in this example). There are different types of sfgs (POINT, LINESTRING, POLYGON, MULTIPOLYGON, etc). Here, sfgs representing counties in NC are of type MULTIPOLYGON. Let’s take a look inside the sfg for Ashe County using st_geometry(). st_geometry(nc[1, ])[[1]][[1]] [[1]] [,1] [,2] [1,] -81.47276 36.23436 [2,] -81.54084 36.27251 [3,] -81.56198 36.27359 [4,] -81.63306 36.34069 [5,] -81.74107 36.39178 [6,] -81.69828 36.47178 [7,] -81.70280 36.51934 [8,] -81.67000 36.58965 [9,] -81.34530 36.57286 [10,] -81.34754 36.53791 [11,] -81.32478 36.51368 [12,] -81.31332 36.48070 [13,] -81.26624 36.43721 [14,] -81.26284 36.40504 [15,] -81.24069 36.37942 [16,] -81.23989 36.36536 [17,] -81.26424 36.35241 [18,] -81.32899 36.36350 [19,] -81.36137 36.35316 [20,] -81.36569 36.33905 [21,] -81.35413 36.29972 [22,] -81.36745 36.27870 [23,] -81.40639 36.28505 [24,] -81.41233 36.26729 [25,] -81.43104 36.26072 [26,] -81.45289 36.23959 [27,] -81.47276 36.23436 As you can see, the sfg consists of a number of points (pairs of two numbers). Connecting the points in the order they are stored delineate the Ashe County boundary. plot(st_geometry(nc[1, ])) We will take a closer look at different types of sfg in the next section. Finally, the geometry variable is a list of individual sfgs, called simple feature geometry list-column (sfc). dplyr::select(nc, geometry) Simple feature collection with 100 features and 0 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 CRS: 4267 First 10 features: geometry 1 MULTIPOLYGON (((-81.47276 3... 2 MULTIPOLYGON (((-81.23989 3... 3 MULTIPOLYGON (((-80.45634 3... 4 MULTIPOLYGON (((-76.00897 3... 5 MULTIPOLYGON (((-77.21767 3... 6 MULTIPOLYGON (((-76.74506 3... 7 MULTIPOLYGON (((-76.00897 3... 8 MULTIPOLYGON (((-76.56251 3... 9 MULTIPOLYGON (((-78.30876 3... 10 MULTIPOLYGON (((-80.02567 3... Elements of a geometry list-column are allowed to be different in nature from other elements39. In the nc data, all the elements (sfgs) in geometry column are MULTIPOLYGON. However, you could also have LINESTRING or POINT objects mixed with MULTIPOLYGONS objects in a single sf object if you would like. 2.2 Simple feature geometry, simple feature geometry list-column, and simple feature Here, we learn how different types of sfg are constructed. We also learn how to create sfc and sf from sfg from scratch.40 2.2.1 Simple feature geometry (sfg) The sf package uses a class of sfg (simple feature geometry) objects to represent a geometry of a single geometric feature (say, a city as a point, a river as a line, county and school district as polygons). There are different types of sfgs. Here are some example feature types that we commonly encounter as an economist41: POINT: area-less feature that represents a point (e.g., well, city, farmland) LINESTRING: (e.g., a tributary of a river) MULTILINESTRING: (e.g., river with more than one tributaries) POLYGON: geometry with a positive area (e.g., county, state, country) MULTIPOLYGON: collection of polygons to represent a single object (e.g., countries with islands: U.S., Japan) POINT is the simplest geometry type, and is represented by a vector of two42 numeric values. An example below shows how a POINT feature can be made from scratch43: #--- create a POINT ---# a_point &lt;- st_point(c(2,1)) The st_point() function creates a POINT object when supplied with a vector of two numeric values. If you check the class of the newly created object, #--- check the class of the object ---# class(a_point) [1] &quot;XY&quot; &quot;POINT&quot; &quot;sfg&quot; You can see that it’s indeed a POINT object. But, it’s also an sfg object. So, a_point is an sfg object of type POINT. A LINESTRING objects are represented by a sequence of points: #--- collection of points in a matrix form ---# s1 &lt;- rbind(c(2,3),c(3,4),c(3,5),c(1,5)) #--- see what s1 looks like ---# s1 [,1] [,2] [1,] 2 3 [2,] 3 4 [3,] 3 5 [4,] 1 5 #--- create a &quot;LINESTRING&quot; ---# a_linestring &lt;- st_linestring(s1) #--- check the class ---# class(a_linestring) [1] &quot;XY&quot; &quot;LINESTRING&quot; &quot;sfg&quot; s1 is a matrix where each row represents a point. By applying st_linestring() function to s1, you create a LINESTRING object. Let’s see what the line looks like. plot(a_linestring) As you can see, each pair of consecutive points in the matrix are connected by a straight line to form a line. A POLYGON is very similar to LINESTRING in the manner it is represented. #--- collection of points in a matrix form ---# p1 &lt;- rbind(c(0,0), c(3,0), c(3,2), c(2,5), c(1,3), c(0,0)) #--- see what s1 looks like ---# p1 [,1] [,2] [1,] 0 0 [2,] 3 0 [3,] 3 2 [4,] 2 5 [5,] 1 3 [6,] 0 0 #--- create a &quot;LINESTRING&quot; ---# a_polygon &lt;- st_polygon(list(p1)) #--- check the class ---# class(a_polygon) [1] &quot;XY&quot; &quot;POLYGON&quot; &quot;sfg&quot; #--- see what it looks like ---# plot(a_polygon) Just like the LINESTRING object we created earlier, a POLYGON is represented by a collection of points. The biggest difference between them is that we need to have some positive area enclosed by lines connecting the points. To do that, you have the the same point for the first and last points to close the loop: here, it’s c(0,0). A POLYGON can have a hole in it. The first matrix of a list becomes the exterior ring, and all the subsequent matrices will be holes within the exterior ring. #--- a hole within p1 ---# p2 &lt;- rbind(c(1,1), c(1,2), c(2,2), c(1,1)) #--- create a polygon with hole ---# a_plygon_with_a_hole &lt;- st_polygon(list(p1,p2)) #--- see what it looks like ---# plot(a_plygon_with_a_hole) You can create a MULTIPOLYGON object in a similar manner. The only difference is that you supply a list of lists of matrices, with each inner list representing a polygon. An example below: #--- second polygon ---# p3 &lt;- rbind(c(4,0), c(5,0), c(5,3), c(4,2), c(4,0)) #--- create a multipolygon ---# a_multipolygon &lt;- st_multipolygon(list(list(p1,p2), list(p3))) #--- see what it looks like ---# plot(a_multipolygon) Each of list(p1,p2), list(p3,p4), list(p5) represents a polygon. You supply a list of these lists to the st_multipolygon() function to make a MULTIPOLYGON object. 2.2.2 Create simple feature geometry list-column (sfc) and simple feature (sf) from scratch To make a simple feature geometry list-column (sfc), you can simply supply a list of sfg to the st_sfc() function as follows: #--- create an sfc ---# sfc_ex &lt;- st_sfc(list(a_point,a_linestring,a_polygon,a_multipolygon)) To create an sf object, you first add an sfc as a column to a data.frame. #--- create a data.frame ---# df_ex &lt;- data.frame( name=c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;) ) #--- add the sfc as a column ---# df_ex$geometry &lt;- sfc_ex #--- take a look ---# df_ex name geometry 1 A POINT (2 1) 2 B LINESTRING (2 3, 3 4, 3 5, ... 3 C POLYGON ((0 0, 3 0, 3 2, 2 ... 4 D MULTIPOLYGON (((0 0, 3 0, 3... At this point, it is not recognized as an sf by R yet. #--- see what it looks like (this is not an sf object yet) ---# class(df_ex) [1] &quot;data.frame&quot; You can register it as an sf object using st_as_sf(). #--- let R recognize the data frame as sf ---# sf_ex &lt;- st_as_sf(df_ex) #--- see what it looks like ---# sf_ex Simple feature collection with 4 features and 1 field geometry type: GEOMETRY dimension: XY bbox: xmin: 0 ymin: 0 xmax: 5 ymax: 5 CRS: NA name geometry 1 A POINT (2 1) 2 B LINESTRING (2 3, 3 4, 3 5, ... 3 C POLYGON ((0 0, 3 0, 3 2, 2 ... 4 D MULTIPOLYGON (((0 0, 3 0, 3... As you can see sf_ex is now recognized also as an sf object. #--- check the class ---# class(sf_ex) [1] &quot;sf&quot; &quot;data.frame&quot; 2.3 Reading and writing vector data I claimed that you do not need ArcGIS \\(99\\%\\) of your work as an economist. However, the vast majority of people still use ArcGIS to handle spatial data, which has its own system of storing spatial data44 called shapefile. So, chances are that your collaborators still use shapefiles. Moreover, there are many GIS data online that are available only as shapefiles. So, it is important to learn how to read and write shapefiles. 2.3.1 Reading a shapefile We can use st_read() function to read a shapefile. It reads in a shapefile and then turn the data into an sf object. Let’s take a look at an example. #--- read a NE county boundary shapefile ---# nc_loaded &lt;- st_read(dsn = &quot;./Data&quot;, &quot;nc&quot;) Reading layer `nc&#39; from data source `/Users/tmieno2/Box/Teaching/AAEA R/GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 100 features and 1 field geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965 CRS: 4267 Typically, you have two arguments to specify for st_read(). The first one is dsn, which is basically the path to the shapefile you want to import. The second one is the name of the shapefile. Notice that you do not add .shp extension to the file name: NE_county, not NE_county.shp.45. 2.3.2 Writing to a shapefile Writing an sf object as a shapefile is just as easy. You use the st_write() function, with the first argument being the name of the sf object you are exporting, and the second being the name of the new shapefile. For example, the code below will export an sf object called NE_county as NE_county_2.shp (along with other supporting files). st_write(nc_loaded, dsn=&quot;./Data&quot;, &quot;nc&quot;, driver=&quot;ESRI Shapefile&quot;, append = FALSE) append = FALSE forces writing the data when there already exists a file with the same name. Without the option, this happens. st_write(nc_loaded, dsn=&quot;./Data&quot;, &quot;nc&quot;, driver=&quot;ESRI Shapefile&quot;) Layer nc in dataset ./Data already exists: use either append=TRUE to append to layer or append=FALSE to overwrite layer Error in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options), : Dataset already exists. 2.3.3 Better alternatives Now, if your collaborator is using ArcGIS and demanding that he/she needs a shapefile for his/her work, sure you can use the above command to write a shapefile. But, there is really no need to work with the shapefile system. One of the alternative data formats that are considered superior to the shapefile system is GeoPackage46, which overcomes various limitations associated with shapefile47. Unlike the shapefile system, it produces only a single file with .gpkg extension. Note that GeoPackage file can also be easily read into ArcGIS. So, it might be worthwhile to convince your collaborators to stop using shapefiles and start using GeoPackage. #--- write as a gpkg file ---# st_write(nc, dsn = &quot;./Data/nc.gpkg&quot;) #--- read a gpkg file ---# nc &lt;- st_read(&quot;./Data/nc.gpkg&quot;) Or better yet, if your collaborator uses R (or if it is only you who is going to use the data), then just save it as an rds file using saveRDS(), which can be of course read using readRDS(). #--- save as an rds ---# saveRDS(nc, &quot;/Users/tmieno2/Box/Teaching/AAEA R/GIS/nc_county.rds&quot;) #--- read an rds ---# nc &lt;- readRDS(&quot;/Users/tmieno2/Box/Teaching/AAEA R/GIS/nc_county.rds&quot;) The use of rds files can be particularly attractive when the dataset is large because rds files are typically more memory efficient than shapefiles, eating up less of your disk memory. 2.4 Projection with a different Coordinate Reference Systems You often need to reproject an sf using a different coordinate reference system (CRS) because you need it to have the same CRS as an sf object that you are interacting it with (spatial join) or mapping it with. In order to check the current CRS for an sf object, you can use the st_crs() function. st_crs(nc) Coordinate Reference System: User input: 4267 wkt: GEOGCS[&quot;NAD27&quot;, DATUM[&quot;North_American_Datum_1927&quot;, SPHEROID[&quot;Clarke 1866&quot;,6378206.4,294.9786982138982, AUTHORITY[&quot;EPSG&quot;,&quot;7008&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;6267&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4267&quot;]] wkt stands for Well Known Text48, which is one of many many formats to store CRS information.49 4267 is the SRID (Spatial Reference System Identifier) defined by the European Petroleum Survey Group (EPSG) for the CRS50. When you transform your sf using a different CRS, you can use its EPSG number if the CRS has an EPSG number.51 Let’s transform the sf to WGS 84 (another commonly used GCS), whose EPSG number is 4326. We can use the st_transform() function to achieve that, with the first argument being the sf object you are transforming and the second being the EPSG number of the new CRS. #--- transform ---# nc_wgs84 &lt;- st_transform(nc, 4326) #--- check if the transformation was successful ---# st_crs(nc_wgs84) Coordinate Reference System: User input: EPSG:4326 wkt: GEOGCS[&quot;WGS 84&quot;, DATUM[&quot;WGS_1984&quot;, SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563, AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]] Notice that wkt was also altered accordingly to reflect the change in CRS: datum was changed to WGS 84. Now, let’s transform (reproject) the data using NAD83 / UTM zone 17N CRS. Its EPSG number is \\(26917\\).52 So, the following code does the job. #--- transform ---# nc_utm17N &lt;- st_transform(nc_wgs84, 26917) #--- check if the transformation was successful ---# st_crs(nc_utm17N) Coordinate Reference System: User input: EPSG:26917 wkt: PROJCS[&quot;NAD83 / UTM zone 17N&quot;, GEOGCS[&quot;NAD83&quot;, DATUM[&quot;North_American_Datum_1983&quot;, SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101, AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]], PROJECTION[&quot;Transverse_Mercator&quot;], PARAMETER[&quot;latitude_of_origin&quot;,0], PARAMETER[&quot;central_meridian&quot;,-81], PARAMETER[&quot;scale_factor&quot;,0.9996], PARAMETER[&quot;false_easting&quot;,500000], PARAMETER[&quot;false_northing&quot;,0], UNIT[&quot;metre&quot;,1, AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], AXIS[&quot;Easting&quot;,EAST], AXIS[&quot;Northing&quot;,NORTH], AUTHORITY[&quot;EPSG&quot;,&quot;26917&quot;]] As you can see in its CRS information, the projection system is now UTM zone 17N. You often need to change the CRS of an sf object when you interact53 it with another sf object. In such a case, you can extract the CRS of the other sf object using st_crs() and use it for transformation54. #--- transform ---# nc_utm17N_2 &lt;- st_transform(nc_wgs84, st_crs(nc_utm17N)) #--- check if the transformation was successful ---# st_crs(nc_utm17N_2) Coordinate Reference System: User input: EPSG:26917 wkt: PROJCS[&quot;NAD83 / UTM zone 17N&quot;, GEOGCS[&quot;NAD83&quot;, DATUM[&quot;North_American_Datum_1983&quot;, SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101, AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]], PROJECTION[&quot;Transverse_Mercator&quot;], PARAMETER[&quot;latitude_of_origin&quot;,0], PARAMETER[&quot;central_meridian&quot;,-81], PARAMETER[&quot;scale_factor&quot;,0.9996], PARAMETER[&quot;false_easting&quot;,500000], PARAMETER[&quot;false_northing&quot;,0], UNIT[&quot;metre&quot;,1, AUTHORITY[&quot;EPSG&quot;,&quot;9001&quot;]], AXIS[&quot;Easting&quot;,EAST], AXIS[&quot;Northing&quot;,NORTH], AUTHORITY[&quot;EPSG&quot;,&quot;26917&quot;]] 2.5 Non-spatial transformation of sf An important feature of an sf object is that it is basically a data.frame with geometric information stored as a variable (column). This means that transforming an sf object works just like transforming a data.frame. Basically, everything you can do to a data.frame, you can do to an sf as well. The code below just provides an example of basic operations including select(), filter(), and mutate() in action with an sf object to just confirm that dplyr operations works with an sf object just like a data.frame. #--- here is what the data looks like ---# dplyr::select(wells_sf, wellid, nrdname, acres, regdate, nrdname) Simple feature collection with 105822 features and 4 fields geometry type: POINT dimension: XY bbox: xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942 CRS: unknown First 10 features: wellid nrdname acres regdate geometry 1 2 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 2 3 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 3 4 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 4 5 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 5 6 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 6 7 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) 7 8 South Platte 113 8/7/37 POINT (-103.5257 41.24492) 8 10 South Platte 160 5/4/38 POINT (-103.0284 41.13243) 9 11 Middle Republican 807 5/6/38 POINT (-101.1193 40.3527) 10 12 Middle Republican 148 11/29/77 POINT (-101.1146 40.35631) #--- do some transformations ---# wells_sf %&gt;% #--- select variables (geometry will always remain after select) ---# dplyr::select(wellid, nrdname, acres, regdate, nrdname) %&gt;% #--- removes observations with acre &lt; 30 ---# filter(acres &gt; 30) %&gt;% #--- hectare instead of acre ---# mutate(hectare = acres * 0.404686) Simple feature collection with 63271 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -104.0529 ymin: 40.00161 xmax: -96.87681 ymax: 41.73599 CRS: unknown First 10 features: wellid nrdname acres regdate geometry hectare 1 2 Central Platte 160 12/30/55 POINT (-99.58401 40.69825) 64.74976 2 3 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 3 4 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 4 5 South Platte 46 4/29/31 POINT (-102.6249 41.11699) 18.61556 5 6 Central Platte 160 8/29/32 POINT (-99.6258 40.73268) 64.74976 6 7 Central Platte 120 2/15/35 POINT (-99.64524 40.73164) 48.56232 7 8 South Platte 113 8/7/37 POINT (-103.5257 41.24492) 45.72952 8 10 South Platte 160 5/4/38 POINT (-103.0284 41.13243) 64.74976 9 11 Middle Republican 807 5/6/38 POINT (-101.1193 40.3527) 326.58160 10 12 Middle Republican 148 11/29/77 POINT (-101.1146 40.35631) 59.89353 Now, let’s try to get a summary of a variable by group using the group_by() and summarize() functions. #--- summary by group ---# wells_by_nrd &lt;- wells_sf %&gt;% #--- group by nrdname ---# group_by(nrdname) %&gt;% #--- summarize ---# summarize(tot_acres = sum(acres, na.rm = TRUE)) #--- take a look ---# wells_by_nrd Simple feature collection with 9 features and 2 fields geometry type: MULTIPOINT dimension: XY bbox: xmin: -104.0531 ymin: 40.00161 xmax: -96.87681 ymax: 41.85942 CRS: unknown # A tibble: 9 x 3 nrdname tot_acres geometry &lt;chr&gt; &lt;dbl&gt; &lt;MULTIPOINT [°]&gt; 1 Central Plat… 1890918. ((-100.2329 41.14385), (-100.2328 41.05678), (-100.23… 2 Little Blue 995900. ((-98.72659 40.30463), (-98.72434 40.68021), (-98.724… 3 Lower Republ… 543079. ((-100.1968 40.32314), (-100.196 40.33553), (-100.195… 4 Middle Repub… 443472. ((-101.3691 40.1208), (-101.3448 40.64638), (-101.344… 5 South Platte 216109. ((-104.0531 41.18248), (-104.053 41.19347), (-104.052… 6 Tri-Basin 847058. ((-100.0927 40.42312), (-100.0904 40.43158), (-100.08… 7 Twin Platte 452678. ((-102.0557 41.05204), (-102.0556 41.05488), (-102.05… 8 Upper Big Bl… 1804782. ((-98.83619 40.85932), (-98.81149 40.78093), (-98.549… 9 Upper Republ… 551906. ((-102.0516 40.24644), (-102.0515 40.6287), (-102.051… So, we got total acres by NRD as we expected. One interesting change that happened is geometry variable. Each NRD now has multipoint sfg, which is the combination of all the wells (points) located inside the NRD as you can see below. tm_shape(wells_by_nrd) + tm_symbols(col = &quot;nrdname&quot;, size = 0.2) + tm_layout( frame = NA, legend.outside = TRUE, legend.outside.position = &quot;bottom&quot; ) This feature is unlikely to be of much use to us. If you would like to drop a geometry column, you can use the st_drop_geometry() function: #--- remove geometry ---# wells_no_longer_sf &lt;- st_drop_geometry(wells_by_nrd) #--- take a look ---# wells_no_longer_sf # A tibble: 9 x 2 nrdname tot_acres * &lt;chr&gt; &lt;dbl&gt; 1 Central Platte 1890918. 2 Little Blue 995900. 3 Lower Republican 543079. 4 Middle Republican 443472. 5 South Platte 216109. 6 Tri-Basin 847058. 7 Twin Platte 452678. 8 Upper Big Blue 1804782. 9 Upper Republican 551906. Finally, data.table does not work as well with sf objects as dplyr does. #--- convert an sf to data.table ---# wells_by_nrd_dt &lt;- data.table(wells_by_nrd) #--- take a look ---# wells_by_nrd_dt nrdname tot_acres geometry 1: Central Platte 1890918.2 MULTIPOINT ((-100.2329 41.1...,... 2: Little Blue 995900.3 MULTIPOINT ((-98.72659 40.3...,... 3: Lower Republican 543079.2 MULTIPOINT ((-100.1968 40.3...,... 4: Middle Republican 443472.2 MULTIPOINT ((-101.3691 40.1...,... 5: South Platte 216109.0 MULTIPOINT ((-104.0531 41.1...,... 6: Tri-Basin 847058.4 MULTIPOINT ((-100.0927 40.4...,... 7: Twin Platte 452677.6 MULTIPOINT ((-102.0557 41.0...,... 8: Upper Big Blue 1804781.5 MULTIPOINT ((-98.83619 40.8...,... 9: Upper Republican 551906.2 MULTIPOINT ((-102.0516 40.2...,... #--- check the class ---# class(wells_by_nrd_dt) [1] &quot;data.table&quot; &quot;data.frame&quot; You see that wells_by_nrd_dt is no longer an sf object even though geometry still remains in the data. If you try to run sf operations on it, it will of course give you an error. Like this: st_buffer(wells_by_nrd_dt, dist = 2) Error in UseMethod(&quot;st_buffer&quot;): no applicable method for &#39;st_buffer&#39; applied to an object of class &quot;c(&#39;data.table&#39;, &#39;data.frame&#39;)&quot; But, it is easy to revert it back to an sf object again by using the st_as_sf() function55. wells_by_nrd_sf_again &lt;- st_as_sf(wells_by_nrd_dt) So, this means that if you need fast data transformation, you can first turn an sf to a data.table, transform the data using the data.table functionality, and then revert back to sf56. However, for most economists, the geometry variable itself is not of interest in the sense that it never enters econometric models. For most of us, the geographic information contained in the geometry variable is just a glue to tie two datasets together by geographic referencing. Once we get values of spatial variables of interest, then there is no point in keeping your data an sf object. Personally, whenever I no longer need to carry around the geometry variable, I immediately turn an sf object into a data.table for fast data transformation especially when the data is large. 2.6 Turning a data.frame of points into an sf Often times, you have a dataset with geographic coordinates as variables in a csv or other formats, which would not be recognized as a spatial dataset by R immediately when it is read into R. In this case, you need to identify which variables represent the geographic coordinates from the data set, and create an sf yourself. Fortunately, it is easy to do so using the st_as_sf() function. #--- read well registration data ---# wells &lt;- readRDS(&#39;./Data/registration.rds&#39;) #--- recognize it as an sf ---# wells_sf &lt;- st_as_sf(wells, coords = c(&quot;longdd&quot;,&quot;latdd&quot;)) #--- take a look at the data ---# head(wells_sf[,1:5]) Simple feature collection with 6 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699 CRS: NA wellid long_utm lat_utm useid ownerid geometry 1 2 450660.7 4505424 I 106106 POINT (-99.58401 40.69825) 2 3 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 3 4 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 4 5 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 5 6 447157.4 4509271 I 15837 POINT (-99.6258 40.73268) 6 7 445515.0 4509168 I 90248 POINT (-99.64524 40.73164) Note that the CRS of wells_sf is NA. Obviously, \\(R\\) does not know the reference system without you telling it. We know57 that the geographic coordinates in the wells data is NAD 83 (\\(epsg=4269\\)) for this dataset. So, we can assign the right CRS using either st_set_crs() or st_crs(). #--- set CRS ---# wells_sf &lt;- st_set_crs(wells_sf, 4269) #--- or this ---# st_crs(wells_sf) &lt;- 4269 #--- see the change ---# head(wells_sf[,1:5]) Simple feature collection with 6 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.6249 ymin: 40.69824 xmax: -99.58401 ymax: 41.11699 CRS: EPSG:4269 wellid long_utm lat_utm useid ownerid geometry 1 2 450660.7 4505424 I 106106 POINT (-99.58401 40.69825) 2 3 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 3 4 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 4 5 195648.1 4558080 I 14133 POINT (-102.6249 41.11699) 5 6 447157.4 4509271 I 15837 POINT (-99.6258 40.73268) 6 7 445515.0 4509168 I 90248 POINT (-99.64524 40.73164) 2.7 Conversion to and from sp Though unlikely, you may find instances where sp objects are necessary or desirable.58 In that case, it is good to know how to convert an sf object to an sp object, vice versa. You can convert an sf object to its sp counterpart using as(sf_object, &quot;Spatial&quot;): #--- conversion ---# wells_sp &lt;- as(wells_sf,&quot;Spatial&quot;) #--- check the class of NE_sp ---# class(wells_sp) [1] &quot;SpatialPointsDataFrame&quot; attr(,&quot;package&quot;) [1] &quot;sp&quot; As you can see NE_sp is a class of SpatialPointsDataFrame, polygons with data frame supported by the sp package. The above syntax works for converting an sf of polygons into SpatialPolygonsDataFrame as well59. You can revert NE_sp back to an sf object using the st_as_sf() function, as follows: #--- revert back to sf ---# wells_sf &lt;- st_as_sf(wells_sp) #--- check the class ---# class(wells_sf) [1] &quot;sf&quot; &quot;data.frame&quot; We do not cover how to use the sp package as the benefit of learning it has become marginal compared to when sf was just introduced a few years back60. 2.8 Geometrical operations There are various geometrical operations that are particularly useful for economists. Here, some of the most commonly used geometrical operations are introduced61. You can see the practical use of some of these functions in Chapter 1. 2.8.1 st_buffer st_buffer() creates a buffer around points, lines, or the border of polygons. Let’s create buffers around points. First, we read well locations data. #--- read wells location data ---# urnrd_wells_sf &lt;- readRDS(&quot;./Data/urnrd_wells.rds&quot;) %&gt;% #--- project to UTM 14N WGS 84 ---# st_transform(32614) Here is the spatial distribution of the wells (Figure 2.2). tm_shape(urnrd_wells_sf) + tm_symbols(col = &quot;red&quot;, size = 0.1) + tm_layout(frame = NA) Figure 2.2: Map of the wells Let’s create buffers around the wells. #--- create a one-mile buffer around the wells ---# wells_buffer &lt;- st_buffer(urnrd_wells_sf, dist = 1600) As you can see, you see bunch of circles around wells with the radius of \\(1,600\\) meters (Figure 2.3). tm_shape(wells_buffer) + tm_polygons(alpha = 0) + tm_shape(urnrd_wells_sf) + tm_symbols(col = &quot;red&quot;, size = 0.1) + tm_layout(frame = NA) Figure 2.3: Buffers around wells A practical application of buffer creation will be seen in Chapter 1.1. We now create buffers around polygons. First, read NE county boundary data and select three counties (Chase, Dundy, and Perkins). Here is what they look like (Figure 2.4): tm_shape(NE_counties) + tm_polygons(&#39;NAME&#39;, palette=&quot;RdYlGn&quot;, contrast=.3, title=&quot;County&quot;) + tm_layout(frame = NA) Figure 2.4: Map of the three counties The following code creates buffers around polygons (see the results in Figure 2.5): NE_buffer &lt;- st_buffer(NE_counties, dist = 2000) tm_shape(NE_buffer) + tm_polygons(col=&#39;blue&#39;,alpha=0.2) + tm_shape(NE_counties) + tm_polygons(&#39;NAME&#39;, palette=&quot;RdYlGn&quot;, contrast=.3, title=&quot;County&quot;) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.5: Buffers around the three counties For example, this can use useful to identify observations are close enough to the border of political boundaries when you want to take advantage of spatial discontinuity of policies across adjacent political boundaries. 2.8.2 st_area The st_area() function calculates the area of polygons. #--- generate area by polygon ---# ( NE_counties &lt;- mutate(NE_counties, area = st_area(NE_counties)) ) Simple feature collection with 3 features and 10 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676 CRS: EPSG:32614 STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 [m^2] 2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 [m^2] 3 3046331 MULTIPOLYGON (((240811.3 44... 2389890531 [m^2] Now, as you can see below, the default class of the results of st_area() is units, which does not accept numerical operations. class(NE_counties$area) [1] &quot;units&quot; So, let’s turn it into double. ( NE_counties &lt;- mutate(NE_counties, area = as.numeric(area)) ) Simple feature collection with 3 features and 10 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 239494.1 ymin: 4430632 xmax: 310778.1 ymax: 4543676 CRS: EPSG:32614 STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 MULTIPOLYGON (((243340.2 45... 2302174854 2 7978172 MULTIPOLYGON (((241201.4 44... 2316908196 3 3046331 MULTIPOLYGON (((240811.3 44... 2389890531 st_area() is useful when you want to find area-weighted average of characteristics after spatially joining two polygon layers using the st_intersection() function (See Chapter 3.3.3). 2.8.3 st_centroid The st_centroid() function finds the centroid of each polygon. #--- create centroids ---# ( NE_centroids &lt;- st_centroid(NE_counties) ) Simple feature collection with 3 features and 10 fields geometry type: POINT dimension: XY bbox: xmin: 271156.7 ymin: 4450826 xmax: 276594.1 ymax: 4525635 CRS: EPSG:32614 STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND 1 31 135 00835889 0500000US31135 31135 Perkins 06 2287828025 2 31 029 00835836 0500000US31029 31029 Chase 06 2316533447 3 31 057 00835850 0500000US31057 31057 Dundy 06 2381956151 AWATER geometry area 1 2840176 POINT (276594.1 4525635) 2302174854 2 7978172 POINT (271469.9 4489429) 2316908196 3 3046331 POINT (271156.7 4450826) 2389890531 Here’s what the map of the output (Figure 2.6). tm_shape(NE_counties) + tm_polygons() + tm_shape(NE_centroids)+ tm_symbols(size=0.5) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.6: The centroids of the polygons It can be useful when creating a map with labels because the centroid of polygons tend to be a good place to place labels at like this (Figure 2.7).62 tm_shape(NE_counties) + tm_polygons() + tm_shape(NE_centroids)+ tm_text(&quot;NAME&quot;) + tm_layout( legend.outside=TRUE, frame=FALSE ) Figure 2.7: County names placed at the centroids of the counties It may be also useful when you somehow need to calculate the “distance” between polygons. 2.8.4 st_length We can use st_length() to calculate great circle distances63 of LINESTRING and MULTILINESTRING when they are represented in geodetic coordinates. On the other hand, if they are projected and use a Cartesian coordinate system, it will calculate Euclidean distance. We use U.S. railroad data for a demonstration. #--- import US railroad data and take only the first 10 of it ---# ( a_railroad &lt;- rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;)[1:10, ] ) Reading layer `tl_2015_us_rails&#39; from data source `/Users/tmieno2/Box/Teaching/AAEA R/GIS/Data&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 180958 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 CRS: 4269 Simple feature collection with 10 features and 3 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776 CRS: 4269 LINEARID FULLNAME MTFCC geometry 1 11020239500 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058... 2 11020239501 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 3 11020239502 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819... 4 11020239503 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 5 11020239504 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031... 6 11020239575 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 7 11020239576 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852... 8 11020239577 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 9 11020239589 Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736... 10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848... #--- check CRS ---# st_crs(a_railroad) Coordinate Reference System: User input: 4269 wkt: GEOGCS[&quot;NAD83&quot;, DATUM[&quot;North_American_Datum_1983&quot;, SPHEROID[&quot;GRS 1980&quot;,6378137,298.257222101, AUTHORITY[&quot;EPSG&quot;,&quot;7019&quot;]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[&quot;EPSG&quot;,&quot;6269&quot;]], PRIMEM[&quot;Greenwich&quot;,0, AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]], UNIT[&quot;degree&quot;,0.0174532925199433, AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]], AUTHORITY[&quot;EPSG&quot;,&quot;4269&quot;]] It uses geodetic coordinate system. Let’s calculate the great circle distance of the lines (Chapter 1.4 for a practical use case of this function). ( a_railroad &lt;- mutate(a_railroad, length = st_length(a_railroad)) ) Simple feature collection with 10 features and 4 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -79.74031 ymin: 35.0571 xmax: -79.2377 ymax: 35.51776 CRS: 4269 LINEARID FULLNAME MTFCC geometry 1 11020239500 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.47058... 2 11020239501 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 3 11020239502 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.66819... 4 11020239503 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.46687... 5 11020239504 Norfolk Southern Rlwy R1011 MULTILINESTRING ((-79.74031... 6 11020239575 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 7 11020239576 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.47852... 8 11020239577 Seaboard Coast Line RR R1011 MULTILINESTRING ((-79.43695... 9 11020239589 Aberdeen and Rockfish RR R1011 MULTILINESTRING ((-79.38736... 10 11020239591 Aberdeen and Briar Patch RR R1011 MULTILINESTRING ((-79.53848... length 1 661.3381 [m] 2 657.4261 [m] 3 19982.5998 [m] 4 13888.3385 [m] 5 7194.7745 [m] 6 1061.2335 [m] 7 7824.0945 [m] 8 31756.9803 [m] 9 4547.1970 [m] 10 17103.0691 [m] References "],
["spatial-interactions-of-vector-data-subsetting-and-joining.html", "Chapter 3 Spatial Interactions of Vector Data: Subsetting and Joining 3.1 Topological relations 3.2 Spatial Subsetting (or Flagging) 3.3 Spatial Join", " Chapter 3 Spatial Interactions of Vector Data: Subsetting and Joining Introduction 3.1 Topological relations Before we learn spatial subsetting and joining, we first look at topological relations. Topological relations refer to the way multiple spatial objects are spatially related to one another. You can identify various types of spatial relations using the sf package. Our main focus is on the intersections of spatial objects, which can be found using st_intersects().64 We also briefly cover st_is_within_distance()65. We first create sf objects we are going to use for illustrations. POINTS #--- create points ---# point_1 &lt;- st_point(c(2, 2)) point_2 &lt;- st_point(c(1, 1)) point_3 &lt;- st_point(c(1, 3)) #--- combine the points to make a single sf of points ---# ( points &lt;- list(point_1, point_2, point_3) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(name = c(&quot;point 1&quot;, &quot;point 2&quot;, &quot;point 3&quot;)) ) Simple feature collection with 3 features and 1 field geometry type: POINT dimension: XY bbox: xmin: 1 ymin: 1 xmax: 2 ymax: 3 CRS: NA x name 1 POINT (2 2) point 1 2 POINT (1 1) point 2 3 POINT (1 3) point 3 LINES #--- create points ---# line_1 &lt;- st_linestring(rbind(c(0, 0), c(2.5, 0.5))) line_2 &lt;- st_linestring(rbind(c(1.5, 0.5), c(2.5, 2))) #--- combine the points to make a single sf of points ---# ( lines &lt;- list(line_1, line_2) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(name = c(&quot;line 1&quot;, &quot;line 2&quot;)) ) Simple feature collection with 2 features and 1 field geometry type: LINESTRING dimension: XY bbox: xmin: 0 ymin: 0 xmax: 2.5 ymax: 2 CRS: NA x name 1 LINESTRING (0 0, 2.5 0.5) line 1 2 LINESTRING (1.5 0.5, 2.5 2) line 2 POLYGONS #--- create polygons ---# polygon_1 &lt;- st_polygon(list( rbind(c(0, 0), c(2, 0), c(2, 2), c(0, 2), c(0, 0)) )) polygon_2 &lt;- st_polygon(list( rbind(c(0.5, 1.5), c(0.5, 3.5), c(2.5, 3.5), c(2.5, 1.5), c(0.5, 1.5)) )) polygon_3 &lt;- st_polygon(list( rbind(c(0.5, 2.5), c(0.5, 3.2), c(2.3, 3.2), c(2, 2), c(0.5, 2.5)) )) #--- combine the polygons to make an sf of polygons ---# ( polygons &lt;- list(polygon_1, polygon_2, polygon_3) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(name = c(&quot;polygon 1&quot;, &quot;polygon 2&quot;, &quot;polygon 3&quot;)) ) Simple feature collection with 3 features and 1 field geometry type: POLYGON dimension: XY bbox: xmin: 0 ymin: 0 xmax: 2.5 ymax: 3.5 CRS: NA x name 1 POLYGON ((0 0, 2 0, 2 2, 0 ... polygon 1 2 POLYGON ((0.5 1.5, 0.5 3.5,... polygon 2 3 POLYGON ((0.5 2.5, 0.5 3.2,... polygon 3 Figure 3.1 shows how they look: ggplot() + geom_sf(data = polygons, aes(fill = name), alpha = 0.3) + scale_fill_discrete(name = &quot;Polygons&quot;) + geom_sf(data = lines, aes(color = name)) + scale_color_discrete(name = &quot;Lines&quot;) + geom_sf(data = points, aes(shape = name), size = 3) + scale_shape_discrete(name = &quot;Points&quot;) Figure 3.1: Visualization of the points, lines, and polygons 3.1.1 st_intersects() This function identifies which sfg object in an sf (or sfc) intersects with sfg object(s) in another sf. For example, you can use the function to identify which well is located within which NRD. st_intersects() is the most commonly used topological relations, and it is the default topological relation used when performing spatial subsetting and joining, which we will cover later. points and polygons st_intersects(points, polygons) Sparse geometry binary predicate list of length 3, where the predicate was `intersects&#39; 1: 1, 2, 3 2: 1 3: 2, 3 As you can see, the output is a list of which polygon(s) each of the points intersect with. 1, 2, and 3 for the first row means that 1st (polygon 1), 2nd (polygon 2), and 3rd (polygon 3) objects of the polygons intersect with the first point (point 1) of the points object. The fact that point 1 is considered to be intersecting with polygon 2 means that the area inside the border is considered a part of the polygon (of course). If you would like the results of st_intersects() in a matrix form with boolean values filling the matrix, you can add sparse = FALSE option. st_intersects(points, polygons, sparse = FALSE) [,1] [,2] [,3] [1,] TRUE TRUE TRUE [2,] TRUE FALSE FALSE [3,] FALSE TRUE TRUE lines and polygons st_intersects(lines, polygons) Sparse geometry binary predicate list of length 2, where the predicate was `intersects&#39; 1: 1 2: 1, 2 The output is a list of which polygon(s) each of the lines intersect with. polygons and polygons For polygons vs polygons interaction, st_intersects() identifies any polygons that either touches (even at a point) or share some area. st_intersects(polygons, polygons) Sparse geometry binary predicate list of length 3, where the predicate was `intersects&#39; 1: 1, 2, 3 2: 1, 2, 3 3: 1, 2, 3 3.1.2 st_intersection() Instead of getting just indices of intersecting objects, st_intersection() returns intersecting spatial objects. lines and polygons The following code gets the intersection of line 2 and the polygons. intersections &lt;- st_intersection(lines[2, ], polygons) %&gt;% mutate(int_name = paste0(name, &quot;-&quot;, name.1)) #--- take a look ---# intersections Simple feature collection with 2 features and 3 fields geometry type: LINESTRING dimension: XY bbox: xmin: 1.5 ymin: 0.5 xmax: 2.5 ymax: 2 CRS: NA name name.1 x int_name 1 line 2 polygon 1 LINESTRING (1.5 0.5, 2 1.25) line 2-polygon 1 2 line 2 polygon 2 LINESTRING (2.166667 1.5, 2... line 2-polygon 2 As you can see in Figure 3.2 below, each instance of the intersections of the line and polygons become an observation (line 2-polygon 1 and line 2-polygon 2). Note also that the part of the line that did not intersect is cut out and does not remain in the returned sf.66 ggplot() + #--- here are all the original polygons ---# geom_sf(data = polygons, aes(fill = name), alpha = 0.1) + #--- here is what is returned after st_intersection ---# geom_sf(data = intersections, aes(color = int_name), size = 1.5) Figure 3.2: The outcome of the intersections of the lines and polygons polygons and polygons The following code gets the intersection of polygon 1 and polygon 3 with polygon 2. intersections &lt;- st_intersection(polygons[c(1,3), ], polygons[2, ]) %&gt;% mutate(int_name = paste0(name, &quot;-&quot;, name.1)) #--- take a look ---# intersections Simple feature collection with 2 features and 3 fields geometry type: POLYGON dimension: XY bbox: xmin: 0.5 ymin: 1.5 xmax: 2.3 ymax: 3.2 CRS: NA name name.1 x int_name 1 polygon 1 polygon 2 POLYGON ((0.5 2, 2 2, 2 1.5... polygon 1-polygon 2 2 polygon 3 polygon 2 POLYGON ((0.5 2.5, 0.5 3.2,... polygon 3-polygon 2 As you can see in Figure 3.3, each instance of the intersections of polygons 1 and 3 against polygon 2 becomes an observation (polygon 1-polygon 2 and polygon 3-polygon 2). Just like the lines-polygons case, the non-intersecting part of polygons 1 and 3 are cut out and do not remain in the returned sf. We will see later that st_intersection() can be used to find area-weighted values from the intersecting polygons with a help from st_area(). ggplot() + #--- here are all the original polygons ---# geom_sf(data = polygons, aes(fill = name), alpha = 0.1) + #--- here is what is returned after st_intersection ---# geom_sf(data = intersections, aes(fill = int_name)) Figure 3.3: The outcome of the intersections of polygon 2 and polygons 1 and 3 3.1.3 st_is_within_distance() This function identifies whether two spatial objects are within the distance you specify as the name suggests67. Let’s first create two sets of points. set.seed(38424738) points_set_1 &lt;- lapply(1:5, function(x) st_point(runif(2))) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) points_set_2 &lt;- lapply(1:5, function(x) st_point(runif(2))) %&gt;% st_sfc() %&gt;% st_as_sf() %&gt;% mutate(id = 1:nrow(.)) Here is how they are spatially distributed (Figure 3.4). Instead of circles of points, their corresponding id (or equivalently row number here) values are displayed. ggplot() + geom_sf_text(data = points_set_1, aes(label = id), color = &quot;red&quot;) + geom_sf_text(data = points_set_2, aes(label = id), color = &quot;blue&quot;) Figure 3.4: The locations of the set of points We want to know which of the blue points (points_set_2) are located within 0.2 from each of the red points (points_set_1). The following figure (Figure 3.5) gives use the answer visually. #--- create 0.2 buffers around points in points_set_1 ---# buffer_1 &lt;- st_buffer(points_set_1, dist = 0.2) ggplot() + geom_sf(data = buffer_1, color = &quot;red&quot;, fill = NA) + geom_sf_text(data = points_set_1, aes(label = id), color = &quot;red&quot;) + geom_sf_text(data = points_set_2, aes(label = id), color = &quot;blue&quot;) Figure 3.5: The blue points within 0.2 radius of the red points Confirm your visual inspection results with the outcome of the following code using st_is_within_distance() function. st_is_within_distance(points_set_1, points_set_2, dist = 0.2) Sparse geometry binary predicate list of length 5, where the predicate was `is_within_distance&#39; 1: 1 2: (empty) 3: (empty) 4: (empty) 5: 3 3.2 Spatial Subsetting (or Flagging) Spatial subsetting refers to operations that narrow down the geographic scope of a spatial object based on another spatial object. We illustrate spatial subsetting using Kansas county borders, the boundary of the High-Plains Aquifer (HPA), and agricultural irrigation wells in Kansas. First, let’s import all the files we will use in this section. #--- Kansas county borders ---# KS_counties &lt;- readRDS(&quot;./Data/KS_county_borders.rds&quot;) #--- HPA boundary ---# hpa &lt;- st_read(dsn = &quot;./Data&quot;, layer = &quot;hp_bound2010&quot;) %&gt;% .[1, ] %&gt;% st_transform(st_crs(KS_counties)) #--- all the irrigation wells in KS ---# KS_wells &lt;- readRDS(&quot;./Data/Kansas_wells.rds&quot;) %&gt;% st_transform(st_crs(KS_counties)) #--- US railroad ---# rail_roads &lt;- st_read(dsn = &quot;./Data/&quot;, layer = &quot;tl_2015_us_rails&quot;) %&gt;% st_transform(st_crs(KS_counties)) 3.2.1 polygons vs polygons The following map (Figure 3.6) shows the Kansas portion of the HPA and KS counties. #--- add US counties layer ---# tm_shape(KS_counties) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) Figure 3.6: Kansas portion of High-Plains Aquifer and Kansas counties The goal here is to select only the counties that intersects with the HPA boundary. When subsetting a data.frame by specifying the row numbers you would like to select, you can do data.frame[vector of row numbers, ] Spatial subsetting of sf objects works in a similar syntax: sf_1[sf_2, ] where you are subsetting sf_1 based on sf_2. Instead of row numbers, you provide another sf object in place. The following code spatially subsets KS counties based on the HPA boundary. counties_in_hpa &lt;- KS_counties[hpa, ] See the results below in Figure ??. #--- add US counties layer ---# tm_shape(counties_in_hpa) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) (#fig:default_subset)The results of spatially subsetting KS counties based on HPA boundary You can see that only the counties that intersect with the HPA boundary remained. This is because when you use the above syntax of sf_1[sf_2, ], the default underlying topological relations is st_intersects(). So, if an object in sf_1 intersects with any of the objects in sf_2 even slightly, then it will remain after subsetting. You can specify the spatial operation to be used as an option as in sf_1[sf_2, op = topological_relation_type] For example, if you only want counties that are completely within the HPA boundary, you can do the following (the map of the results in Figure 3.7): counties_within_hpa &lt;- KS_counties[hpa, , op = st_within] #--- add US counties layer ---# tm_shape(counties_within_hpa) + tm_polygons() + #--- add High-Plains Aquifer layer ---# tm_shape(hpa) + tm_fill(col = &quot;blue&quot;, alpha = 0.3) Figure 3.7: Kansas counties that are completely within HPA boundary 3.2.2 points vs polygons The following map (Figure 3.8) shows the Kansas portion of the HPA and all the irrigation wells in KS. tm_shape(KS_wells) + tm_symbols(size = 0.1) + tm_shape(hpa) + tm_polygons(col = &quot;blue&quot;, alpha = 0.1) Figure 3.8: A map of Kansas irrigation wells and HPA We can select only wells that reside within the HPA boundary using the same syntax as the above example. KS_wells_in_hpa &lt;- KS_wells[hpa, ] As you can see in Figure 3.9 below, only the wells that are inside (or intersects with) the HPA remained as the default topological relation is st_intersects(). tm_shape(KS_wells_in_hpa) + tm_symbols(size = 0.1) + tm_shape(hpa) + tm_polygons(col = &quot;blue&quot;, alpha = 0.1) Figure 3.9: A map of Kansas irrigation wells and HPA 3.2.3 lines vs polygons The following map (Figure 3.10) shows the Kansas counties and U.S. railroads. ggplot() + geom_sf(data = rail_roads, col = &quot;blue&quot;) + geom_sf(data = KS_counties, fill = NA) Figure 3.10: U.S. railroads and Kansas county boundary We can select only railroads that intersects with Kansas. railroads_KS &lt;- rail_roads[KS_counties, ] As you can see in Figure 3.11 below, only the railroads that intersect with Kansas were selected. Note the the lines that go beyond the Kansas boundary are also selected. Remember, the default is st_intersect(). If you would like the lines beyond the state boundary to be cut out, but the intersecting parts of those lines to remain, use st_intersection(). tm_shape(railroads_KS) + tm_lines(col = &quot;blue&quot;) + tm_shape(KS_counties) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE) Figure 3.11: Railroads that intersects Kansas county boundary 3.2.4 Flagging instead of subsetting Sometimes, you just want to flag whether two spatial objects intersect or not, instead of dropping non-overlapping observations. In that case, you can use st_intersects(). Counties (polygons) against HPA boundary (polygons) #--- county ---# KS_counties &lt;- mutate(KS_counties, intersects_hpa = st_intersects(KS_counties, hpa, sparse = FALSE)) #--- take a look ---# dplyr::select(KS_counties, COUNTYFP, intersects_hpa) Simple feature collection with 105 features and 2 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 First 10 features: COUNTYFP intersects_hpa geometry 1 133 FALSE MULTIPOLYGON (((-95.5255 37... 2 075 TRUE MULTIPOLYGON (((-102.0446 3... 3 123 FALSE MULTIPOLYGON (((-98.48738 3... 4 189 TRUE MULTIPOLYGON (((-101.5566 3... 5 155 TRUE MULTIPOLYGON (((-98.47279 3... 6 129 TRUE MULTIPOLYGON (((-102.0419 3... 7 073 FALSE MULTIPOLYGON (((-96.52278 3... 8 023 TRUE MULTIPOLYGON (((-102.0517 4... 9 089 TRUE MULTIPOLYGON (((-98.50445 4... 10 059 FALSE MULTIPOLYGON (((-95.50827 3... Wells (points) against HPA boundary (polygons) #--- wells ---# KS_wells &lt;- mutate(KS_wells, in_hpa = st_intersects(KS_wells, hpa, sparse = FALSE)) #--- take a look ---# dplyr::select(KS_wells, site, in_hpa) Simple feature collection with 37647 features and 2 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199 CRS: EPSG:4269 First 10 features: site in_hpa geometry 1 1 TRUE POINT (-100.4423 37.52046) 2 3 TRUE POINT (-100.7118 39.91526) 3 5 TRUE POINT (-99.15168 38.48849) 4 7 TRUE POINT (-101.8995 38.78077) 5 8 TRUE POINT (-100.7122 38.0731) 6 9 FALSE POINT (-97.70265 39.04055) 7 11 TRUE POINT (-101.7114 39.55035) 8 12 FALSE POINT (-95.97031 39.16121) 9 15 TRUE POINT (-98.30759 38.26787) 10 17 TRUE POINT (-100.2785 37.71539) U.S. railroads (lines) against Kansas county (polygons) Unlike the previous two cases, multiple objects (lines) are checked against multiple objects (polygons) for intersection68. Therefore, we cannot use the strategy we took above of returning a vector of true or false using sparse = TRUE option. Here, we need to count the number of intersecting counties and then assign TRUE if the number is greater than 0. #--- check the number of intersecting KS counties ---# int_mat &lt;- st_intersects(rail_roads, KS_counties) %&gt;% lapply(length) %&gt;% unlist() #--- railroads ---# rail_roads &lt;- mutate(rail_roads, intersect_ks = int_mat &gt; 0) #--- take a look ---# dplyr::select(rail_roads, LINEARID, intersect_ks) Simple feature collection with 180958 features and 2 fields geometry type: MULTILINESTRING dimension: XY bbox: xmin: -165.4011 ymin: 17.95174 xmax: -65.74931 ymax: 65.00006 CRS: 4269 First 10 features: LINEARID intersect_ks geometry 1 11020239500 FALSE MULTILINESTRING ((-79.47058... 2 11020239501 FALSE MULTILINESTRING ((-79.46687... 3 11020239502 FALSE MULTILINESTRING ((-79.66819... 4 11020239503 FALSE MULTILINESTRING ((-79.46687... 5 11020239504 FALSE MULTILINESTRING ((-79.74031... 6 11020239575 FALSE MULTILINESTRING ((-79.43695... 7 11020239576 FALSE MULTILINESTRING ((-79.47852... 8 11020239577 FALSE MULTILINESTRING ((-79.43695... 9 11020239589 FALSE MULTILINESTRING ((-79.38736... 10 11020239591 FALSE MULTILINESTRING ((-79.53848... 3.3 Spatial Join By spatial join, we mean spatial operations that involve the followings: overlay one spatial layer (target layer) onto another spatial layer (source layer) for each of the observation in the target layer identify which objects in the source layer it geographically intersects (or being close) with extract values associated with the intersecting objects in the source layer (and summarize if necessary), assign the extracted value to the object in the target layer For economists, this is probably the most common motivation of using GIS software, with the ultimate goal being including the spatially joined variables as covariates in regression analysis. We can classify spatial join into four categories by the type of the underlying spatial objects: vector-vector: vector data (target) against vector data (source) vector-raster: vector data (target) against raster data (source) raster-vector: raster data (target) against vector data (source) raster-raster: raster data (target) against raster data (source) Among the four, our focus here is the first case. The second case will be discussed in Chapter 5. We will not cover the third and fourth cases in this class.69 Category 1 can be further broken down into different sub categories depending on the type of spatial objects (point, line, and polygon). Here, we will ignore any spatial joins that involve lines. This is because objects represented by lines are rarely observations units in econometric analysis nor the source data that we will extract values from.70. So, here is the list of the types of spatial joins we will learn. points (target) against polygons (source) polygons (target) against points (source) polygons (target) against polygons (source) 3.3.1 Case 1: points (target) vs polygons (source) Case 1, for each of the observations (points) in the target data, finds which polygon in the source file it intersects, and then assign the value associated with the polygon to the point71. In order to achieve this, we can use the st_join() function, whose syntax is as follows: st_join(target_sf, source_sf) Similar to spatial subsetting, the default topological relation is st_intersects()72. We use the KS irrigation wells data (points) and KS county boundary data (polygons) for a demonstration. Our goal is to assign the county-level corn price information from the KS county data to wells. First let me create and add a fake county-level corn price variable to the KS county data. KS_corn_price &lt;- KS_counties %&gt;% mutate( corn_price = seq(3.2, 3.9, length = nrow(.)) ) %&gt;% dplyr::select(COUNTYFP, corn_price) Here is the map of KS county color-differentiated by fake corn price (Figure 3.12): tm_shape(KS_corn_price) + tm_polygons(col = &quot;corn_price&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.12: Map of county-level fake corn price For this particular context, the following code will do the job: #--- spatial join ---# ( KS_wells_County &lt;- st_join(KS_wells, KS_corn_price) ) Simple feature collection with 37647 features and 5 fields geometry type: POINT dimension: XY bbox: xmin: -102.0495 ymin: 36.99552 xmax: -94.62089 ymax: 40.00199 CRS: EPSG:4269 First 10 features: site af_used in_hpa COUNTYFP corn_price geometry 1 1 232.099948 TRUE 069 3.556731 POINT (-100.4423 37.52046) 2 3 13.183940 TRUE 039 3.449038 POINT (-100.7118 39.91526) 3 5 99.187052 TRUE 165 3.287500 POINT (-99.15168 38.48849) 4 7 0.000000 TRUE 199 3.644231 POINT (-101.8995 38.78077) 5 8 145.520499 TRUE 055 3.832692 POINT (-100.7122 38.0731) 6 9 3.614535 FALSE 143 3.799038 POINT (-97.70265 39.04055) 7 11 188.423543 TRUE 181 3.590385 POINT (-101.7114 39.55035) 8 12 77.335960 FALSE 177 3.550000 POINT (-95.97031 39.16121) 9 15 0.000000 TRUE 159 3.610577 POINT (-98.30759 38.26787) 10 17 167.819034 TRUE 069 3.556731 POINT (-100.2785 37.71539) You can see from Figure 3.13 below that all the wells inside the same county has the same corn price value. tm_shape(KS_counties) + tm_polygons() + tm_shape(KS_wells_County) + tm_symbols(col = &quot;corn_price&quot;, size = 0.2) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.13: Map of wells color-differentiated by corn price 3.3.2 Case 2: polygons (target) vs points (source) Case 2, for each of the observations (polygons) in the target data, find which observations (points) in the source file it intersects, and then assign the values associated with the points to the polygon. We use the same function: st_join()73. Suppose you are now interested in county-level analysis and you would like to get county-level total groundwater pumping. The target file is KS_counties, and the source file is KS_wells. #--- spatial join ---# KS_County_wells &lt;- st_join(KS_counties, KS_wells) #--- take a look ---# dplyr::select(KS_County_wells, COUNTYFP, site, af_used) Simple feature collection with 37652 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 First 10 features: COUNTYFP site af_used geometry 1 133 53861 17.01790 MULTIPOLYGON (((-95.5255 37... 1.1 133 70592 0.00000 MULTIPOLYGON (((-95.5255 37... 2 075 328 394.04513 MULTIPOLYGON (((-102.0446 3... 2.1 075 336 80.65036 MULTIPOLYGON (((-102.0446 3... 2.2 075 436 568.25359 MULTIPOLYGON (((-102.0446 3... 2.3 075 1007 215.80416 MULTIPOLYGON (((-102.0446 3... 2.4 075 1170 0.00000 MULTIPOLYGON (((-102.0446 3... 2.5 075 1192 77.39120 MULTIPOLYGON (((-102.0446 3... 2.6 075 1249 0.00000 MULTIPOLYGON (((-102.0446 3... 2.7 075 1300 320.22612 MULTIPOLYGON (((-102.0446 3... As you can see, in the resulting dataset, all the unique polygon - point intersecting combinations comprise the observations. For each of the polygons, you will have as many observations as the number of wells that intersect with the polygon. Once you joined the two layers, you can find statistics by polygon (county here). Since we want groundwater extraction by county, the following does the job. KS_County_wells %&gt;% group_by(COUNTYFP) %&gt;% summarize(af_used = sum(af_used, na.rm = TRUE)) Simple feature collection with 105 features and 2 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 # A tibble: 105 x 3 COUNTYFP af_used geometry &lt;fct&gt; &lt;dbl&gt; &lt;MULTIPOLYGON [°]&gt; 1 001 0 (((-95.51931 37.82026, -95.51897 38.03823, -95.07788 38.037… 2 003 0 (((-95.50833 38.39028, -95.06583 38.38994, -95.07788 38.037… 3 005 771. (((-95.56413 39.65287, -95.33974 39.65298, -95.11519 39.652… 4 007 4972. (((-99.0126 37.47042, -98.46466 37.47101, -98.46493 37.3841… 5 009 61083. (((-99.03297 38.69676, -98.48611 38.69688, -98.47991 38.681… 6 011 0 (((-95.08808 37.73248, -95.07969 37.8198, -95.07788 38.0377… 7 013 480. (((-95.78811 40.00047, -95.78457 40.00046, -95.3399 40.0000… 8 015 343. (((-97.15248 37.91273, -97.15291 38.0877, -96.84077 38.0856… 9 017 0 (((-96.83765 38.34864, -96.81951 38.52245, -96.35378 38.521… 10 019 0 (((-96.52487 37.30273, -95.9644 37.29923, -95.96427 36.9992… # … with 95 more rows Of course, it is just as easy to get other types of statistics by simply modifying the summarize() part. However, this two step process can be actually done in one step using the aggregate() function as follows: #--- mean ---# aggregate(KS_wells, KS_counties, FUN = mean) Simple feature collection with 105 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 First 10 features: site af_used in_hpa geometry 1 62226.50 8.508950 0.0000000 MULTIPOLYGON (((-95.5255 37... 2 35184.64 176.390742 0.4481793 MULTIPOLYGON (((-102.0446 3... 3 40086.82 35.465123 0.0000000 MULTIPOLYGON (((-98.48738 3... 4 40179.41 285.672916 1.0000000 MULTIPOLYGON (((-101.5566 3... 5 51249.39 46.048048 0.9743783 MULTIPOLYGON (((-98.47279 3... 6 33033.13 202.612377 1.0000000 MULTIPOLYGON (((-102.0419 3... 7 29840.40 0.000000 0.0000000 MULTIPOLYGON (((-96.52278 3... 8 28235.82 94.585634 0.9736842 MULTIPOLYGON (((-102.0517 4... 9 36180.06 44.033911 0.3000000 MULTIPOLYGON (((-98.50445 4... 10 40016.00 1.142775 0.0000000 MULTIPOLYGON (((-95.50827 3... #--- sum ---# aggregate(KS_wells, KS_counties, FUN = sum) Simple feature collection with 105 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 First 10 features: site af_used in_hpa geometry 1 124453 1.701790e+01 0 MULTIPOLYGON (((-95.5255 37... 2 12560917 6.297149e+04 160 MULTIPOLYGON (((-102.0446 3... 3 1964254 1.737791e+03 0 MULTIPOLYGON (((-98.48738 3... 4 42389277 3.013849e+05 1055 MULTIPOLYGON (((-101.5566 3... 5 68007942 6.110576e+04 1293 MULTIPOLYGON (((-98.47279 3... 6 15756801 9.664610e+04 477 MULTIPOLYGON (((-102.0419 3... 7 149202 0.000000e+00 0 MULTIPOLYGON (((-96.52278 3... 8 17167377 5.750807e+04 592 MULTIPOLYGON (((-102.0517 4... 9 1809003 2.201696e+03 15 MULTIPOLYGON (((-98.50445 4... 10 160064 4.571102e+00 0 MULTIPOLYGON (((-95.50827 3... Notice that the mean() function was applied to all the columns in KS_wells, including site id number. So, you might want to select variables you want to join before you apply the aggregate() function like this: aggregate(dplyr::select(KS_wells, af_used), KS_counties, FUN = mean) Simple feature collection with 105 features and 1 field geometry type: MULTIPOLYGON dimension: XY bbox: xmin: -102.0517 ymin: 36.99308 xmax: -94.59193 ymax: 40.00308 CRS: EPSG:4269 First 10 features: af_used geometry 1 8.508950 MULTIPOLYGON (((-95.5255 37... 2 176.390742 MULTIPOLYGON (((-102.0446 3... 3 35.465123 MULTIPOLYGON (((-98.48738 3... 4 285.672916 MULTIPOLYGON (((-101.5566 3... 5 46.048048 MULTIPOLYGON (((-98.47279 3... 6 202.612377 MULTIPOLYGON (((-102.0419 3... 7 0.000000 MULTIPOLYGON (((-96.52278 3... 8 94.585634 MULTIPOLYGON (((-102.0517 4... 9 44.033911 MULTIPOLYGON (((-98.50445 4... 10 1.142775 MULTIPOLYGON (((-95.50827 3... 3.3.3 Case 3: polygons (target) vs polygons (source) For this case, st_join(target_sf, source_sf) will return all the unique intersecting polygon-polygon combinations with the information of the polygon from source_sf attached. We will use county-level corn acres in Iowa in 2018 from USDA NASS74 and Hydrologic Units75 Our objective here is to find corn acres by HUC units based on the county-level corn acres data76. We first import the Iowa corn acre data: #--- IA boundary ---# IA_corn &lt;- readRDS(&quot;./Data/IA_corn.rds&quot;) #--- take a look ---# IA_corn Simple feature collection with 93 features and 3 fields geometry type: MULTIPOLYGON dimension: XY bbox: xmin: 203228.6 ymin: 4470941 xmax: 736832.9 ymax: 4822687 CRS: EPSG:26915 First 10 features: county_code year acres geometry 1 083 2018 183500 MULTIPOLYGON (((458997 4711... 2 141 2018 167000 MULTIPOLYGON (((267700.8 47... 3 081 2018 184500 MULTIPOLYGON (((421231.2 47... 4 019 2018 189500 MULTIPOLYGON (((575285.6 47... 5 023 2018 165500 MULTIPOLYGON (((497947.5 47... 6 195 2018 111500 MULTIPOLYGON (((459791.6 48... 7 063 2018 110500 MULTIPOLYGON (((345214.3 48... 8 027 2018 183000 MULTIPOLYGON (((327408.5 46... 9 121 2018 70000 MULTIPOLYGON (((396378.1 45... 10 077 2018 107000 MULTIPOLYGON (((355180.1 46... Here is the map of IA county color-differentiated by corn acres (Figure 3.14): #--- here is the map ---# tm_shape(IA_corn) + tm_polygons(col = &quot;acres&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.14: Map of Iowa counties color-differentiated by corn planted acreage Now import the HUC units data: #--- import HUC units ---# HUC_IA &lt;- st_read(dsn = &quot;./Data/huc250k_shp&quot;, layer = &quot;huc250k&quot;) %&gt;% dplyr::select(HUC_CODE) %&gt;% #--- reproject to the CRS of IA ---# st_transform(st_crs(IA_corn)) %&gt;% #--- select HUC units that overlaps with IA ---# .[IA_corn, ] Here is the map of HUC units (Figure 3.15): tm_shape(HUC_IA) + tm_polygons() + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.15: Map of HUC units that intersect with Iowa state boundary IA county with HUC units superimposed on top (Figure 3.16): tm_shape(IA_corn) + tm_polygons(col = &quot;acres&quot;) + tm_shape(HUC_IA) + tm_polygons(alpha = 0) + tm_layout(frame = FALSE, legend.outside = TRUE) Figure 3.16: Map of HUC units superimposed on Iowas counties HUC_joined &lt;- st_join(HUC_IA, IA_corn) To find an area-weighted average, we can first use st_intersection(). For each of the polygons in the target layer, this function, finds the intersecting polygons from the source data, and then divide the target polygon into parts based on the boundary of the intersecting polygons. HUC_intersections &lt;- st_intersection(HUC_IA, IA_corn) %&gt;% arrange(HUC_CODE) %&gt;% mutate(huc_county = paste0(HUC_CODE, &quot;-&quot;, county_code)) The key difference from the st_join() example (see @ref(st_intersection)) is that it returns a geometry variable that represents the intersecting area of the HUC units and the counties as shown in Figure 3.17 below. tm_shape(filter(HUC_intersections, HUC_CODE == &quot;07020009&quot;)) + tm_polygons(col = &quot;huc_county&quot;) + tm_layout(frame = FALSE) Figure 3.17: Intersections of a HUC unit and Iowa counties HUC_aw_acres &lt;- HUC_intersections %&gt;% #--- get area ---# mutate(area = as.numeric(st_area(.))) %&gt;% #--- get area-weight by HUC unit ---# group_by(HUC_CODE) %&gt;% mutate(weight = area / sum(area)) %&gt;% #--- calculate area-weighted corn acreage ---# summarize(aw_acres = sum(weight * acres)) "]
>>>>>>> development-version
]
